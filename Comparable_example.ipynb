{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_md')\n",
    "from epitator.annotator import AnnoDoc\n",
    "from epitator.geoname_annotator import GeonameAnnotator\n",
    "from epitator.resolved_keyword_annotator import ResolvedKeywordAnnotator\n",
    "from epitator.count_annotator import CountAnnotator\n",
    "from epitator.date_annotator import DateAnnotator\n",
    "from boilerpipe.extract import Extractor\n",
    "from itertools import groupby\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import epitator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sys import stdout\n",
    "from time import sleep\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def get_links_by_year(list_of_years=None, proxies={'http': 'http://fw-bln.rki.local:8020'}):\n",
    "    \"\"\"Returns (all) the anual links of the WHO DONs \n",
    "    \n",
    "    list_of_years -- a list of years (YYYY format) you want to parse (default None)\n",
    "    proxies -- the proxy to use while scraping (default {'http': 'http://fw-bln.rki.local:8020'})\n",
    "    \"\"\"\n",
    "    page = requests.get('http://www.who.int/csr/don/archive/year/en/',proxies=proxies)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    archiv_years = soup.find('ul',attrs={'class':'list'})\n",
    "    years_links_html = archiv_years.find_all('a')\n",
    "    if list_of_years:\n",
    "        return ['http://www.who.int' + link.get('href') for link in years_links_html if any(year in link for year in list_of_years)]\n",
    "    else:\n",
    "        return ['http://www.who.int' + link.get('href') for link in years_links_html]\n",
    "    \n",
    "def get_links_per_year(years_links, list_of_months=None, proxies={'http': 'http://fw-bln.rki.local:8020'}):\n",
    "    '''Take a list of links to the annual archive and return a list of DON links of these years\n",
    "    \n",
    "    years_links -- a list of links of the anual archive to parse \n",
    "    list_of_months -- a list of months (MMM* format) you want to parse (default None)\n",
    "    proxies -- the proxy to use while scraping (default {'http': 'http://fw-bln.rki.local:8020'})\n",
    "    '''\n",
    "    all_links = []\n",
    "    \n",
    "    for year_link in years_links:\n",
    "        page_year = requests.get(year_link,proxies=proxies)\n",
    "        soup_year = BeautifulSoup(page_year.content,'html.parser')\n",
    "        archive_year = soup_year.find('ul',attrs={'class':'auto_archive'})\n",
    "        daily_links = ['http://www.who.int' + link.get('href') for link in archive_year.find_all('a')]\n",
    "        all_links.extend(daily_links)\n",
    "    \n",
    "    if list_of_months:\n",
    "        all_links = [link for link in all_links if any(month in link for month in map(lambda s:s.lower(),list_of_months))]\n",
    "    return all_links\n",
    "    \n",
    "headers = {\n",
    "    'User-Agent': 'Auss Abbood, www.rki.de',\n",
    "    'From': 'abbooda@rki.de'\n",
    "}\n",
    "\n",
    "def scrape(years=None,\n",
    "           months=None,\n",
    "           num_last_reports=None,\n",
    "           headers=None,\n",
    "           proxies={'http': 'http://fw-bln.rki.local:8020'}):\n",
    "    '''Scrapes the WHO DONs using the WHO DON scraping functions and returns the links to these DONs\n",
    "    \n",
    "    years -- a list of strings of years in the format YYYY to be scraped\n",
    "    months -- a list of strings of months in the format MMM* to be scraped\n",
    "    num_list_reports -- an integer to specify how many of the last reports should be scraped. \n",
    "    can be combined with the specification of year and/or month\n",
    "    headers -- use a header for scraping\n",
    "    proxies -- the proxy to use while scraping (default {'http': 'http://fw-bln.rki.local:8020'})\n",
    "    '''\n",
    "    years = get_links_by_year(list_of_years=years,proxies=proxies)\n",
    "    all_links = get_links_per_year(years,list_of_months=months,proxies=proxies)\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all the WHO DONs of the year 2018\n",
    "all_links = scrape(years=['2018'],proxies=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the main text of the given links\n",
    "from boilerpipe.extract import Extractor\n",
    "def extract(list_of_links):\n",
    "    '''Extracts the main content from a list of links and returns a list of texts (str)\n",
    "\n",
    "    list_of_links -- a list containing URLs of webpages to get the main content from\n",
    "    '''\n",
    "    return[Extractor(extractor='ArticleExtractor', url=url).getText().replace('\\n','') \\\n",
    "         for url in tqdm(list_of_links)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this shit (a.k.a annotate all the scraped WHO DONs)\n",
    "def create_annotated_database(texts,raw=False):\n",
    "    '''Given a list of texts (str) annotate and extract disease keywords, geonames, and dates and return\n",
    "    a dictonary of the text and the annotations\n",
    "    \n",
    "    texts -- a list of texts (str)\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    database = {\"text\":texts,\"date\":[],\"confirmed_cases\":[],\"keyword\":[],\"geoname\":[]}\n",
    "    for i,text in enumerate(tqdm(texts)):\n",
    "        try:\n",
    "            doc = annotate(text)\n",
    "            database[\"date\"].append(get_date(doc,raw))\n",
    "            database[\"confirmed_cases\"].append(get_cases(doc,raw))\n",
    "            database[\"keyword\"].append(get_keywords(doc,raw))\n",
    "            database[\"geoname\"].append(get_geonames(doc,raw))\n",
    "        except TypeError as e:\n",
    "            print(\"Type error in text({})\".format(i) + \": \" + str(e))\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text):\n",
    "    ''' Returns an document annotated for dates, disease counts, diseases, and geoneames\n",
    "    \n",
    "    text -- a string to be annotated\n",
    "    '''\n",
    "    doc = AnnoDoc(text)\n",
    "    doc.add_tiers(GeonameAnnotator())\n",
    "    doc.add_tiers(ResolvedKeywordAnnotator())\n",
    "    doc.add_tiers(CountAnnotator())\n",
    "    doc.add_tiers(DateAnnotator())\n",
    "    return doc\n",
    "def get_geonames(doc,raw=False):\n",
    "    '''Returns the most occuring geographical entity in a annotated document\n",
    "    \n",
    "    doc -- an annotated string\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    if raw:\n",
    "        return [doc.tiers[\"geonames\"].spans[i].geoname[\"name\"] for i in range(len(doc.tiers[\"geonames\"].spans))]\n",
    "    else:\n",
    "        geonames = [doc.tiers[\"geonames\"].spans[i].geoname[\"name\"] for i in range(len(doc.tiers[\"geonames\"].spans))]\n",
    "        geoname_counts = [(key,len(list(group))) for key, group in groupby(sorted(geonames))]\n",
    "        geoname_ranking = sorted(geoname_counts,key=lambda x:x[1],reverse=True)\n",
    "        geoname_most_occure = [geoname[0] for geoname in geoname_ranking if geoname[1] == geoname_ranking[0][1]]\n",
    "        return geoname_most_occure\n",
    "def get_keywords(doc,raw=False):\n",
    "    '''Returns the most occuring disease entity in a annotated document\n",
    "    \n",
    "    doc -- an annotated string\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    if raw:\n",
    "        return [doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0]['entity']['label'] \\\n",
    "                for i in range(len(doc.tiers[\"resolved_keywords\"].spans)) \\\n",
    "               if doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0]['entity']['type'] \\\n",
    "                    == 'disease']\n",
    "                     \n",
    "    else:\n",
    "        keywords = [(doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0]['entity']['label'] \\\n",
    "                     ,doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0][\"weight\"]) \\\n",
    "                    for i in range(len(doc.tiers[\"resolved_keywords\"].spans)) \\\n",
    "                    if doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0]['entity']['type'] \\\n",
    "                    == 'disease']\n",
    "\n",
    "        # Here I ignore the included weights and only considere the most occuring disease name\n",
    "        keywords_without_weight = [disease[0] for disease in keywords]\n",
    "        keyword_counts = [(key,len(list(group))) for key, group in groupby(sorted(keywords_without_weight))]\n",
    "        try:\n",
    "            keyword = max(keyword_counts,key=lambda x:x[1])\n",
    "        except ValueError:\n",
    "            keyword = np.nan\n",
    "        if type(keyword) is float:\n",
    "            return keyword\n",
    "        else:\n",
    "            return keyword[0] # Only returns the keyword, not the weight\n",
    "def get_cases(doc,raw=False):\n",
    "    '''Returns the disease counts with the attribute \"confirmed\" in a annotated document\n",
    "    \n",
    "    doc -- an annotated string\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    if raw:\n",
    "        return [doc.tiers[\"counts\"].spans[i].metadata['count'] for i in range(len(doc.tiers[\"counts\"].spans))]\n",
    "    else:\n",
    "        return [doc.tiers[\"counts\"].spans[i].metadata['count'] \\\n",
    "                for i in range(len(doc.tiers[\"counts\"].spans)) \\\n",
    "                if \"confirmed\" in doc.tiers[\"counts\"].spans[i].metadata['attributes']]\n",
    "def get_date(doc,raw=False):\n",
    "    '''Returns most mentioned date in a annotated document\n",
    "    \n",
    "    doc -- an annotated string\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    dates = [doc.tiers[\"dates\"].spans[i].metadata[\"datetime_range\"][0].strftime(\"%Y-%m-%d\") \\\n",
    "             for i in range(len(doc.tiers[\"dates\"].spans))]\n",
    "    if raw:\n",
    "        return dates\n",
    "    else:\n",
    "        date_count_tuple = [(key,len(list(group))) for key, group in groupby(sorted(dates))]\n",
    "        try:\n",
    "            date = max(date_count_tuple,key=lambda x:x[1])\n",
    "        except ValueError:\n",
    "            date = np.nan\n",
    "        if type(date) is float:\n",
    "            return date\n",
    "        else:\n",
    "            return date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = extract(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_whos_df = pd.DataFrame.from_dict(create_annotated_database(extracted))\n",
    "parsed_whos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ereignisdatenbank = pd.read_csv(\"Ereignisse_utf8.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ereignisdatenbank.columns = list(map(lambda x:x.strip(\" \"),ereignisdatenbank.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_not_null = ereignisdatenbank[pd.notnull(ereignisdatenbank[\"Ausgangs- bzw. Ausbruchsland\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = countries_not_null[\"Ausgangs- bzw. Ausbruchsland\"].copy(deep=True)\n",
    "countries = list(map(lambda x:x.strip(\" \"),countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Afghanistan',\n",
       " 'Afghanistan,\\nDR Congo\\nNigeria\\nSomalia',\n",
       " 'Algerien',\n",
       " 'Angola',\n",
       " 'Australien',\n",
       " 'Bangladesch',\n",
       " 'Benin',\n",
       " 'Bolivien',\n",
       " 'Brasilien',\n",
       " 'Burundi',\n",
       " 'China',\n",
       " 'Costa Rica',\n",
       " 'DRC',\n",
       " 'DRCongo',\n",
       " 'Demokratische Republik Kongo',\n",
       " 'Deutschland',\n",
       " 'El Salvador',\n",
       " 'Fiji',\n",
       " 'Frankreich',\n",
       " 'Französich_Polynesien',\n",
       " 'Französiche Guyana',\n",
       " 'Französisch-Polynesien',\n",
       " 'Ghana',\n",
       " 'Haiti',\n",
       " 'Indien',\n",
       " 'Irak',\n",
       " 'Iran',\n",
       " 'Israel',\n",
       " 'Italien',\n",
       " 'Italien, Griechenland, Rumanien, Ungarn, Frankreich',\n",
       " 'Italien, Griechenland, Ungarn, Rumänien',\n",
       " 'Italien, Serbien, Griechenland, Rumänien, Ungarn, Frankreich, Kosovo, Albanien, Macedonien, Montenegro, Serbien, Türkei',\n",
       " 'Kamerun',\n",
       " 'Kanada',\n",
       " 'Kenia',\n",
       " 'Kolumbien',\n",
       " 'Kongo',\n",
       " 'Kroatien',\n",
       " 'Kuwait',\n",
       " 'La Reunion',\n",
       " 'Liberia',\n",
       " 'Madagaskar',\n",
       " 'Malawi',\n",
       " 'Mali',\n",
       " 'Mauretanien',\n",
       " 'Mosambik',\n",
       " 'Myanmar',\n",
       " 'Namibia',\n",
       " 'Niger',\n",
       " 'Nigeria',\n",
       " 'Nordeuropa',\n",
       " 'Oman',\n",
       " 'Pakistan',\n",
       " 'Papua-Neuguinea',\n",
       " 'Peru',\n",
       " 'Philippinen',\n",
       " 'Polen',\n",
       " 'Saudi-Arabien',\n",
       " 'Schweiz',\n",
       " 'Serbien, Italien, Griechenland, Ungarn,  Rumänien',\n",
       " 'Simbabwe',\n",
       " 'Somalia',\n",
       " 'Spanien',\n",
       " 'Sri Lanka',\n",
       " 'Sudan',\n",
       " 'Syrien',\n",
       " 'Süd Sudan',\n",
       " 'Südafrika',\n",
       " 'Südsudan',\n",
       " 'Taiwan',\n",
       " 'Tanzania',\n",
       " 'Trinidad & Tobago',\n",
       " 'Tschad',\n",
       " 'Tschechien',\n",
       " 'Typhus',\n",
       " 'UK',\n",
       " 'USA',\n",
       " 'USA, Delaware',\n",
       " 'Uganda',\n",
       " 'Ukraine',\n",
       " 'VAE',\n",
       " 'VAE Dubai',\n",
       " 'Venezuela',\n",
       " 'Vereinigte Arabische Emirate',\n",
       " 'Vereinigtes Königreich',\n",
       " 'Westafrika',\n",
       " 'Yemen',\n",
       " 'Zambia',\n",
       " 'Zentralafrikanische Republik',\n",
       " 'Äthiopien (AWD)'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the testing begin\n",
    "## Wikipedia parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://de.wikipedia.org/wiki/Liste_der_Staaten_der_Erde\")\n",
    "soup = BeautifulSoup(req.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_soup = soup.find(\"table\",class_=\"wikitable sortable zebra\").find(\"tbody\") # Find table with of all countries\n",
    "parsed_soup = parsed_soup.find_all(\"tr\") # Get entries of countries form table\n",
    "amount_countries = len(parsed_soup)\n",
    "parsed_soup = [parsed_soup[i].find_all('td') \\\n",
    "               for i in range(amount_countries)] # Extract table entries from country entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "wiki_dict = {\"state_name_de\":[],\"full_state_name_de\":[],\"capital_de\":[],\"translation_state_name\":[],\"wiki_abbreviations\":[]}\n",
    "dash = u\"\\u2014\"\n",
    "regex = re.compile(r\"\\[\\d*\\]\") # To remove footnotes in the names\n",
    "for i in range(amount_countries):\n",
    "    try:\n",
    "        state_name_de = regex.sub(\"\",(parsed_soup[i][0].text).replace(\"\\n\",\"\")\\\n",
    "                                  .replace(\"\\xad\",\"\")) # Remove soft hyphen used in Zentralafr. Rep.\n",
    "        state_name_de = re.sub(r\"((mit)|(ohne)).*\",\"\",state_name_de) # Remove some additional information in name\n",
    "        wiki_dict[\"state_name_de\"].append(state_name_de) \n",
    "        wiki_dict[\"full_state_name_de\"].append(regex.sub(\"\",parsed_soup[i][1].text).replace(\"\\n\",\"\")) \n",
    "        wiki_dict[\"capital_de\"].append(regex.sub(\"\",parsed_soup[i][2].text).replace(\"\\n\",\"\")) \n",
    "        wiki_dict[\"translation_state_name\"].append(regex.sub(\"\",parsed_soup[i][10].text).replace(\"\\n\",\"\"))\n",
    "        # Short and long abbreviation\n",
    "        list_abbreviation = [parsed_soup[i][7].text.replace(\"\\n\",\"\"),parsed_soup[i][8].text.replace(\"\\n\",\"\")] \n",
    "        list_abbreviation = list(filter(lambda x:x not in [\"\",dash],list_abbreviation)) # Remove empty entries\n",
    "        if len(list_abbreviation) > 1:\n",
    "            wiki_dict[\"wiki_abbreviations\"].append(list_abbreviation)#\", \".join(list_abbreviation))\n",
    "        else:\n",
    "            wiki_dict[\"wiki_abbreviations\"].append(dash)\n",
    "    except: # Because header and footer are part of the table, soup opperations don't work\n",
    "        print(i) # Except that the first and last entry fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name_de</th>\n",
       "      <th>full_state_name_de</th>\n",
       "      <th>capital_de</th>\n",
       "      <th>translation_state_name</th>\n",
       "      <th>wiki_abbreviations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erde</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>Earth</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Europäische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Brüssel</td>\n",
       "      <td>European Union</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Union Südamerikanischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Union of South American Nations</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afrikanische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Addis Abeba</td>\n",
       "      <td>African Union</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Verband Südostasiatischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Association of Southeast Asian Nations</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        state_name_de full_state_name_de   capital_de  \\\n",
       "0                                Erde                  —            —   \n",
       "1                   Europäische Union                  —      Brüssel   \n",
       "2    Union Südamerikanischer Nationen                  —        Quito   \n",
       "3                  Afrikanische Union                  —  Addis Abeba   \n",
       "4  Verband Südostasiatischer Nationen                  —      Jakarta   \n",
       "\n",
       "                   translation_state_name wiki_abbreviations  \n",
       "0                                   Earth                  —  \n",
       "1                          European Union                  —  \n",
       "2         Union of South American Nations                  —  \n",
       "3                           African Union                  —  \n",
       "4  Association of Southeast Asian Nations                  —  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_country_list = pd.DataFrame.from_dict(wiki_dict)\n",
    "wikipedia_country_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate(country_name):\n",
    "    country_name = re.sub(r\"\\(.*\\)\",\"\", country_name) # Delete content in paranthesis since not relevant\n",
    "    if \",\" in country_name:\n",
    "        # If there is a comma, switch order to yield a more common abbreviation: Korea, Nord --> Nord Korea\n",
    "        matched = re.match(r\"([A-Za-z]*), (.*)\",country_name)\n",
    "        country_name = matched[2] + \" \" + matched[1] \n",
    "    abbreviation = None\n",
    "    if len(re.findall(r\"([A-Z|Ä|Ö|Ü])\",country_name)) > 1:\n",
    "        abbreviation = \"\".join(re.findall(r\"([A-Z|Ä|Ö|Ü])\",country_name))\n",
    "    return abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Search for names that might have abbreviations. If they consist of two or more words that start with a capital\n",
    "# letter, make an abbreviation out of it\n",
    "abb_state_de = list(map(abbreviate,wikipedia_country_list[\"state_name_de\"].tolist()))\n",
    "abb_full_state_de = list(map(abbreviate,wikipedia_country_list[\"full_state_name_de\"].tolist()))\n",
    "abb_state_trans = list(map(abbreviate,wikipedia_country_list[\"translation_state_name\"].tolist()))\n",
    "                   \n",
    "abbreviations = [list(a) for a in zip(abb_state_de,abb_full_state_de,abb_state_trans)]\n",
    "abbreviations = [list(filter(None,abb)) for abb in abbreviations if str(abb) != 'None'] # Removes Nones\n",
    "abbreviations = list(map(lambda x:list(set(x)) if len(x)>1 else \"-\", abbreviations)) # Removes redundance\n",
    "#abbreviations = list(map(\", \".join,cleaned_abbreviations)) # Unpack list of abbreviations to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name_de</th>\n",
       "      <th>full_state_name_de</th>\n",
       "      <th>capital_de</th>\n",
       "      <th>translation_state_name</th>\n",
       "      <th>wiki_abbreviations</th>\n",
       "      <th>inoff_abbreviations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erde</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>Earth</td>\n",
       "      <td>—</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Europäische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Brüssel</td>\n",
       "      <td>European Union</td>\n",
       "      <td>—</td>\n",
       "      <td>[EU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Union Südamerikanischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Union of South American Nations</td>\n",
       "      <td>—</td>\n",
       "      <td>[USN, USAN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afrikanische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Addis Abeba</td>\n",
       "      <td>African Union</td>\n",
       "      <td>—</td>\n",
       "      <td>[AU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Verband Südostasiatischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Association of Southeast Asian Nations</td>\n",
       "      <td>—</td>\n",
       "      <td>[VSN, ASAN]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        state_name_de full_state_name_de   capital_de  \\\n",
       "0                                Erde                  —            —   \n",
       "1                   Europäische Union                  —      Brüssel   \n",
       "2    Union Südamerikanischer Nationen                  —        Quito   \n",
       "3                  Afrikanische Union                  —  Addis Abeba   \n",
       "4  Verband Südostasiatischer Nationen                  —      Jakarta   \n",
       "\n",
       "                   translation_state_name wiki_abbreviations  \\\n",
       "0                                   Earth                  —   \n",
       "1                          European Union                  —   \n",
       "2         Union of South American Nations                  —   \n",
       "3                           African Union                  —   \n",
       "4  Association of Southeast Asian Nations                  —   \n",
       "\n",
       "  inoff_abbreviations  \n",
       "0                   -  \n",
       "1                [EU]  \n",
       "2         [USN, USAN]  \n",
       "3                [AU]  \n",
       "4         [VSN, ASAN]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_country_list[\"inoff_abbreviations\"] =  abbreviations\n",
    "wikipedia_country_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ontology/Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epitator.annotator import AnnoDoc\n",
    "from epitator.geoname_annotator import location_contains\n",
    "doc = AnnoDoc(\"Where is America?\")\n",
    "doc.add_tiers(GeonameAnnotator())\n",
    "annotations = doc.tiers[\"geonames\"]\n",
    "geoname = annotations[0]\n",
    "geoname.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entries(countries):\n",
    "    \"\"\"Takes a list of countries and returns a set of cleaned country names\"\"\"\n",
    "    countries_unique = list(set(countries)) # Optional. Used for better overview\n",
    "    countries_unique = list(map(lambda x: re.sub(r'\\n',', ',x), countries_unique)) # Because someone used new lines in entries\n",
    "    countries_unique = list(map(lambda x: re.sub(r',,',',',x), countries_unique)) # Because the line above adds one comma to much\n",
    "    countries_unique = list(map(lambda x: re.sub(r'\\(.*\\)',\"\",x).strip(\" \"), countries_unique))\n",
    "    countries_unique = list(map(lambda x: x.replace(\"&\", \"und\"), countries_unique))\n",
    "    countries_unique = list(map(lambda x: x.split(\",\") if \",\" in x else x, countries_unique)) # For entries with more than one country\n",
    "    countries_unique = list(map(lambda x: x.replace(\"_\",\" \") if type(x) != list else x,countries_unique))\n",
    "    # To clean s.t. like DRCongo or VAE Dubai\n",
    "#     countries_unique = list(map(lambda x: re.match(r\"([A-Z]{2,}\\W?)*\",x)[1].replace(\" \",\"\")\\\n",
    "#                                 if type(x) != list and  re.match(r\"([A-Z]{2,}\\W?)*\",x)[1]\\\n",
    "#                                 else x, countries_unique))\n",
    "    countries_unique = list(map(lambda x: clean_entries(x) if type(x) == list else x,countries_unique))\n",
    "    return countries_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST\n",
    "from deep_eq import deep_eq\n",
    "clean_entries(countries)\n",
    "example_countries_to_clean = [\" Australien\",\n",
    "                              \"Kongo \\nUSA\",\n",
    "                              \"Italien, Deutschland, Belgien \",\n",
    "                              \"Franz._Polynesien\", \n",
    "                              \"Trinidad & Tobago\"]\n",
    "deep_eq(clean_entries(example_countries_to_clean),[\"Trinidad und Tobago\",\"Australien\",['Belgien', 'Deutschland', 'Italien'], [\"USA\", \"Kongo\"], \"Franz. Polynesien\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING. RETURNS TUPLE WITH ABBREVIATION AND TRANSLATION\n",
    "# # Takes a list of not matched/translated entries and tries to match them to the wikipedia table and find the full name\n",
    "# countries_not_translated = [entry for entry in countries_unique \\\n",
    "#                             if entry not in wikipedia_country_list[\"state_name_de\"].tolist()]\n",
    "# def translate_abbreviation(to_translate):\n",
    "#     abb_to_country = []\n",
    "#     if type(to_translate) == str:\n",
    "#         to_translate = [to_translate]\n",
    "#     for column in [\"wiki_abbreviations\",\"inoff_abbreviations\"]:\n",
    "#         for potential_abbreviation in to_translate:\n",
    "#             if type(potential_abbreviation) == str:\n",
    "#                 for i, abbreviation in enumerate(wikipedia_country_list[column]):\n",
    "#                     if potential_abbreviation in abbreviation:\n",
    "#                         abb_to_country.append((potential_abbreviation,\\\n",
    "#                                                wikipedia_country_list[\"translation_state_name\"].tolist()[i]))\n",
    "#                         to_translate.remove(potential_abbreviation)\n",
    "#             elif type(potential_abbreviation) == list:\n",
    "#                 abb_to_country.append(translate_abbreviation(potential_abbreviation))\n",
    "#     return(abb_to_country,to_translate)\n",
    "\n",
    "# abbreviation_tuple, countries_not_translated = translate_abbreviation(countries_not_translated)\n",
    "\n",
    "# #abbreviation_tuple\n",
    "# #print(\"****************************************************************************************************************\")\n",
    "# #print(countries_not_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_abbreviation(to_translate):\n",
    "    # If not list but single abbreviation, transform into list\n",
    "    if type(to_translate) == str:\n",
    "        to_translate = [to_translate]\n",
    "    for potential_abbreviation in to_translate:\n",
    "        if type(potential_abbreviation) == str and re.findall(r\"([A-Z]{2,})\",potential_abbreviation):\n",
    "            to_delete = potential_abbreviation\n",
    "            potential_abbreviation = re.match(r\"([A-Z]\\W?)*\",potential_abbreviation)\n",
    "            potential_abbreviation = potential_abbreviation[0].replace(\" \",\"\")\n",
    "            for column in [\"wiki_abbreviations\",\"inoff_abbreviations\"]:\n",
    "                for i, abbreviation in enumerate(wikipedia_country_list[column]):\n",
    "                    if potential_abbreviation in abbreviation:\n",
    "                        to_translate.append(wikipedia_country_list[\"state_name_de\"].tolist()[i])\n",
    "                        to_translate.remove(to_delete)\n",
    "        elif type(potential_abbreviation) == list: # and not all(isinstance(i, list) for i in potential_abbreviation):\n",
    "            list_entry = [translate_abbreviation(nested_entry) for nested_entry in potential_abbreviation]\n",
    "            flattened = [entry for sublist in list_entry for entry in sublist]\n",
    "            to_translate.remove(potential_abbreviation)\n",
    "            to_translate.append(flattened)\n",
    "    return(to_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING WHILE LOOPING IS DIFFICULT\n",
    "def how(to_translate):\n",
    "    to_remove = []\n",
    "    if type(to_translate) == str:\n",
    "        to_translate = [to_translate]\n",
    "    for potential_abbreviation in to_translate:\n",
    "        if type(potential_abbreviation) == str and re.findall(r\"([A-Z]{2,})\",potential_abbreviation):\n",
    "            not_found = False\n",
    "            for column in [\"wiki_abbreviations\",\"inoff_abbreviations\"]:\n",
    "                for i, abbreviation in enumerate(wikipedia_country_list[column]):\n",
    "                    if potential_abbreviation in abbreviation:\n",
    "                        to_translate.append(wikipedia_country_list[\"state_name_de\"].tolist()[i])\n",
    "                        to_remove.append(potential_abbreviation)\n",
    "    return [entry for entry in to_translate if entry not in to_remove]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = [\"USA\",\"VAE\",'asdas', \"DR Cong\"]\n",
    "how(ab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_abbreviation = re.match(r\"([A-Z]\\W?)*\",\"DRCongo\")\n",
    "a = potential_abbreviation[0].replace(\" \",\"\")\n",
    "a in \"DRC a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SIMPLE TRANSLATION. FAST BUT DOES NOT TRANSLATE LISTS OF LISTS\n",
    "# # Translate German entries of Ereignisdatenbank to English. Might be inefficient since I go through the wiki list\n",
    "# # entirely which is longer then the list of countries to translate\n",
    "# translated_ereignisdatenbank_countries = [(entry,wikipedia_country_list[\"translation_state_name\"].tolist()[indx])\\\n",
    "#                                           for indx,entry \\\n",
    "#                                           in enumerate(wikipedia_country_list[\"state_name_de\"].tolist())\\\n",
    "#                                           if entry in countries_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate German entries of Ereignisdatenbank to English. Might be inefficient since I go through the wiki list\n",
    "# entirely which is longer then the list of countries to translate\n",
    "def translate(countries_unique):\n",
    "    translated_ereignisdatenbank_countries = []\n",
    "    state_name_de = wikipedia_country_list[\"state_name_de\"].tolist()\n",
    "    translation = wikipedia_country_list[\"translation_state_name\"].tolist()\n",
    "    \n",
    "    for entry in countries_unique:\n",
    "        if entry in state_name_de:\n",
    "            # Tuple of entry and translation\n",
    "            translated_ereignisdatenbank_countries.append((entry,translation[state_name_de.index(entry)]))\n",
    "        elif type(entry) == list:\n",
    "            # TODO: Abbreviations not found in other function shall now be translated\n",
    "            #found_abbreviations = list(filter(re.compile(r\"([A-Z]{2,3})\").match,entry))\n",
    "            #translated_abbreviations = translate_abbreviation(found_abbreviations)\n",
    "            \n",
    "            translated_ereignisdatenbank_countries.append(translate(clean_entries(entry)))\n",
    "    return translated_ereignisdatenbank_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(translate_abbreviation(countries_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate([\"United States\",\"Delaware\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_not_translated = [entry for entry in countries_unique \\\n",
    "                            if entry not in wikipedia_country_list[\"state_name_de\"].tolist()\\\n",
    "                           and type(entry) != list]\n",
    "countries_not_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_abbreviation([\"USA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = search_abbreviation_in_wiki(countries_and_abb_not_found)\n",
    "print(a[\"found\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for not_found in countries_not_found:\n",
    "    for poss_match in wikipedia_country_list[\"state_name_de\"].tolist():\n",
    "        if not_found in poss_match:\n",
    "            print(not_found, poss_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unterstanding geoname annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Geoname Annotator\"\"\"\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "from .maximum_weight_interval_set import Interval, find_maximum_weight_interval_set\n",
    "\n",
    "# Containment levels indicate which properties must match when determing\n",
    "# whether a geoname of a given containment level contains another geoname.\n",
    "# The admin codes generally correspond to states, provinces and cities.\n",
    "CONTAINMENT_LEVELS = [\n",
    "    'country_code',\n",
    "    'admin1_code',\n",
    "    'admin2_code',\n",
    "    'admin3_code',\n",
    "    'admin4_code'\n",
    "]\n",
    "\n",
    "GEONAME_ATTRS = [\n",
    "    'geonameid',\n",
    "    'name',\n",
    "    'feature_code',\n",
    "    'country_code',\n",
    "    'admin1_code',\n",
    "    'admin2_code',\n",
    "    'admin3_code',\n",
    "    'admin4_code',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'population',\n",
    "    'asciiname',\n",
    "    'names_used',\n",
    "    'name_count']\n",
    "def location_contains(loc_outer, loc_inner):\n",
    "    \"\"\"\n",
    "    Do a comparison to see if the first geoname contains the second.\n",
    "    It returns an integer to indicate the level of containment.\n",
    "    0 indicates no containment. Siblings locations and identical locations\n",
    "    have 0 containment. The level of containment is determined by the specificty\n",
    "    of the outer location. e.g. USA would be a smaller number than Texas.\n",
    "    In order for containment to be detected the outer location must have a\n",
    "    ADM* or PCL* feature code, which is most countries, states, and districts.\n",
    "    \"\"\"\n",
    "    # Test the country code in advance for efficiency. The country code must match for\n",
    "    # any level of containment.\n",
    "    if loc_outer.country_code != loc_inner.country_code or loc_outer.country_code == '':\n",
    "        return 0\n",
    "    feature_code = loc_outer.feature_code\n",
    "    if feature_code == 'ADM1':\n",
    "        outer_feature_level = 2\n",
    "    elif feature_code == 'ADM2':\n",
    "        outer_feature_level = 3\n",
    "    elif feature_code == 'ADM3':\n",
    "        outer_feature_level = 4\n",
    "    elif feature_code == 'ADM4':\n",
    "        outer_feature_level = 5\n",
    "    elif re.match(\"^PCL.\", feature_code):\n",
    "        outer_feature_level = 1\n",
    "    else:\n",
    "        return 0\n",
    "    for prop in CONTAINMENT_LEVELS[1:outer_feature_level]:\n",
    "        if loc_outer[prop] == '':\n",
    "            return 0\n",
    "        if loc_outer[prop] != loc_inner[prop]:\n",
    "            return 0\n",
    "    if loc_outer.geonameid == loc_inner.geonameid:\n",
    "        return 0\n",
    "    return outer_feature_level\n",
    "\n",
    "\n",
    "\n",
    "GEONAME_ATTRS = [\n",
    "    'geonameid',\n",
    "    'name',\n",
    "    'feature_code',\n",
    "    'country_code',\n",
    "    'admin1_code',\n",
    "    'admin2_code',\n",
    "    'admin3_code',\n",
    "    'admin4_code',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'population',\n",
    "    'asciiname',\n",
    "    'names_used',\n",
    "    'name_count']\n",
    "\n",
    "\n",
    "ADMINNAME_ATTRS = [\n",
    "    'country_name',\n",
    "    'admin1_name',\n",
    "    'admin2_name',\n",
    "    'admin3_name']\n",
    "\n",
    "\n",
    "class GeonameRow(object):\n",
    "    __slots__ = GEONAME_ATTRS + ADMINNAME_ATTRS + [\n",
    "        'alternate_locations',\n",
    "        'spans',\n",
    "        'parents',\n",
    "        'score',\n",
    "        'lat_long',\n",
    "        'high_confidence']\n",
    "\n",
    "    def __init__(self, sqlite3_row):\n",
    "        for key in sqlite3_row.keys():\n",
    "            if key in GEONAME_ATTRS:\n",
    "                setattr(self, key, sqlite3_row[key])\n",
    "        self.lat_long = (self.latitude, self.longitude,)\n",
    "        self.alternate_locations = set()\n",
    "        self.spans = set()\n",
    "        self.parents = set()\n",
    "        self.score = None\n",
    "\n",
    "\n",
    "\n",
    "    def to_dict(self):\n",
    "        result = {}\n",
    "        for key in GEONAME_ATTRS:\n",
    "            result[key] = self[key]\n",
    "        for key in ADMINNAME_ATTRS:\n",
    "            if hasattr(self, key):\n",
    "                result[key] = self[key]\n",
    "        result['parents'] = [p.to_dict() for p in self.parents]\n",
    "        result['score'] = self.score\n",
    "        return result\n",
    "\n",
    "\n",
    "class GeonameFeatures(object):\n",
    "    \"\"\"\n",
    "    This represents the aspects of a condidate geoname that are used to\n",
    "    determine whether it is being referenced.\n",
    "    \"\"\"\n",
    "    # The feature name array is used to maintain the order of the\n",
    "    # values in the feature vector.\n",
    "    feature_names = [\n",
    "        'log_population',\n",
    "        'name_count',\n",
    "        'num_spans',\n",
    "        'max_span_length',\n",
    "        'cannonical_name_used',\n",
    "        'loc_NE_portion',\n",
    "        'other_NE_portion',\n",
    "        'noun_portion',\n",
    "        'other_pos_portion',\n",
    "        'num_tokens',\n",
    "        'ambiguity',\n",
    "        'PPL_feature_code',\n",
    "        'ADM_feature_code',\n",
    "        'CONT_feature_code',\n",
    "        'other_feature_code',\n",
    "        'combined_span_parents',\n",
    "        'close_locations',\n",
    "        'very_close_locations',\n",
    "        'containing_locations',\n",
    "        'max_containment_level',\n",
    "        # high_confidence indicates the base feature set received a high score.\n",
    "        # It is an useful feature for preventing high confidence geonames\n",
    "        # from receiving low final scores when they lack contextual cues -\n",
    "        # for example, when they are the only location mentioned.\n",
    "        'high_confidence',\n",
    "    ]\n",
    "\n",
    "    def __init__(self, geoname, spans_to_nes, span_to_tokens):\n",
    "        self.geoname = geoname\n",
    "        # The set of geonames that are mentioned in proximity to the spans\n",
    "        # corresponding to this feature.\n",
    "        # This will be populated by the add_contextual_features function.\n",
    "        self.nearby_mentions = set()\n",
    "        d = {}\n",
    "        d['log_population'] = math.log(geoname.population + 1)\n",
    "        # Geonames with lots of alternate names\n",
    "        # tend to be the ones most commonly referred to.\n",
    "        d['name_count'] = geoname.name_count\n",
    "        d['num_spans'] = len(geoname.spans)\n",
    "        d['max_span_length'] = max([\n",
    "            len(span.text) for span in geoname.spans])\n",
    "\n",
    "        def cannonical_name_match(span, geoname):\n",
    "            first_leaf = next(span.iterate_leaf_base_spans(), None)\n",
    "            if first_leaf:\n",
    "                span_text = first_leaf.text\n",
    "            else:\n",
    "                span_text = span.text\n",
    "            span_in_name = span_text in geoname.name or span_text in geoname.asciiname\n",
    "            return (float(len(span_text)) if span_in_name else 0) / len(geoname.name)\n",
    "        d['cannonical_name_used'] = max([\n",
    "            cannonical_name_match(span, geoname)\n",
    "            for span in geoname.spans\n",
    "        ])\n",
    "        loc_NEs_overlap = 0\n",
    "        other_NEs_overlap = 0\n",
    "        total_spans = len(geoname.spans)\n",
    "        for span in geoname.spans:\n",
    "            for ne_span in spans_to_nes[span]:\n",
    "                if ne_span.label == 'GPE' or ne_span.label == 'LOC':\n",
    "                    loc_NEs_overlap += 1\n",
    "                else:\n",
    "                    other_NEs_overlap += 1\n",
    "        d['loc_NE_portion'] = float(loc_NEs_overlap) / total_spans\n",
    "        d['other_NE_portion'] = float(other_NEs_overlap) / total_spans\n",
    "        noun_pos_tags = 0\n",
    "        other_pos_tags = 0\n",
    "        pos_tags = 0\n",
    "        for span in geoname.spans:\n",
    "            for token_span in span_to_tokens[span]:\n",
    "                token = token_span.token\n",
    "                pos_tags += 1\n",
    "                if token.tag_.startswith(\"NN\") or token.tag_ == \"FW\":\n",
    "                    noun_pos_tags += 1\n",
    "                else:\n",
    "                    other_pos_tags += 1\n",
    "        d['combined_span_parents'] = len(geoname.parents)\n",
    "        d['noun_portion'] = float(noun_pos_tags) / pos_tags\n",
    "        d['other_pos_portion'] = float(other_pos_tags) / pos_tags\n",
    "        d['num_tokens'] = pos_tags\n",
    "        d['ambiguity'] = len(geoname.alternate_locations)\n",
    "        feature_code = geoname.feature_code\n",
    "        if feature_code.startswith('PPL'):\n",
    "            d['PPL_feature_code'] = 1\n",
    "        elif feature_code.startswith('ADM'):\n",
    "            d['ADM_feature_code'] = 1\n",
    "        elif feature_code.startswith('CONT'):\n",
    "            d['CONT_feature_code'] = 1\n",
    "        else:\n",
    "            d['other_feature_code'] = 1\n",
    "        self._values = [0] * len(self.feature_names)\n",
    "        self.set_values(d)\n",
    "\n",
    "    def set_value(self, feature_name, value):\n",
    "        self._values[self.feature_names.index(feature_name)] = value\n",
    "\n",
    "    def set_values(self, value_dict):\n",
    "        for idx, name in enumerate(self.feature_names):\n",
    "            if name in value_dict:\n",
    "                self._values[idx] = value_dict[name]\n",
    "\n",
    "    def set_contextual_features(self):\n",
    "        \"\"\"\n",
    "        GeonameFeatures are initialized with only values that can be extracted\n",
    "        from the geoname database and span. This extends the GeonameFeature\n",
    "        with values that require information from nearby_mentions.\n",
    "        \"\"\"\n",
    "        geoname = self.geoname\n",
    "        close_locations = 0\n",
    "        very_close_locations = 0\n",
    "        containing_locations = 0\n",
    "        max_containment_level = 0\n",
    "        for recently_mentioned_geoname in self.nearby_mentions:\n",
    "            if recently_mentioned_geoname == geoname:\n",
    "                continue\n",
    "            containment_level = max(\n",
    "                location_contains(geoname, recently_mentioned_geoname),\n",
    "                location_contains(recently_mentioned_geoname, geoname))\n",
    "            if containment_level > 0:\n",
    "                containing_locations += 1\n",
    "            if containment_level > max_containment_level:\n",
    "                max_containment_level = containment_level\n",
    "            distance = great_circle(\n",
    "                recently_mentioned_geoname.lat_long, geoname.lat_long\n",
    "            ).kilometers\n",
    "            if distance < 400:\n",
    "                close_locations += 1\n",
    "            if distance < 100:\n",
    "                very_close_locations += 1\n",
    "        self.set_values(dict(\n",
    "            close_locations=close_locations,\n",
    "            very_close_locations=very_close_locations,\n",
    "            containing_locations=containing_locations,\n",
    "            max_containment_level=max_containment_level))\n",
    "\n",
    "\n",
    "class GeonameAnnotator(Annotator):\n",
    "    def __init__(self, custom_classifier=None):\n",
    "        self.connection = get_database_connection()\n",
    "        self.connection.row_factory = sqlite3.Row\n",
    "        if custom_classifier:\n",
    "            self.geoname_classifier = custom_classifier\n",
    "        else:\n",
    "            self.geoname_classifier = geoname_classifier\n",
    "\n",
    "    def get_candidate_geonames(self, doc):\n",
    "        \"\"\"\n",
    "        Returns an array of geoname dicts correponding to locations that the\n",
    "        document may refer to.\n",
    "        The dicts are extended with lists of associated AnnoSpans.\n",
    "        \"\"\"\n",
    "        if 'ngrams' not in doc.tiers:\n",
    "            doc.add_tiers(NgramAnnotator())\n",
    "        logger.info('Ngrams annotated')\n",
    "        if 'nes' not in doc.tiers:\n",
    "            doc.add_tiers(NEAnnotator())\n",
    "        logger.info('Named entities annotated')\n",
    "\n",
    "        all_ngrams = list(set([span.text.lower()\n",
    "                               for span in doc.tiers['ngrams'].spans\n",
    "                               if is_possible_geoname(span.text)\n",
    "                               ]))\n",
    "        cursor = self.connection.cursor()\n",
    "        geoname_results = list(cursor.execute('''\n",
    "        SELECT\n",
    "            geonames.*,\n",
    "            count AS name_count,\n",
    "            group_concat(alternatename, \";\") AS names_used\n",
    "        FROM geonames\n",
    "        JOIN alternatename_counts USING ( geonameid )\n",
    "        JOIN alternatenames USING ( geonameid )\n",
    "        WHERE alternatename_lemmatized IN\n",
    "        (''' + ','.join('?' for x in all_ngrams) + ''')\n",
    "        GROUP BY geonameid''', all_ngrams))\n",
    "        logger.info('%s geonames fetched' % len(geoname_results))\n",
    "        geoname_results = [GeonameRow(g) for g in geoname_results]\n",
    "        # Associate spans with the geonames.\n",
    "        # This is done up front so span information can be used in the scoring\n",
    "        # function\n",
    "        span_text_to_spans = defaultdict(list)\n",
    "        for span in doc.tiers['ngrams'].spans:\n",
    "            if is_possible_geoname(span.text):\n",
    "                span_text_to_spans[span.text.lower()].append(span)\n",
    "        candidate_geonames = []\n",
    "        for geoname in geoname_results:\n",
    "            geoname.add_spans(span_text_to_spans)\n",
    "            # In rare cases geonames may have no matching spans because\n",
    "            # sqlite unicode equivalency rules match geonames that use different\n",
    "            # characters the document spans used to query them.\n",
    "            # These geonames are ignored.\n",
    "            if len(geoname.spans) > 0:\n",
    "                candidate_geonames.append(geoname)\n",
    "        # Add combined spans to locations that are adjacent to a span linked to\n",
    "        # an administrative division. e.g. Seattle, WA\n",
    "        span_to_geonames = defaultdict(list)\n",
    "        for geoname in candidate_geonames:\n",
    "            for span in geoname.spans:\n",
    "                span_to_geonames[span].append(geoname)\n",
    "        geoname_spans = span_to_geonames.keys()\n",
    "        combined_spans = AnnoTier(geoname_spans).chains(at_least=2, at_most=4, max_dist=4)\n",
    "        for combined_span in combined_spans:\n",
    "            leaf_spans = combined_span.iterate_leaf_base_spans()\n",
    "            first_spans = next(leaf_spans)\n",
    "            potential_geonames = {geoname: set()\n",
    "                                  for geoname in span_to_geonames[first_spans]}\n",
    "            for leaf_span in leaf_spans:\n",
    "                leaf_span_geonames = span_to_geonames[leaf_span]\n",
    "                next_potential_geonames = defaultdict(set)\n",
    "                for potential_geoname, prev_containing_geonames in potential_geonames.items():\n",
    "                    containing_geonames = [\n",
    "                        containing_geoname\n",
    "                        for containing_geoname in leaf_span_geonames\n",
    "                        if location_contains(containing_geoname, potential_geoname) > 0]\n",
    "                    if len(containing_geonames) > 0:\n",
    "                        next_potential_geonames[potential_geoname] |= prev_containing_geonames | set(containing_geonames)\n",
    "                potential_geonames = next_potential_geonames\n",
    "            for geoname, containing_geonames in potential_geonames.items():\n",
    "                geoname.spans.add(combined_span)\n",
    "                geoname.parents |= containing_geonames\n",
    "        # Replace individual spans with combined spans.\n",
    "        span_to_geonames = defaultdict(list)\n",
    "        for geoname in candidate_geonames:\n",
    "            geoname.spans = set(AnnoTier(geoname.spans).optimal_span_set().spans)\n",
    "            for span in geoname.spans:\n",
    "                span_to_geonames[span].append(geoname)\n",
    "        # Find locations with overlapping spans\n",
    "        # Note that is is possible for two valid locations to have\n",
    "        # overlapping names. For example, Harare Province has\n",
    "        # Harare as an alternate name, so the city Harare is very\n",
    "        # likely to be an alternate location that competes with it.\n",
    "        for span, geonames in span_to_geonames.items():\n",
    "            geoname_set = set(geonames)\n",
    "            for geoname in geonames:\n",
    "                geoname.alternate_locations |= geoname_set\n",
    "        for geoname in candidate_geonames:\n",
    "            geoname.alternate_locations -= set([geoname])\n",
    "        logger.info('%s alternative locations found' % sum([\n",
    "            len(geoname.alternate_locations)\n",
    "            for geoname in candidate_geonames]))\n",
    "        logger.info('%s candidate locations prepared' %\n",
    "                    len(candidate_geonames))\n",
    "        return candidate_geonames\n",
    "\n",
    "   \n",
    "\n",
    "    def add_contextual_features(self, features):\n",
    "        \"\"\"\n",
    "        Extend a list of features with values that are based on the geonames\n",
    "        mentioned nearby.\n",
    "        \"\"\"\n",
    "        logger.info('adding contextual features')\n",
    "        span_to_features = defaultdict(list)\n",
    "        for feature in features:\n",
    "            for span in feature.geoname.spans:\n",
    "                span_to_features[span].append(feature)\n",
    "        geoname_span_tier = AnnoTier(list(span_to_features.keys()))\n",
    "\n",
    "\n",
    "\n",
    "    def annotate(self, doc):\n",
    "        logger.info('geoannotator started')\n",
    "        candidate_geonames = self.get_candidate_geonames(doc)\n",
    "        features = self.extract_features(candidate_geonames, doc)\n",
    "        if len(features) == 0:\n",
    "            doc.tiers['geonames'] = AnnoTier([])\n",
    "            return doc\n",
    "\n",
    "        scores = self.geoname_classifier.predict_proba_base([\n",
    "            list(f.values()) for f in features])\n",
    "        for geoname, feature, score in zip(candidate_geonames, features, scores):\n",
    "            geoname.high_confidence = float(\n",
    "                score[1]) > self.geoname_classifier.HIGH_CONFIDENCE_THRESHOLD\n",
    "            feature.set_value('high_confidence', geoname.high_confidence)\n",
    "        has_high_confidence_features = any(\n",
    "            [geoname.high_confidence for geoname in candidate_geonames])\n",
    "        if has_high_confidence_features:\n",
    "            self.add_contextual_features(features)\n",
    "            scores = self.geoname_classifier.predict_proba_contextual([\n",
    "                list(f.values()) for f in features])\n",
    "        for geoname, score in zip(candidate_geonames, scores):\n",
    "            geoname.score = float(score[1])\n",
    "        culled_geonames = [geoname\n",
    "                           for geoname in candidate_geonames\n",
    "                           if geoname.score > self.geoname_classifier.GEONAME_SCORE_THRESHOLD]\n",
    "        cursor = self.connection.cursor()\n",
    "        for geoname in culled_geonames:\n",
    "            geoname_results = list(cursor.execute('''\n",
    "                SELECT\n",
    "                    cc.name,\n",
    "                    a1.name,\n",
    "                    a2.name,\n",
    "                    a3.name\n",
    "                FROM adminnames a3\n",
    "                JOIN adminnames a2 ON (\n",
    "                    a2.country_code = a3.country_code AND\n",
    "                    a2.admin1_code = a3.admin1_code AND\n",
    "                    a2.admin2_code = a3.admin2_code AND\n",
    "                    a2.admin3_code = \"\" )\n",
    "                JOIN adminnames a1 ON (\n",
    "                    a1.country_code = a3.country_code AND\n",
    "                    a1.admin1_code = a3.admin1_code AND\n",
    "                    a1.admin2_code = \"\" AND\n",
    "                    a1.admin3_code = \"\" )\n",
    "                JOIN adminnames cc ON (\n",
    "                    cc.country_code = a3.country_code AND\n",
    "                    cc.admin1_code = \"00\" AND\n",
    "                    cc.admin2_code = \"\" AND\n",
    "                    cc.admin3_code = \"\" )\n",
    "                WHERE (a3.country_code = ? AND a3.admin1_code = ? AND a3.admin2_code = ? AND a3.admin3_code = ?)\n",
    "                ''', (\n",
    "                geoname.country_code or \"\",\n",
    "                geoname.admin1_code or \"\",\n",
    "                geoname.admin2_code or \"\",\n",
    "                geoname.admin3_code or \"\",)))\n",
    "            for result in geoname_results:\n",
    "                prev_val = None\n",
    "                for idx, attr in enumerate(['country_name', 'admin1_name', 'admin2_name', 'admin3_name']):\n",
    "                    val = result[idx]\n",
    "                    if val == prev_val:\n",
    "                        # Names are repeated for admin levels beyond that of\n",
    "                        # the geoname.\n",
    "                        break\n",
    "                    setattr(geoname, attr, val)\n",
    "                    prev_val = val\n",
    "        logger.info('admin names added')\n",
    "        geo_spans = []\n",
    "        for geoname in culled_geonames:\n",
    "            for span in geoname.spans:\n",
    "                geo_span = GeoSpan(\n",
    "                    span.start, span.end, doc, geoname)\n",
    "                geo_spans.append(geo_span)\n",
    "        culled_geospans = AnnoTier(geo_spans).optimal_span_set(prefer=lambda x: (x.size(), x.geoname.score,))\n",
    "        logger.info('overlapping geospans removed')\n",
    "        return {'geonames': culled_geospans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==abE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
