{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_md')\n",
    "from epitator.annotator import AnnoDoc\n",
    "from epitator.geoname_annotator import GeonameAnnotator\n",
    "from epitator.resolved_keyword_annotator import ResolvedKeywordAnnotator\n",
    "from epitator.count_annotator import CountAnnotator\n",
    "from epitator.date_annotator import DateAnnotator\n",
    "from boilerpipe.extract import Extractor\n",
    "from itertools import groupby\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import epitator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sys import stdout\n",
    "from time import sleep\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def get_links_by_year(list_of_years=None, proxies={'http': 'http://fw-bln.rki.local:8020'}):\n",
    "    \"\"\"Returns (all) the anual links of the WHO DONs \n",
    "    \n",
    "    list_of_years -- a list of years (YYYY format) you want to parse (default None)\n",
    "    proxies -- the proxy to use while scraping (default {'http': 'http://fw-bln.rki.local:8020'})\n",
    "    \"\"\"\n",
    "    page = requests.get('http://www.who.int/csr/don/archive/year/en/',proxies=proxies)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    archiv_years = soup.find('ul',attrs={'class':'list'})\n",
    "    years_links_html = archiv_years.find_all('a')\n",
    "    if list_of_years:\n",
    "        return ['http://www.who.int' + link.get('href') for link in years_links_html if any(year in link for year in list_of_years)]\n",
    "    else:\n",
    "        return ['http://www.who.int' + link.get('href') for link in years_links_html]\n",
    "    \n",
    "def get_links_per_year(years_links, list_of_months=None, proxies={'http': 'http://fw-bln.rki.local:8020'}):\n",
    "    '''Take a list of links to the annual archive and return a list of DON links of these years\n",
    "    \n",
    "    years_links -- a list of links of the anual archive to parse \n",
    "    list_of_months -- a list of months (MMM* format) you want to parse (default None)\n",
    "    proxies -- the proxy to use while scraping (default {'http': 'http://fw-bln.rki.local:8020'})\n",
    "    '''\n",
    "    all_links = []\n",
    "    \n",
    "    for year_link in years_links:\n",
    "        page_year = requests.get(year_link,proxies=proxies)\n",
    "        soup_year = BeautifulSoup(page_year.content,'html.parser')\n",
    "        archive_year = soup_year.find('ul',attrs={'class':'auto_archive'})\n",
    "        daily_links = ['http://www.who.int' + link.get('href') for link in archive_year.find_all('a')]\n",
    "        all_links.extend(daily_links)\n",
    "    \n",
    "    if list_of_months:\n",
    "        all_links = [link for link in all_links if any(month in link for month in map(lambda s:s.lower(),list_of_months))]\n",
    "    return all_links\n",
    "    \n",
    "headers = {\n",
    "    'User-Agent': 'Auss Abbood, www.rki.de',\n",
    "    'From': 'abbooda@rki.de'\n",
    "}\n",
    "\n",
    "def scrape(years=None,\n",
    "           months=None,\n",
    "           num_last_reports=None,\n",
    "           headers=None,\n",
    "           proxies={'http': 'http://fw-bln.rki.local:8020'}):\n",
    "    '''Scrapes the WHO DONs using the WHO DON scraping functions and returns the links to these DONs\n",
    "    \n",
    "    years -- a list of strings of years in the format YYYY to be scraped\n",
    "    months -- a list of strings of months in the format MMM* to be scraped\n",
    "    num_list_reports -- an integer to specify how many of the last reports should be scraped. \n",
    "    can be combined with the specification of year and/or month\n",
    "    headers -- use a header for scraping\n",
    "    proxies -- the proxy to use while scraping (default {'http': 'http://fw-bln.rki.local:8020'})\n",
    "    '''\n",
    "    years = get_links_by_year(list_of_years=years,proxies=proxies)\n",
    "    all_links = get_links_per_year(years,list_of_months=months,proxies=proxies)\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all the WHO DONs of the year 2018\n",
    "all_links = scrape(years=['2018'],proxies=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the main text of the given links\n",
    "from boilerpipe.extract import Extractor\n",
    "def extract(list_of_links):\n",
    "    '''Extracts the main content from a list of links and returns a list of texts (str)\n",
    "\n",
    "    list_of_links -- a list containing URLs of webpages to get the main content from\n",
    "    '''\n",
    "    return[Extractor(extractor='ArticleExtractor', url=url).getText().replace('\\n','') \\\n",
    "         for url in tqdm(list_of_links)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this shit (a.k.a annotate all the scraped WHO DONs)\n",
    "def create_annotated_database(texts,raw=False):\n",
    "    '''Given a list of texts (str) annotate and extract disease keywords, geonames, and dates and return\n",
    "    a dictonary of the text and the annotations\n",
    "    \n",
    "    texts -- a list of texts (str)\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    database = {\"text\":texts,\"date\":[],\"confirmed_cases\":[],\"keyword\":[],\"geoname\":[]}\n",
    "    for i,text in enumerate(tqdm(texts)):\n",
    "        try:\n",
    "            doc = annotate(text)\n",
    "            database[\"date\"].append(get_date(doc,raw))\n",
    "            database[\"confirmed_cases\"].append(get_cases(doc,raw))\n",
    "            database[\"keyword\"].append(get_keywords(doc,raw))\n",
    "            database[\"geoname\"].append(get_geonames(doc,raw))\n",
    "        except TypeError as e:\n",
    "            print(\"Type error in text({})\".format(i) + \": \" + str(e))\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(text):\n",
    "    ''' Returns an document annotated for dates, disease counts, diseases, and geoneames\n",
    "    \n",
    "    text -- a string to be annotated\n",
    "    '''\n",
    "    doc = AnnoDoc(text)\n",
    "    doc.add_tiers(GeonameAnnotator())\n",
    "    doc.add_tiers(ResolvedKeywordAnnotator())\n",
    "    doc.add_tiers(CountAnnotator())\n",
    "    doc.add_tiers(DateAnnotator())\n",
    "    return doc\n",
    "def get_geonames(doc,raw=False):\n",
    "    '''Returns the most occuring geographical entity in a annotated document\n",
    "    \n",
    "    doc -- an annotated string\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    if raw:\n",
    "        return [doc.tiers[\"geonames\"].spans[i].geoname[\"name\"] for i in range(len(doc.tiers[\"geonames\"].spans))]\n",
    "    else:\n",
    "        geonames = [doc.tiers[\"geonames\"].spans[i].geoname[\"name\"] for i in range(len(doc.tiers[\"geonames\"].spans))]\n",
    "        geoname_counts = [(key,len(list(group))) for key, group in groupby(sorted(geonames))]\n",
    "        geoname_ranking = sorted(geoname_counts,key=lambda x:x[1],reverse=True)\n",
    "        geoname_most_occure = [geoname[0] for geoname in geoname_ranking if geoname[1] == geoname_ranking[0][1]]\n",
    "        return geoname_most_occure\n",
    "def get_keywords(doc,raw=False):\n",
    "    '''Returns the most occuring disease entity in a annotated document\n",
    "    \n",
    "    doc -- an annotated string\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    if raw:\n",
    "        return [doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0]['entity']['label'] \\\n",
    "                for i in range(len(doc.tiers[\"resolved_keywords\"].spans)) \\\n",
    "               if doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0]['entity']['type'] \\\n",
    "                    == 'disease']\n",
    "                     \n",
    "    else:\n",
    "        keywords = [(doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0]['entity']['label'] \\\n",
    "                     ,doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0][\"weight\"]) \\\n",
    "                    for i in range(len(doc.tiers[\"resolved_keywords\"].spans)) \\\n",
    "                    if doc.tiers[\"resolved_keywords\"].spans[i].metadata[\"resolutions\"][0]['entity']['type'] \\\n",
    "                    == 'disease']\n",
    "\n",
    "        # Here I ignore the included weights and only considere the most occuring disease name\n",
    "        keywords_without_weight = [disease[0] for disease in keywords]\n",
    "        keyword_counts = [(key,len(list(group))) for key, group in groupby(sorted(keywords_without_weight))]\n",
    "        try:\n",
    "            keyword = max(keyword_counts,key=lambda x:x[1])\n",
    "        except ValueError:\n",
    "            keyword = np.nan\n",
    "        if type(keyword) is float:\n",
    "            return keyword\n",
    "        else:\n",
    "            return keyword[0] # Only returns the keyword, not the weight\n",
    "def get_cases(doc,raw=False):\n",
    "    '''Returns the disease counts with the attribute \"confirmed\" in a annotated document\n",
    "    \n",
    "    doc -- an annotated string\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    if raw:\n",
    "        return [doc.tiers[\"counts\"].spans[i].metadata['count'] for i in range(len(doc.tiers[\"counts\"].spans))]\n",
    "    else:\n",
    "        return [doc.tiers[\"counts\"].spans[i].metadata['count'] \\\n",
    "                for i in range(len(doc.tiers[\"counts\"].spans)) \\\n",
    "                if \"confirmed\" in doc.tiers[\"counts\"].spans[i].metadata['attributes']]\n",
    "def get_date(doc,raw=False):\n",
    "    '''Returns most mentioned date in a annotated document\n",
    "    \n",
    "    doc -- an annotated string\n",
    "    raw -- returns a not preprocessed annotation (Default False)\n",
    "    '''\n",
    "    dates = [doc.tiers[\"dates\"].spans[i].metadata[\"datetime_range\"][0].strftime(\"%Y-%m-%d\") \\\n",
    "             for i in range(len(doc.tiers[\"dates\"].spans))]\n",
    "    if raw:\n",
    "        return dates\n",
    "    else:\n",
    "        date_count_tuple = [(key,len(list(group))) for key, group in groupby(sorted(dates))]\n",
    "        try:\n",
    "            date = max(date_count_tuple,key=lambda x:x[1])\n",
    "        except ValueError:\n",
    "            date = np.nan\n",
    "        if type(date) is float:\n",
    "            return date\n",
    "        else:\n",
    "            return date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1e9a41bf774ff9be2a8d2686d079a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extracted = extract(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6318ddd5c31e4eebadd5a5395581046b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>confirmed_cases</th>\n",
       "      <th>keyword</th>\n",
       "      <th>geoname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ebola virus disease – Democratic Republic of t...</td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>[]</td>\n",
       "      <td>Ebola hemorrhagic fever</td>\n",
       "      <td>[Democratic Republic of the Congo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle East respiratory syndrome coronavirus (...</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>[2005]</td>\n",
       "      <td>Middle East respiratory syndrome</td>\n",
       "      <td>[Kingdom of Saudi Arabia, Middle East]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ebola virus disease – Democratic Republic of t...</td>\n",
       "      <td>2018-10-30</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Ebola hemorrhagic fever</td>\n",
       "      <td>[Democratic Republic of the Congo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Circulating vaccine-derived poliovirus type 2 ...</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>[]</td>\n",
       "      <td>poliomyelitis</td>\n",
       "      <td>[Federal Republic of Nigeria, Republic of Niger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ebola virus disease – Democratic Republic of t...</td>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>[27, 2]</td>\n",
       "      <td>Ebola hemorrhagic fever</td>\n",
       "      <td>[Democratic Republic of the Congo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        date  \\\n",
       "0  Ebola virus disease – Democratic Republic of t...  2018-11-06   \n",
       "1  Middle East respiratory syndrome coronavirus (...  2018-09-17   \n",
       "2  Ebola virus disease – Democratic Republic of t...  2018-10-30   \n",
       "3  Circulating vaccine-derived poliovirus type 2 ...  2018-01-01   \n",
       "4  Ebola virus disease – Democratic Republic of t...  2018-10-23   \n",
       "\n",
       "  confirmed_cases                           keyword  \\\n",
       "0              []           Ebola hemorrhagic fever   \n",
       "1          [2005]  Middle East respiratory syndrome   \n",
       "2          [1, 1]           Ebola hemorrhagic fever   \n",
       "3              []                     poliomyelitis   \n",
       "4         [27, 2]           Ebola hemorrhagic fever   \n",
       "\n",
       "                                            geoname  \n",
       "0                [Democratic Republic of the Congo]  \n",
       "1            [Kingdom of Saudi Arabia, Middle East]  \n",
       "2                [Democratic Republic of the Congo]  \n",
       "3  [Federal Republic of Nigeria, Republic of Niger]  \n",
       "4                [Democratic Republic of the Congo]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_whos_df = pd.DataFrame.from_dict(create_annotated_database(extracted))\n",
    "parsed_whos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ereignisdatenbank = pd.read_csv(\"Ereignisse_utf.csv\",sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
