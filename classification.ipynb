{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import inflect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nlp_surveillance.classifier import extract_sentence\n",
    "from nlp_surveillance.pipeline import ExtractSentencesAndLabel, RecommenderLabeling, RecommenderTierAnnotation\n",
    "from nlp_surveillance.classifier import summarize\n",
    "from utils.my_utils import split_list_and_distribute_to_new_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count/Date Classifier: Most Informative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = ExtractSentencesAndLabel('counts').data_output()\n",
    "df_sent['sentence']= df_sent['sentence'].apply(lambda x: list(set(x.split()) - set(stopwords.words('english'))))\n",
    "df_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = split_list_and_distribute_to_new_rows(df_sent, 'sentence')\n",
    "df_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "as_tuples = df_sent.apply(tuple, axis=1).tolist()\n",
    "as_tuples = [({'word':word}, label) for label, word in as_tuples]\n",
    "as_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = nltk.NaiveBayesClassifier.train(as_tuples)\n",
    "clf.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_date = ExtractSentencesAndLabel('dates').data_output()\n",
    "df_sent_date['sentence']= df_sent_date['sentence'].apply(lambda x: list(set(x.split()) - set(stopwords.words('english'))))\n",
    "df_sent_date = split_list_and_distribute_to_new_rows(df_sent_date, 'sentence')\n",
    "as_tuples_date = df_sent_date.apply(tuple, axis=1).tolist()\n",
    "as_tuples_date = [({'sent':word}, label) for label, word in as_tuples_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_date = nltk.NaiveBayesClassifier.train(as_tuples_date)\n",
    "clf_date.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommand = RecommenderLabeling().data_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiers = RecommenderTierAnnotation().data_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tiers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = pd.concat([df_recommand, df_tiers],axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf = df_clf.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_clf['counts'] = df_clf['counts'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf['counts'] = df_clf['counts'].apply(np.nanmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = inflect.engine()\n",
    "df_clf['counts'] = (df_clf['counts']\n",
    "                    .apply(np.log10)\n",
    "                    .apply(lambda x: max(x, 0))\n",
    "                    .apply(lambda x: engine.number_to_words(int(x)))\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clf['features'] = df_clf[['geoname', 'diseases', 'counts']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.feature_extraction.text as text\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = make_pipeline_imb(TfidfVectorizer(),\n",
    "                         ADASYN(),\n",
    "                         MultinomialNB())\n",
    "pipe2 = make_pipeline_imb(TfidfVectorizer(),\n",
    "                         ADASYN(),\n",
    "                         MultinomialNB())\n",
    "pipe3 = make_pipeline_imb(TfidfVectorizer(),\n",
    "                         MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count = to_train_d['counts'].replace(-np.inf, 0.).apply(int).apply(lambda x: p.number_to_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train_d['features'] = to_train_d['features'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = \\\n",
    "    ms.train_test_split(df['annotated'], df['label'], test_size=.2)\n",
    "y_balanced = compute_sample_weight(class_weight='balanced', y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe1.fit(X_train, y_train)\n",
    "# pipe2.fit(X_train, y_train)\n",
    "pipe3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_pred, y_train))  #  TEXT WITHOUT STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test, y_pred))  #  TEXT WITHOUT STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test, y_pred))  #  TEXT WITHOUT STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test, y_pred))  # RAW TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test, y_pred))  # COMBINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test, y_pred))  # DISEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test, y_pred))  # GEONAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test, y_pred))  # COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_balanced = y_balanced/min(y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_balanced = [int(np.ceil(i)) for i in y_balanced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [int(np.ceil(i)) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_balanced)\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test, y_pred))  #  weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = text.TfidfVectorizer()\n",
    "X = tf.fit_transform(to_train_d['geoname'])\n",
    "y = to_train_d['label'].apply(int)\n",
    "y_balanced = compute_sample_weight(class_weight='balanced', y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 100 * X.nnz / float(X.shape[0] * X.shape[1])\n",
    "print(f\"Each sample has ~{p:.2f}% non-zero features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = \\\n",
    "    ms.train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = ms.GridSearchCV(\n",
    "    nb.BernoulliNB(),\n",
    "    param_grid={'alpha': np.logspace(-2., 2., 50)})\n",
    "bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = bnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, predicted))\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first get the words corresponding to each feature\n",
    "names = np.asarray(tf.get_feature_names())\n",
    "# Next, we display the 50 words with the largest\n",
    "# coefficients.\n",
    "print(','.join(names[np.argsort(\n",
    "    bnb.best_estimator_.coef_[0, :])[::-1][:50]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._vocab, self._E = self._load_words()\n",
    "        \n",
    "    \n",
    "    def _load_words(self):\n",
    "        E = {}\n",
    "        vocab = []\n",
    "\n",
    "        with open('nlp_surveillance/glove.6B.50d.txt', 'r', encoding=\"utf8\") as file:\n",
    "            for i, line in enumerate(file):\n",
    "                l = line.split(' ')\n",
    "                if l[0].isalpha():\n",
    "                    v = [float(i) for i in l[1:]]\n",
    "                    E[l[0]] = np.array(v)\n",
    "                    vocab.append(l[0])\n",
    "        return np.array(vocab), E            \n",
    "\n",
    "    \n",
    "    def _get_word(self, v):\n",
    "        for i, emb in enumerate(self._E):\n",
    "            if np.array_equal(emb, v):\n",
    "                return self._vocab[i]\n",
    "        return None\n",
    "    \n",
    "    def _doc_mean(self, doc):\n",
    "        return np.mean(np.array([self._E[w.lower().strip()] for w in doc if w.lower().strip() in self._E]), axis=0)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([self._doc_mean(doc) for doc in X])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(clf, X_test, y_test):\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred[:,1])\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    \n",
    "def print_scores(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "#     print(classification.accuracy_score(y_test, y_pred.round(), normalize=False))\n",
    "#     print(confusion_matrix(y_pred,y_test))\n",
    "#     print('F1 score: {:3f}'.format(f1_score(y_test, y_pred.round())))\n",
    "#     print('AUC score: {:3f}'.format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/recommender/with_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16167717"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage(deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3077\n",
       "True      155\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['extracted_text'].values\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_transform(X, sample_size):\n",
    "    essays1 = X\n",
    "    tok_es1 = [word_tokenize(doc) for doc in essays1[:sample_size]]\n",
    "    met = MeanEmbeddingTransformer()\n",
    "    X_transform = met.fit_transform(tok_es1)\n",
    "    return X_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = tokenize_and_transform(X, 3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_embed.csv', X_transform, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = np.loadtxt('X_embed.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3232,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_resample,\n",
    "#                                                     y_resample, stratify=y_resample, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transform,\n",
    "                                                    y[:3200], random_state=0)\n",
    "ada = ADASYN(random_state=0)\n",
    "X_resample, y_resample = ada.fit_sample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomOverSampler(random_state=0)\n",
    "X_resample, y_resample = rus.fit_sample(X_transform, y[:X_transform.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "print_scores(lr, X_resample, y_resample, X_test, y_test)\n",
    "plot_roc(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "print_scores(knn, X_resample, y_resample, X_test, y_test)\n",
    "plot_roc(knn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier().fit(X_resample, y_resample)\n",
    "print_scores(rf, X_resample, y_resample, X_test, y_test)\n",
    "plot_roc(rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC().fit(X_resample, y_resample)\n",
    "print_scores(svc, X_resample, y_resample, X_test, y_test)\n",
    "plot_roc(svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC().fit(X_resample, y_resample)\n",
    "print_scores(svc, X_resample, y_resample, X_test, y_test)\n",
    "plot_roc(svc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier().fit(X_resample, y_resample)\n",
    "y_pred = dtc.predict(X_test)\n",
    "print_scores(dtc, X_resample, y_resample, X_test, y_test)\n",
    "plot_roc(dtc, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp = MLPClassifier(max_iter=2000, early_stopping=True).fit(X_resample, y_resample)\n",
    "print_scores(mlp, X_resample, y_resample, X_test, y_test)\n",
    "plot_roc(mlp, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp_r = MLPRegressor(max_iter=2000, early_stopping=True).fit(X_resample, y_resample)\n",
    "print_scores(mlp_r, X_resample, y_resample, X_test, y_test)\n",
    "plot_roc(mlp_r, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('mlp_clf.pkl', 'wb') as f:\n",
    "    pickle.dump(mlp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(LogisticRegression(), \n",
    "             param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1]}, scoring=\"roc_auc\", cv=4)\n",
    "gs = gs.fit(X_resample, y_resample)\n",
    "print(gs.best_params_)\n",
    "print('best score: {:3f}'.format(gs.best_score_))\n",
    "plot_roc(gs, X_resample, y_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "gs = GridSearchCV(LinearSVC(), \n",
    "             param_grid=param_grid, scoring=\"roc_auc\", cv=4)\n",
    "gs = gs.fit(X_resample, y_resample)\n",
    "print(gs.best_params_)\n",
    "print('best score: {:3f}'.format(gs.best_score_))\n",
    "plot_roc(gs, X_resample, y_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'activation': ['relu', 'logistic', 'tanh'],\n",
    "              'alpha': [0.0001, 0.001, 0.01],\n",
    "              'learning_rate': ['constant', 'invscaling', 'adaptive'], 'tol': [0.01]}\n",
    "gs = GridSearchCV(MLPClassifier(), \n",
    "             param_grid=param_grid, scoring=\"roc_auc\", cv=4)\n",
    "gs = gs.fit(X_transform, y[:3200])\n",
    "print(gs.best_params_)\n",
    "print('best score: {:3f}'.format(gs.best_score_))\n",
    "plot_roc(gs, X_resample, y_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform_cluster = KMeans(n_clusters=5).fit_transform(X_transform, y[:3200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resample_cluster, y_resample_cluster = rus.fit_sample(X_transform_cluster, y[:X_transform_cluster.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cluster, X_test_cluster, y_train_cluster, y_test_cluster = train_test_split(X_resample_cluster,\n",
    "                                                    y_resample_cluster, stratify=y_resample_cluster, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "print_scores(lr, X_train_cluster, y_train_cluster, X_test_cluster, y_test_cluster)\n",
    "plot_roc(lr, X_test_cluster, y_test_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform_pca = PCA().fit_transform(X_transform, y[:3200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resample_pca, y_resample_pca = rus.fit_sample(X_transform_pca, y[:X_transform_pca.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cluster, X_test_cluster, y_train_cluster, y_test_cluster = train_test_split(X_resample_cluster,\n",
    "                                                    y_resample_cluster, stratify=y_resample_cluster, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "print_scores(lr, X_train_cluster, y_train_cluster, X_test_cluster, y_test_cluster)\n",
    "plot_roc(lr, X_test_cluster, y_test_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sne = 3200\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=10, n_iter=500)\n",
    "tsne_results = tsne.fit_transform(X_transform)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_tsne = pd.DataFrame(tsne_results, columns=['x', 'y'])\n",
    "df_tsne['labels'] = y[:3200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "theme_set(theme_bw())\n",
    "(ggplot(df_tsne, aes(x='x', y='y', color='labels')) \n",
    " + geom_point()\n",
    " + xlab(\"t-SNE-x\") + ylab(\"t-SNE-y\") + ggtitle(\"doc embedding t-SNE\")\n",
    " + scale_color_manual(labels = (True, False), values = (\"pink\", \"purple\"))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_app.app import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize('https://www.who.int/csr/don/04-march-2019-mers-oman/en/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
