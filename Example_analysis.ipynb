{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from epitator.annotator import AnnoDoc\n",
    "from epitator.geoname_annotator import GeonameAnnotator\n",
    "from epitator.resolved_keyword_annotator import ResolvedKeywordAnnotator\n",
    "from epitator.count_annotator import CountAnnotator\n",
    "from epitator.date_annotator import DateAnnotator\n",
    "from boilerpipe.extract import Extractor\n",
    "from itertools import groupby\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import epitator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from who_scraper import *\n",
    "# Scrape all the WHO DONs of the year 2018\n",
    "all_links = scrape(years=['2018'],proxies=None)\n",
    "# Extract the main text of the given links\n",
    "from boilerpipe.extract import Extractor\n",
    "def extract(list_of_links):\n",
    "    \"\"\"Extracts the main content from a list of links and returns a list of texts (str)\n",
    "\n",
    "    list_of_links -- a list containing URLs of webpages to get the main content from\n",
    "    \"\"\"\n",
    "    if type(list_of_links) == str:\n",
    "        list_of_links = [list_of_links]\n",
    "    return[Extractor(extractor='ArticleExtractor', url=url).getText().replace('\\n','') \\\n",
    "         for url in tqdm(list_of_links)]\n",
    "parsed_whos_df = pd.DataFrame.from_dict(create_annotated_database(extract(all_links)))\n",
    "parsed_whos_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Ereignisdatenbank (incident report). From here not put into .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in with columns with sources only\n",
    "ereignisdatenbank = pd.read_csv(\"Ereignisse_utf8.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ereignisdatenbank.iloc[:,15:26] # Get only the columns mentioning sources\n",
    "sources = sources.dropna(how=\"all\").reset_index(drop=True) # Drop empty rows at the end\n",
    "sources.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sources.copy(deep=True) # Create a mask for filtering\n",
    "for column in sources.columns:\n",
    "    mask[column] = sources[column].str.contains('who',na=False) # Extract all the entries that have the word \"who\"\n",
    "    mask[column] = sources[column].str.contains('don',na=False) # and \"don\"\n",
    "indices = [i for i in range(len(mask)) if not mask.iloc[i].any()] \n",
    "sources_filtered = sources.drop(np.reshape(indices,(len(indices),))) # Drop all rows that don't mention \"who\" or \"don\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datenbank_clean import *\n",
    "sources_filtered = sources_filtered.apply(edb_to_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea which is not correct and not complete\n",
    "date_matches = {}\n",
    "for column in sources_filtered.columns[::-3]: # Use only the columns mentioning dates\n",
    "    date_matches[column] = [i for i in range(len(parsed_whos_df))\\\n",
    "                            if list(map(lambda x: str(x)[:-3],parsed_whos_df[\"date\"].tolist()))[i] \\\n",
    "                            in list(map(lambda x: str(x)[:-12],sources_filtered[column].tolist()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "indices_that_matched = list(set(itertools.chain(*date_matches.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ereignisdatenbank.iloc[sources_filtered.index.tolist(),[3,6,7,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_whos_df.iloc[indices_that_matched,1:].sort_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettify the link description\n",
    "link_description = [re.search(r'don/(.*)/en',all_links[i])[1]\\\n",
    "                    .replace('-', ' ',2).replace('-',', ',2).replace('-',' ')\\\n",
    "                    for i in range(len(all_links))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the most important columns\n",
    "compare = parsed_whos_df.iloc[:,[1,3,4]].copy()\n",
    "compare['link_description'] = pd.Series(link_description,index=compare.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare[\"date\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the links that were faulty during annotation\n",
    "to_check = compare[compare[\"keyword\"].isnull()==True].index.values\n",
    "links_to_check = np.asarray(all_links)[to_check] # Get the links that caused the bad annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_faulty_text = create_annotated_database(extract(links_to_check),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_df = pd.DataFrame.from_dict(annotated_faulty_text).iloc[:,1:]\n",
    "faulty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entries that had the occurence of a geographical entity more than once\n",
    "parsed_whos_df[parsed_whos_df[\"geoname\"].str.len()>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geo Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_names = [list(pycountry.countries)[i].name for i in range(len(pycountry.countries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(country_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geograpy\n",
    "url = 'http://www.bbc.com/news/world-europe-26919928'\n",
    "places = geograpy.get_place_context(url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goodnes Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_filtered = sources_filtered.fillna(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_from_sources_filtered = [sources_filtered[\"Link zur Quelle 1\"].iloc[i] \n",
    "                               if (\"pdf\" not in sources_filtered[\"Link zur Quelle 1\"].iloc[i]) \n",
    "                               and (sources_filtered[\"Link zur Quelle 1\"].iloc[i] != \"nan\") \n",
    "                               else sources_filtered[\"Link zur Quelle 2\"].iloc[i] \n",
    "                               for i in range(len(sources_filtered))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = extract(links_from_sources_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_links_ereignisdatenbank = create_annotated_database(extracted,raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(parsed_links_ereignisdatenbank)\n",
    "df = df.iloc[:,1:]\n",
    "df[\"links\"] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compare_from_ereignisdatenbank = ereignisdatenbank.iloc[sources_filtered.index.tolist()\\\n",
    "                                                           ,[3,6,7,8,10,11,12,13,14]].reset_index().drop(\"index\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHY DOES THIS NOT WORK\n",
    "to_compare_from_ereignisdatenbank[\"Warcheinlische FÃ¤lle\"].apply((lambda x: int(x) if not np.isnan(x) else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract(\"http://www.promedmail.org/post/5838919\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.concat([df, to_compare_from_ereignisdatenbank], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df[\"date\"] = comparison_df[\"date\"].astype(object) # To restore lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = ereignisdatenbank_to_timestamp(comparison_df)\n",
    "comparison_df[\"combined_dates\"] = comparison_df.iloc[:,7:9].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df[\"combined_counts\"] = comparison_df.iloc[:,[10,12]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = comparison_df.iloc[:,[0,1,2,3,5,13,14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = {\"date\":[],\"cases\":[]}\n",
    "for index, row in comparison_df.iterrows():\n",
    "    print(row['combined_counts'])\n",
    "    matches[\"date\"].append([any(date in epi_date  for epi_date in row[\"date\"]) for date in row['combined_dates']])\n",
    "    matches[\"cases\"].append([any(count in epi_case for epi_case in row[\"confirmed_cases\"]) for count in row['combined_counts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[\"cases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = ['2018-04-01', '2018-03-02', '2018-01-17', '2018-04-06', '2018-01-17', '2018-03-02', '2018-03-03', '2018-03-07', '2018-03-08', '2018-03-08', '2018-03-09', '2018-03-08', '2018-04-04', '2018-04-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erg = ['2018-01-17', 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[any(date in epi_date for epi_date in epi) for date in erg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
