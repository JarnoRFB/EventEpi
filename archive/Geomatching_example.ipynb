{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_md')\n",
    "from epitator.annotator import AnnoDoc\n",
    "from epitator.geoname_annotator import GeonameAnnotator\n",
    "from epitator.resolved_keyword_annotator import ResolvedKeywordAnnotator\n",
    "from epitator.count_annotator import CountAnnotator\n",
    "from epitator.date_annotator import DateAnnotator\n",
    "from boilerpipe.extract import Extractor\n",
    "from itertools import groupby\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import epitator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "type(pd.DataFrame({\"a\":[1],\"b\":[2]})) == pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Ereignisdatenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ereignisdatenbank = pd.read_csv(\"Ereignisse_utf8.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ereignisdatenbank.columns = list(map(lambda x:x.strip(\" \"),ereignisdatenbank.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_not_null = ereignisdatenbank[pd.notnull(ereignisdatenbank[\"Ausgangs- bzw. Ausbruchsland\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = countries_not_null[\"Ausgangs- bzw. Ausbruchsland\"].copy(deep=True)\n",
    "countries = list(map(lambda x:x.strip(\" \"),countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Afghanistan',\n",
       " 'Afghanistan,\\nDR Congo\\nNigeria\\nSomalia',\n",
       " 'Algerien',\n",
       " 'Angola',\n",
       " 'Australien',\n",
       " 'Bangladesch',\n",
       " 'Benin',\n",
       " 'Bolivien',\n",
       " 'Brasilien',\n",
       " 'Burundi',\n",
       " 'China',\n",
       " 'Costa Rica',\n",
       " 'DRC',\n",
       " 'DRCongo',\n",
       " 'Demokratische Republik Kongo',\n",
       " 'Deutschland',\n",
       " 'El Salvador',\n",
       " 'Fiji',\n",
       " 'Frankreich',\n",
       " 'Französich_Polynesien',\n",
       " 'Französiche Guyana',\n",
       " 'Französisch-Polynesien',\n",
       " 'Ghana',\n",
       " 'Haiti',\n",
       " 'Indien',\n",
       " 'Irak',\n",
       " 'Iran',\n",
       " 'Israel',\n",
       " 'Italien',\n",
       " 'Italien, Griechenland, Rumanien, Ungarn, Frankreich',\n",
       " 'Italien, Griechenland, Ungarn, Rumänien',\n",
       " 'Italien, Serbien, Griechenland, Rumänien, Ungarn, Frankreich, Kosovo, Albanien, Macedonien, Montenegro, Serbien, Türkei',\n",
       " 'Kamerun',\n",
       " 'Kanada',\n",
       " 'Kenia',\n",
       " 'Kolumbien',\n",
       " 'Kongo',\n",
       " 'Kroatien',\n",
       " 'Kuwait',\n",
       " 'La Reunion',\n",
       " 'Liberia',\n",
       " 'Madagaskar',\n",
       " 'Malawi',\n",
       " 'Mali',\n",
       " 'Mauretanien',\n",
       " 'Mosambik',\n",
       " 'Myanmar',\n",
       " 'Namibia',\n",
       " 'Niger',\n",
       " 'Nigeria',\n",
       " 'Nordeuropa',\n",
       " 'Oman',\n",
       " 'Pakistan',\n",
       " 'Papua-Neuguinea',\n",
       " 'Peru',\n",
       " 'Philippinen',\n",
       " 'Polen',\n",
       " 'Saudi-Arabien',\n",
       " 'Schweiz',\n",
       " 'Serbien, Italien, Griechenland, Ungarn,  Rumänien',\n",
       " 'Simbabwe',\n",
       " 'Somalia',\n",
       " 'Spanien',\n",
       " 'Sri Lanka',\n",
       " 'Sudan',\n",
       " 'Syrien',\n",
       " 'Süd Sudan',\n",
       " 'Südafrika',\n",
       " 'Südsudan',\n",
       " 'Taiwan',\n",
       " 'Tanzania',\n",
       " 'Trinidad & Tobago',\n",
       " 'Tschad',\n",
       " 'Tschechien',\n",
       " 'Typhus',\n",
       " 'UK',\n",
       " 'USA',\n",
       " 'USA, Delaware',\n",
       " 'Uganda',\n",
       " 'Ukraine',\n",
       " 'VAE',\n",
       " 'VAE Dubai',\n",
       " 'Venezuela',\n",
       " 'Vereinigte Arabische Emirate',\n",
       " 'Vereinigtes Königreich',\n",
       " 'Westafrika',\n",
       " 'Yemen',\n",
       " 'Zambia',\n",
       " 'Zentralafrikanische Republik',\n",
       " 'Äthiopien (AWD)'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the testing begin\n",
    "## Parser for Wikipedia table of countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://de.wikipedia.org/wiki/Liste_der_Staaten_der_Erde\")\n",
    "soup = BeautifulSoup(req.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_soup = soup.find(\"table\",class_=\"wikitable sortable zebra\").find(\"tbody\") # Find table with of all countries\n",
    "parsed_soup = parsed_soup.find_all(\"tr\") # Get entries of countries form table\n",
    "amount_countries = len(parsed_soup)\n",
    "parsed_soup = [parsed_soup[i].find_all('td') \\\n",
    "               for i in range(amount_countries)] # Extract table entries from country entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dict = {\"state_name_de\":[],\n",
    "             \"full_state_name_de\":[],\n",
    "             \"capital_de\":[],\"translation_state_name\":[],\n",
    "             \"wiki_abbreviations\":[]}\n",
    "dash = u\"\\u2014\" # Used dash for missing entry in Wikipedia table\n",
    "regex = re.compile(r\"\\[\\d*\\]\") # To remove footnotes in the names\n",
    "for i in range(amount_countries):\n",
    "    try:\n",
    "        state_name_de = regex.sub(\"\",(parsed_soup[i][0].text).replace(\"\\n\",\"\")\\\n",
    "                                  .replace(\"\\xad\",\"\")) # Remove soft hyphen used in Zentralafr. Rep.\n",
    "        \n",
    "        # Remove additional information that are note the state name\n",
    "        state_name_de = re.sub(r\"((mit)|(ohne)).*\",\"\",state_name_de) \n",
    "        wiki_dict[\"state_name_de\"].append(state_name_de) \n",
    "        \n",
    "        # Removes new lines\n",
    "        wiki_dict[\"full_state_name_de\"].append(regex.sub(\"\",parsed_soup[i][1].text).replace(\"\\n\",\"\")) \n",
    "        wiki_dict[\"capital_de\"].append(regex.sub(\"\",parsed_soup[i][2].text).replace(\"\\n\",\"\")) \n",
    "        wiki_dict[\"translation_state_name\"].append(regex.sub(\"\",parsed_soup[i][10].text).replace(\"\\n\",\"\"))\n",
    "        \n",
    "        # Also removes new lines. Column 7 and 8 are long and short official abbreviations for the countries\n",
    "        list_abbreviation = [parsed_soup[i][7].text.replace(\"\\n\",\"\"),parsed_soup[i][8].text.replace(\"\\n\",\"\")] \n",
    "        \n",
    "        # Remove empty abbrev. entries. E.g. Abchasien | [\"ABC\", \"-\"]| Abkhazia --> Abchasien |\"ABC\" | Abhkazia\n",
    "        list_abbreviation = list(filter(lambda x: x not in [\"\",dash],list_abbreviation)) \n",
    "        if len(list_abbreviation) > 1:\n",
    "            wiki_dict[\"wiki_abbreviations\"].append(list_abbreviation)\n",
    "        else:\n",
    "            # When after removal of empty entries no abbrev. remains, enter a single dash\n",
    "            wiki_dict[\"wiki_abbreviations\"].append(dash)\n",
    "    except IndexError as e: # Because header and footer are part of the table, soup opperations don't work\n",
    "        if i not in [0, 213]:\n",
    "            print(\"Entry {} failed unexpected because of {}\".format(i,e)) # Except that the first and last entry fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name_de</th>\n",
       "      <th>full_state_name_de</th>\n",
       "      <th>capital_de</th>\n",
       "      <th>translation_state_name</th>\n",
       "      <th>wiki_abbreviations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erde</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>Earth</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Europäische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Brüssel</td>\n",
       "      <td>European Union</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Union Südamerikanischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Union of South American Nations</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afrikanische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Addis Abeba</td>\n",
       "      <td>African Union</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Verband Südostasiatischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Association of Southeast Asian Nations</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        state_name_de full_state_name_de   capital_de  \\\n",
       "0                                Erde                  —            —   \n",
       "1                   Europäische Union                  —      Brüssel   \n",
       "2    Union Südamerikanischer Nationen                  —        Quito   \n",
       "3                  Afrikanische Union                  —  Addis Abeba   \n",
       "4  Verband Südostasiatischer Nationen                  —      Jakarta   \n",
       "\n",
       "                   translation_state_name wiki_abbreviations  \n",
       "0                                   Earth                  —  \n",
       "1                          European Union                  —  \n",
       "2         Union of South American Nations                  —  \n",
       "3                           African Union                  —  \n",
       "4  Association of Southeast Asian Nations                  —  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_country_list = pd.DataFrame.from_dict(wiki_dict)\n",
    "wikipedia_country_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate(country_name):\n",
    "    \"\"\"Abbreviates entries of list of country names\n",
    "    \n",
    "    Example: United Kingdom --> UK\n",
    "    \"\"\"\n",
    "    country_name = re.sub(r\"\\(.*\\)\",\"\", country_name) # Delete content in paranthesis since not relevant for abbrev.\n",
    "    if \",\" in country_name:\n",
    "        # If there is a comma, switch order to yield a more common abbreviation: Korea, Nord --> Nord Korea\n",
    "        matched = re.match(r\"([A-Za-z]*), (.*)\",country_name) # Extract capital letters\n",
    "        country_name = matched[2] + \" \" + matched[1] # Patch capital letters together\n",
    "    abbreviation = None\n",
    "    if len(re.findall(r\"([A-Z|Ä|Ö|Ü])\",country_name)) > 1:\n",
    "        abbreviation = \"\".join(re.findall(r\"([A-Z|Ä|Ö|Ü])\",country_name))\n",
    "    return abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Search for names that might have abbreviations. If they consist of two or more words that start with a capital\n",
    "# letter, it makes an abbreviation out of it\n",
    "abb_state_de = list(map(abbreviate,wikipedia_country_list[\"state_name_de\"].tolist()))\n",
    "abb_full_state_de = list(map(abbreviate,wikipedia_country_list[\"full_state_name_de\"].tolist()))\n",
    "abb_state_trans = list(map(abbreviate,wikipedia_country_list[\"translation_state_name\"].tolist()))\n",
    "                   \n",
    "abbreviations = [list(a) for a in zip(abb_state_de,abb_full_state_de,abb_state_trans)]\n",
    "abbreviations = [list(filter(None,abb)) for abb in abbreviations if str(abb) != 'None'] # Removes Nones\n",
    "abbreviations = list(map(lambda x: list(set(x)) if len(x)>0 else \"-\", abbreviations)) # Removes redundance\n",
    "#abbreviations = list(map(\", \".join,cleaned_abbreviations)) # Unpack list of abbreviations to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name_de</th>\n",
       "      <th>full_state_name_de</th>\n",
       "      <th>capital_de</th>\n",
       "      <th>translation_state_name</th>\n",
       "      <th>wiki_abbreviations</th>\n",
       "      <th>inoff_abbreviations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erde</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>Earth</td>\n",
       "      <td>—</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Europäische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Brüssel</td>\n",
       "      <td>European Union</td>\n",
       "      <td>—</td>\n",
       "      <td>[EU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Union Südamerikanischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Union of South American Nations</td>\n",
       "      <td>—</td>\n",
       "      <td>[USN, USAN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afrikanische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Addis Abeba</td>\n",
       "      <td>African Union</td>\n",
       "      <td>—</td>\n",
       "      <td>[AU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Verband Südostasiatischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Association of Southeast Asian Nations</td>\n",
       "      <td>—</td>\n",
       "      <td>[VSN, ASAN]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        state_name_de full_state_name_de   capital_de  \\\n",
       "0                                Erde                  —            —   \n",
       "1                   Europäische Union                  —      Brüssel   \n",
       "2    Union Südamerikanischer Nationen                  —        Quito   \n",
       "3                  Afrikanische Union                  —  Addis Abeba   \n",
       "4  Verband Südostasiatischer Nationen                  —      Jakarta   \n",
       "\n",
       "                   translation_state_name wiki_abbreviations  \\\n",
       "0                                   Earth                  —   \n",
       "1                          European Union                  —   \n",
       "2         Union of South American Nations                  —   \n",
       "3                           African Union                  —   \n",
       "4  Association of Southeast Asian Nations                  —   \n",
       "\n",
       "  inoff_abbreviations  \n",
       "0                   -  \n",
       "1                [EU]  \n",
       "2         [USN, USAN]  \n",
       "3                [AU]  \n",
       "4         [VSN, ASAN]  "
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_country_list[\"inoff_abbreviations\"] =  abbreviations\n",
    "wikipedia_country_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ontology/Comparison (Transformed to .py until here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'Munich',\n",
       " 'textOffsets': [[10, 15]],\n",
       " 'geoname': {'geonameid': '2867714',\n",
       "  'name': 'Munich',\n",
       "  'feature_code': 'PPLA',\n",
       "  'country_code': 'DE',\n",
       "  'admin1_code': '02',\n",
       "  'admin2_code': '091',\n",
       "  'admin3_code': '09162',\n",
       "  'admin4_code': '09162000',\n",
       "  'longitude': 11.57549,\n",
       "  'latitude': 48.13743,\n",
       "  'population': 1260391,\n",
       "  'asciiname': 'Munich',\n",
       "  'names_used': 'Munic',\n",
       "  'name_count': 88,\n",
       "  'country_name': 'Federal Republic of Germany',\n",
       "  'admin1_name': 'Bavaria',\n",
       "  'admin2_name': 'Upper Bavaria',\n",
       "  'admin3_name': 'Kreisfreie Stadt München',\n",
       "  'parents': [],\n",
       "  'score': 0.21453999698331166}}"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from epitator.annotator import AnnoDoc\n",
    "from epitator.geoname_annotator import location_contains\n",
    "doc = AnnoDoc(\"I live in Munic!\")\n",
    "doc.add_tiers(GeonameAnnotator())\n",
    "annotations = doc.tiers[\"geonames\"]\n",
    "geoname = annotations[0]\n",
    "geoname.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To .py and renamed to clean_country_names\n",
    "def clean_entries(countries):\n",
    "    \"\"\"Takes a list of countries (from Ereginsdatenbank) and returns a set of cleaned country names\"\"\"\n",
    "    card_dir = re.compile(r\"(Süd|Nord|West|Ost)\\s(\\S*)\") # Matches cardinal directions and the string after it\n",
    "    countries_unique = list(set(countries)) # Optional. Used for better overview and faster calculation\n",
    "    \n",
    "    # Because someone used new lines in entries instead of comma to list countries\n",
    "    countries_unique = list(map(lambda x: re.sub(r'\\n',', ',x), countries_unique))\n",
    "    \n",
    "    # Because the line above adds one comma to much\n",
    "    countries_unique = list(map(lambda x: re.sub(r',,',',',x), countries_unique)) \n",
    "    countries_unique = list(map(lambda x: re.sub(r'\\(.*\\)',\"\",x).strip(\" \"), countries_unique))\n",
    "    countries_unique = list(map(lambda x: x.replace(\"&\", \"und\"), countries_unique))\n",
    "    countries_unique = list(map(lambda x: x.split(\",\") if \",\" in x else x, countries_unique)) # For entries with more than one country\n",
    "    countries_unique = list(map(lambda x: x.replace(\"_\",\" \") if type(x) != list else x,countries_unique))\n",
    "    \n",
    "    # To transform Süd Sudan to Südsudan\n",
    "    try:\n",
    "        countries_unique = list(map(lambda x: card_dir.match(x)[1] + card_dir.match(x)[2].lower()\\\n",
    "                                    if type(x) != list and card_dir.match(x) else x, countries_unique ))\n",
    "    except IndexError:\n",
    "        print(card_dir.match, \" has a cardinal direction but is not of the form 'Süd Sudan'\")\n",
    "    \n",
    "    #\"Recursively\" clean lists\n",
    "    countries_unique = list(map(lambda x: clean_entries(x) if type(x) == list else x,countries_unique))\n",
    "    return countries_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test succesful\n"
     ]
    }
   ],
   "source": [
    "# Test for clean_entries()\n",
    "from deep_eq import deep_eq\n",
    "clean_entries(countries)\n",
    "example_countries_to_clean = [\" Australien\",\n",
    "                              \"Kongo \\nUSA\",\n",
    "                              \"Italien, Deutschland, Belgien \",\n",
    "                              \"Franz._Polynesien\", \n",
    "                              \"Trinidad & Tobago\"]\n",
    "expected_countries_to_clean = [\"Trinidad und Tobago\",\"Australien\"\n",
    "                               ,['Belgien', 'Deutschland', 'Italien']\n",
    "                               ,[\"USA\", \"Kongo\"], \"Franz. Polynesien\"]\n",
    "if deep_eq(clean_entries(example_countries_to_clean),expected_countries_to_clean):\n",
    "    print(\"Test succesful\")\n",
    "else:\n",
    "    print(\"Test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING. RETURNS TUPLE WITH ABBREVIATION AND TRANSLATION\n",
    "# # Takes a list of not matched/translated entries and tries to match them to the wikipedia table and find the full name\n",
    "# countries_not_translated = [entry for entry in countries_unique \\\n",
    "#                             if entry not in wikipedia_country_list[\"state_name_de\"].tolist()]\n",
    "# def translate_abbreviation(to_translate):\n",
    "#     abb_to_country = []\n",
    "#     if type(to_translate) == str:\n",
    "#         to_translate = [to_translate]\n",
    "#     for column in [\"wiki_abbreviations\",\"inoff_abbreviations\"]:\n",
    "#         for potential_abbreviation in to_translate:\n",
    "#             if type(potential_abbreviation) == str:\n",
    "#                 for i, abbreviation in enumerate(wikipedia_country_list[column]):\n",
    "#                     if potential_abbreviation in abbreviation:\n",
    "#                         abb_to_country.append((potential_abbreviation,\\\n",
    "#                                                wikipedia_country_list[\"translation_state_name\"].tolist()[i]))\n",
    "#                         to_translate.remove(potential_abbreviation)\n",
    "#             elif type(potential_abbreviation) == list:\n",
    "#                 abb_to_country.append(translate_abbreviation(potential_abbreviation))\n",
    "#     return(abb_to_country,to_translate)\n",
    "\n",
    "# abbreviation_tuple, countries_not_translated = translate_abbreviation(countries_not_translated)\n",
    "\n",
    "# #abbreviation_tuple\n",
    "# #print(\"****************************************************************************************************************\")\n",
    "# #print(countries_not_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO if abbreviation found, don't continue searching\n",
    "def translate_abbreviation(to_translate):\n",
    "    \"\"\"Takes a list of countries and/or abbreviations and translates the abbreviations to the full state name\"\"\"\n",
    "    to_return = []\n",
    "    if type(to_translate) == str:\n",
    "        to_translate = [to_translate]\n",
    "    for potential_abbreviation in to_translate:\n",
    "        if type(potential_abbreviation) == str and not re.findall(r\"([^A-Z]+)\",potential_abbreviation):\n",
    "            \n",
    "            # First check the official abrev. than the self created ones e.g. VAE for the Emirates\n",
    "            for column in [\"wiki_abbreviations\",\"inoff_abbreviations\"]:\n",
    "                for i, abbreviation in enumerate(wikipedia_country_list[column]):\n",
    "                    if potential_abbreviation in abbreviation:\n",
    "                        to_return.append(wikipedia_country_list[\"state_name_de\"].tolist()[i])\n",
    "        elif type(potential_abbreviation) == list:\n",
    "            list_entry = [translate_abbreviation(nested_entry) for nested_entry in potential_abbreviation]\n",
    "            flattened = [entry for sublist in list_entry for entry in sublist]\n",
    "            to_return.append(flattened)\n",
    "        else:\n",
    "            to_return.append(potential_abbreviation)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test succesful\n"
     ]
    }
   ],
   "source": [
    "# Test for translate_abbreviation()\n",
    "example_to_abbreviate = [\"USA\",\"VAE\",'Italien', \"DR Cong\",[\"Deutschland\", \"EU\"],[\"Belgien\",\"DRC\"]]\n",
    "desired_output = ['Vereinigte Staaten','Vereinigte Arabische Emirate','Italien','DR Cong',\n",
    "                  ['Deutschland', 'Europäische Union'],\n",
    "                  ['Belgien', 'Kongo, Demokratische Republik']]\n",
    "if deep_eq(translate_abbreviation(example_to_abbreviate),desired_output):\n",
    "    print(\"Test succesful\")\n",
    "else:\n",
    "    Print(\"Test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SIMPLE TRANSLATION. FAST BUT DOES NOT TRANSLATE LISTS OF LISTS\n",
    "# # Translate German entries of Ereignisdatenbank to English. Might be inefficient since I go through the wiki list\n",
    "# # entirely which is longer then the list of countries to translate\n",
    "# translated_ereignisdatenbank_countries = [(entry,wikipedia_country_list[\"translation_state_name\"].tolist()[indx])\\\n",
    "#                                           for indx,entry \\\n",
    "#                                           in enumerate(wikipedia_country_list[\"state_name_de\"].tolist())\\\n",
    "#                                           if entry in countries_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "from didyoumean import didyoumean\n",
    "\n",
    "def translate(countries_unique):\n",
    "    \"\"\"Translate German entries of Ereignisdatenbank to English an returns tuple \n",
    "    of German word and English translation(s if ambigious)\"\"\" \n",
    "\n",
    "    continents = [\"europa\",\"africa\",\"america\",\"australien\",\"asia\"]\n",
    "    translated_ereignisdatenbank_countries = []\n",
    "    state_name_de = wikipedia_country_list[\"state_name_de\"].tolist()\n",
    "    full_state_name_de = wikipedia_country_list[\"full_state_name_de\"].tolist()\n",
    "    translation = wikipedia_country_list[\"translation_state_name\"].tolist()\n",
    "    \n",
    "    if type(countries_unique) == str:\n",
    "        countries_unique = [countries_unique]\n",
    "    \n",
    "    for entry in countries_unique:\n",
    "        if type(entry) == str:\n",
    "            sucessfull_search = list(filter(lambda x: re.findall(entry,x),state_name_de))\n",
    "            if sucessfull_search:\n",
    "                found = [translation[state_name_de.index(entry)] for entry in sucessfull_search]\n",
    "                if len(found) == 1:\n",
    "                    translated_ereignisdatenbank_countries.append((entry,found[0]))\n",
    "                else:\n",
    "                    \n",
    "                    # Check for idendity in not ambigious case otherwise e.g Niger --> (Niger, Nigeria)\n",
    "                    identical = [found_ent for found_ent in found if entry == found_ent]\n",
    "                    if identical:\n",
    "                        translated_ereignisdatenbank_countries.append((entry,identical[0]))\n",
    "                    else:\n",
    "                        translated_ereignisdatenbank_countries.append((entry,found))\n",
    "            else:\n",
    "                \n",
    "                # If entry not in state_name_de, search in full_state_name_de\n",
    "                sucessfull_search = list(filter(lambda x: re.findall(entry,x),full_state_name_de))\n",
    "                if sucessfull_search:\n",
    "                    found = [translation[full_state_name_de.index(entry)] for entry in sucessfull_search]\n",
    "                    if len(found) == 1:\n",
    "                        translated_ereignisdatenbank_countries.append((entry,found[0]))\n",
    "                    else:\n",
    "                        translated_ereignisdatenbank_countries.append((entry,found))\n",
    "                else:\n",
    "                    sucessfull_search_en = list(filter(lambda x: re.findall(entry,x),translation))\n",
    "                    if sucessfull_search_en:\n",
    "                        found = [state_name_de[translation.index(entry)] for entry in sucessfull_search_en]\n",
    "                        if len(found) == 1:\n",
    "                            translated_ereignisdatenbank_countries.append((found[0],entry))\n",
    "                        else:\n",
    "                            translated_ereignisdatenbank_countries.append((found,entry))\n",
    "                    else:\n",
    "                        \n",
    "                        # If there was not match at all, check for spelling mistakes\n",
    "                        did_u_mean = didyoumean.didYouMean(entry,state_name_de)\n",
    "                        \n",
    "                        \"\"\"Exlude words with continent names since there are countries with a continent name\n",
    "                        but there are also entries in the Ereignisdatenbank that mean the whole country. They\n",
    "                        are not matched (e.g. Nordafrika) and there is must not be a match, otherwise \n",
    "                        didYouMean would falsly return Südafrika.\n",
    "                        \"\"\"\n",
    "                        if did_u_mean and (did_u_mean not in continents):\n",
    "                            translated_ereignisdatenbank_countries.append(translate(did_u_mean))\n",
    "                        else:\n",
    "                            translated_ereignisdatenbank_countries.append(entry)\n",
    "                                                            \n",
    "        elif type(entry) == list:\n",
    "            translated_ereignisdatenbank_countries.append(translate(entry))\n",
    "        else:\n",
    "            translated_ereignisdatenbank_countries.append(entry)\n",
    "    return translated_ereignisdatenbank_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test succesfull\n"
     ]
    }
   ],
   "source": [
    "example_to_translate = [\"Deutschland\",\"Delaware\",[\"Kongo\",\"China\"],\"Niger\"]\n",
    "expected_result_translate = [('Deutschland', 'Germany'),\n",
    "                             'Delaware',\n",
    "                             [('Kongo',\n",
    "                               ['Congo, Democratic Republic of the (Kinshasa)','Congo, Republic of (Brazzaville)']),\n",
    "                              ('China','China')],\n",
    "                            ('Niger', 'Niger')]\n",
    "if deep_eq(translate(example_to_translate),expected_result_translate):\n",
    "    print(\"Test succesfull\")\n",
    "else:\n",
    "    print(\"Test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Algerien', 'Algeria'),\n",
       " ('Peru', 'Peru'),\n",
       " ('Äthiopien', 'Ethiopia'),\n",
       " [('Ungarn', 'Hungary'),\n",
       "  ('Serbien', 'Serbia'),\n",
       "  ('Rumänien', 'Romania'),\n",
       "  ('Italien', 'Italy'),\n",
       "  ('Griechenland', 'Greece')],\n",
       " ('Sri Lanka', 'Sri Lanka'),\n",
       " ('El Salvador', 'El Salvador'),\n",
       " ('Jemen', 'Yemen'),\n",
       " ('Namibia', 'Namibia'),\n",
       " ('Simbabwe', 'Zimbabwe'),\n",
       " ('Kongo, Demokratische Republik',\n",
       "  'Congo, Democratic Republic of the (Kinshasa)'),\n",
       " ('Angola', 'Angola'),\n",
       " ('Burundi', 'Burundi'),\n",
       " ('Vereinigte Staaten', 'United States'),\n",
       " ('Kolumbien', 'Colombia'),\n",
       " ('Bolivien', 'Bolivia'),\n",
       " ('Schweiz', 'Switzerland'),\n",
       " ('Südsudan', 'South Sudan'),\n",
       " ('Haiti', 'Haiti'),\n",
       " ('Venezuela', 'Venezuela'),\n",
       " ('Niger', 'Niger'),\n",
       " ('Ghana', 'Ghana'),\n",
       " ('Costa Rica', 'Costa Rica'),\n",
       " ('China', 'China'),\n",
       " ('Bangladesch', 'Bangladesh'),\n",
       " ('Somalia', 'Somalia'),\n",
       " ('Syrien', 'Syria'),\n",
       " 'Französich Polynesien',\n",
       " ('Kanada', 'Canada'),\n",
       " ('Tansania', 'Tanzania'),\n",
       " ('Nigeria', 'Nigeria'),\n",
       " ('Taiwan', 'Taiwan oder Republic of China'),\n",
       " ('Saudi-Arabien', 'Saudi Arabia'),\n",
       " ('Südsudan', 'South Sudan'),\n",
       " ('Mosambik', 'Mozambique'),\n",
       " 'Nordeuropa',\n",
       " ('Kongo',\n",
       "  ['Congo, Democratic Republic of the (Kinshasa)',\n",
       "   'Congo, Republic of (Brazzaville)']),\n",
       " 'Typhus',\n",
       " 'DRCongo',\n",
       " ('Benin', 'Benin'),\n",
       " ('Liberia', 'Liberia'),\n",
       " ('Israel', 'Israel'),\n",
       " ('Trinidad und Tobago', 'Trinidad and Tobago'),\n",
       " ('Sambia', 'Zambia'),\n",
       " [('Ungarn', 'Hungary'),\n",
       "  ('Italien', 'Italy'),\n",
       "  [('Rumänien', 'Romania')],\n",
       "  ('Frankreich', 'France'),\n",
       "  ('Griechenland', 'Greece')],\n",
       " [('Afghanistan', 'Afghanistan'),\n",
       "  ('Nigeria', 'Nigeria'),\n",
       "  'DR Congo',\n",
       "  ('Somalia', 'Somalia')],\n",
       " ('Fidschi', 'Fiji'),\n",
       " ('Kamerun', 'Cameroon'),\n",
       " ('Irak', 'Iraq'),\n",
       " ('Vereinigte Arabische Emirate', 'United Arab Emirates'),\n",
       " ('Komoren', 'Comoros'),\n",
       " ('Vereinigtes Königreich', 'United Kingdom'),\n",
       " ('Deutschland', 'Germany'),\n",
       " ('Australien', 'Australia'),\n",
       " 'Französiche Guyana',\n",
       " ('Brasilien', 'Brazil'),\n",
       " ('Zentralafrikanische Republik', 'Central African Republic'),\n",
       " ('Spanien', 'Spain'),\n",
       " ('Ukraine', 'Ukraine'),\n",
       " ('Kroatien', 'Croatia'),\n",
       " ('Tschad', 'Chad'),\n",
       " ('Oman', 'Oman'),\n",
       " 'La Reunion',\n",
       " ('Kenia', 'Kenya'),\n",
       " ('Sudan', 'Sudan'),\n",
       " ('Polen', 'Poland'),\n",
       " ['Delaware', ('Vereinigte Staaten', 'United States')],\n",
       " 'VAE Dubai',\n",
       " ('Kuwait', 'Kuwait'),\n",
       " 'Französisch-Polynesien',\n",
       " ('Vereinigte Arabische Emirate', 'United Arab Emirates'),\n",
       " ('Frankreich', 'France'),\n",
       " ('Demokratische Republik Kongo',\n",
       "  'Congo, Democratic Republic of the (Kinshasa)'),\n",
       " ('Südafrika', 'South Africa'),\n",
       " ('Philippinen', 'Philippines'),\n",
       " ('Mali', 'Mali'),\n",
       " ('Pakistan', 'Pakistan'),\n",
       " ('Vereinigtes Königreich', 'United Kingdom'),\n",
       " ('Afghanistan', 'Afghanistan'),\n",
       " ('Tschechien', 'Czech Republic'),\n",
       " ('Uganda', 'Uganda'),\n",
       " ('Myanmar', 'Myanmar oder Burma'),\n",
       " ('Indien', 'India'),\n",
       " ('Iran', 'Iran'),\n",
       " [('Südafrika', 'South Africa')],\n",
       " ('Papua-Neuguinea', 'Papua New Guinea'),\n",
       " ('Italien', 'Italy'),\n",
       " [('Rumänien', 'Romania'),\n",
       "  ('Griechenland', 'Greece'),\n",
       "  ('Ungarn', 'Hungary'),\n",
       "  ('Italien', 'Italy')],\n",
       " ('Malawi', 'Malawi'),\n",
       " [('Rumänien', 'Romania'),\n",
       "  ('Serbien', 'Serbia'),\n",
       "  ('Ungarn', 'Hungary'),\n",
       "  ('Montenegro', 'Montenegro'),\n",
       "  ('Italien', 'Italy'),\n",
       "  ('Albanien', 'Albania'),\n",
       "  ('Frankreich', 'France'),\n",
       "  ('Türkei', 'Turkey'),\n",
       "  ('Griechenland', 'Greece'),\n",
       "  ('Kosovo', 'Kosovo'),\n",
       "  [('Mazedonien', 'Macedonia')]],\n",
       " ('Madagaskar', 'Madagascar'),\n",
       " ('Mauretanien', 'Mauritania')]"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated = translate(translate_abbreviation(clean_entries(countries)))\n",
    "translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DR Congo',\n",
       " 'DRCongo',\n",
       " 'Delaware',\n",
       " 'Französich Polynesien',\n",
       " 'Französiche Guyana',\n",
       " 'Französisch-Polynesien',\n",
       " 'La Reunion',\n",
       " 'Nordeuropa',\n",
       " 'Typhus',\n",
       " 'VAE Dubai'}"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = [entry if type(sublist) == list else sublist for sublist in translated for entry in sublist ]\n",
    "countries_not_translated = set([entry for entry in flattened if type(entry) == str])\n",
    "countries_not_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unterstanding geoname annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Geoname Annotator\"\"\"\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "from .maximum_weight_interval_set import Interval, find_maximum_weight_interval_set\n",
    "\n",
    "# Containment levels indicate which properties must match when determing\n",
    "# whether a geoname of a given containment level contains another geoname.\n",
    "# The admin codes generally correspond to states, provinces and cities.\n",
    "CONTAINMENT_LEVELS = [\n",
    "    'country_code',\n",
    "    'admin1_code',\n",
    "    'admin2_code',\n",
    "    'admin3_code',\n",
    "    'admin4_code'\n",
    "]\n",
    "\n",
    "GEONAME_ATTRS = [\n",
    "    'geonameid',\n",
    "    'name',\n",
    "    'feature_code',\n",
    "    'country_code',\n",
    "    'admin1_code',\n",
    "    'admin2_code',\n",
    "    'admin3_code',\n",
    "    'admin4_code',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'population',\n",
    "    'asciiname',\n",
    "    'names_used',\n",
    "    'name_count']\n",
    "def location_contains(loc_outer, loc_inner):\n",
    "    \"\"\"\n",
    "    Do a comparison to see if the first geoname contains the second.\n",
    "    It returns an integer to indicate the level of containment.\n",
    "    0 indicates no containment. Siblings locations and identical locations\n",
    "    have 0 containment. The level of containment is determined by the specificty\n",
    "    of the outer location. e.g. USA would be a smaller number than Texas.\n",
    "    In order for containment to be detected the outer location must have a\n",
    "    ADM* or PCL* feature code, which is most countries, states, and districts.\n",
    "    \"\"\"\n",
    "    # Test the country code in advance for efficiency. The country code must match for\n",
    "    # any level of containment.\n",
    "    if loc_outer.country_code != loc_inner.country_code or loc_outer.country_code == '':\n",
    "        return 0\n",
    "    feature_code = loc_outer.feature_code\n",
    "    if feature_code == 'ADM1':\n",
    "        outer_feature_level = 2\n",
    "    elif feature_code == 'ADM2':\n",
    "        outer_feature_level = 3\n",
    "    elif feature_code == 'ADM3':\n",
    "        outer_feature_level = 4\n",
    "    elif feature_code == 'ADM4':\n",
    "        outer_feature_level = 5\n",
    "    elif re.match(\"^PCL.\", feature_code):\n",
    "        outer_feature_level = 1\n",
    "    else:\n",
    "        return 0\n",
    "    for prop in CONTAINMENT_LEVELS[1:outer_feature_level]:\n",
    "        if loc_outer[prop] == '':\n",
    "            return 0\n",
    "        if loc_outer[prop] != loc_inner[prop]:\n",
    "            return 0\n",
    "    if loc_outer.geonameid == loc_inner.geonameid:\n",
    "        return 0\n",
    "    return outer_feature_level\n",
    "\n",
    "\n",
    "\n",
    "GEONAME_ATTRS = [\n",
    "    'geonameid',\n",
    "    'name',\n",
    "    'feature_code',\n",
    "    'country_code',\n",
    "    'admin1_code',\n",
    "    'admin2_code',\n",
    "    'admin3_code',\n",
    "    'admin4_code',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'population',\n",
    "    'asciiname',\n",
    "    'names_used',\n",
    "    'name_count']\n",
    "\n",
    "\n",
    "ADMINNAME_ATTRS = [\n",
    "    'country_name',\n",
    "    'admin1_name',\n",
    "    'admin2_name',\n",
    "    'admin3_name']\n",
    "\n",
    "\n",
    "class GeonameRow(object):\n",
    "    __slots__ = GEONAME_ATTRS + ADMINNAME_ATTRS + [\n",
    "        'alternate_locations',\n",
    "        'spans',\n",
    "        'parents',\n",
    "        'score',\n",
    "        'lat_long',\n",
    "        'high_confidence']\n",
    "\n",
    "    def __init__(self, sqlite3_row):\n",
    "        for key in sqlite3_row.keys():\n",
    "            if key in GEONAME_ATTRS:\n",
    "                setattr(self, key, sqlite3_row[key])\n",
    "        self.lat_long = (self.latitude, self.longitude,)\n",
    "        self.alternate_locations = set()\n",
    "        self.spans = set()\n",
    "        self.parents = set()\n",
    "        self.score = None\n",
    "\n",
    "\n",
    "\n",
    "    def to_dict(self):\n",
    "        result = {}\n",
    "        for key in GEONAME_ATTRS:\n",
    "            result[key] = self[key]\n",
    "        for key in ADMINNAME_ATTRS:\n",
    "            if hasattr(self, key):\n",
    "                result[key] = self[key]\n",
    "        result['parents'] = [p.to_dict() for p in self.parents]\n",
    "        result['score'] = self.score\n",
    "        return result\n",
    "\n",
    "\n",
    "class GeonameFeatures(object):\n",
    "    \"\"\"\n",
    "    This represents the aspects of a condidate geoname that are used to\n",
    "    determine whether it is being referenced.\n",
    "    \"\"\"\n",
    "    # The feature name array is used to maintain the order of the\n",
    "    # values in the feature vector.\n",
    "    feature_names = [\n",
    "        'log_population',\n",
    "        'name_count',\n",
    "        'num_spans',\n",
    "        'max_span_length',\n",
    "        'cannonical_name_used',\n",
    "        'loc_NE_portion',\n",
    "        'other_NE_portion',\n",
    "        'noun_portion',\n",
    "        'other_pos_portion',\n",
    "        'num_tokens',\n",
    "        'ambiguity',\n",
    "        'PPL_feature_code',\n",
    "        'ADM_feature_code',\n",
    "        'CONT_feature_code',\n",
    "        'other_feature_code',\n",
    "        'combined_span_parents',\n",
    "        'close_locations',\n",
    "        'very_close_locations',\n",
    "        'containing_locations',\n",
    "        'max_containment_level',\n",
    "        # high_confidence indicates the base feature set received a high score.\n",
    "        # It is an useful feature for preventing high confidence geonames\n",
    "        # from receiving low final scores when they lack contextual cues -\n",
    "        # for example, when they are the only location mentioned.\n",
    "        'high_confidence',\n",
    "    ]\n",
    "\n",
    "    def __init__(self, geoname, spans_to_nes, span_to_tokens):\n",
    "        self.geoname = geoname\n",
    "        # The set of geonames that are mentioned in proximity to the spans\n",
    "        # corresponding to this feature.\n",
    "        # This will be populated by the add_contextual_features function.\n",
    "        self.nearby_mentions = set()\n",
    "        d = {}\n",
    "        d['log_population'] = math.log(geoname.population + 1)\n",
    "        # Geonames with lots of alternate names\n",
    "        # tend to be the ones most commonly referred to.\n",
    "        d['name_count'] = geoname.name_count\n",
    "        d['num_spans'] = len(geoname.spans)\n",
    "        d['max_span_length'] = max([\n",
    "            len(span.text) for span in geoname.spans])\n",
    "\n",
    "        def cannonical_name_match(span, geoname):\n",
    "            first_leaf = next(span.iterate_leaf_base_spans(), None)\n",
    "            if first_leaf:\n",
    "                span_text = first_leaf.text\n",
    "            else:\n",
    "                span_text = span.text\n",
    "            span_in_name = span_text in geoname.name or span_text in geoname.asciiname\n",
    "            return (float(len(span_text)) if span_in_name else 0) / len(geoname.name)\n",
    "        d['cannonical_name_used'] = max([\n",
    "            cannonical_name_match(span, geoname)\n",
    "            for span in geoname.spans\n",
    "        ])\n",
    "        loc_NEs_overlap = 0\n",
    "        other_NEs_overlap = 0\n",
    "        total_spans = len(geoname.spans)\n",
    "        for span in geoname.spans:\n",
    "            for ne_span in spans_to_nes[span]:\n",
    "                if ne_span.label == 'GPE' or ne_span.label == 'LOC':\n",
    "                    loc_NEs_overlap += 1\n",
    "                else:\n",
    "                    other_NEs_overlap += 1\n",
    "        d['loc_NE_portion'] = float(loc_NEs_overlap) / total_spans\n",
    "        d['other_NE_portion'] = float(other_NEs_overlap) / total_spans\n",
    "        noun_pos_tags = 0\n",
    "        other_pos_tags = 0\n",
    "        pos_tags = 0\n",
    "        for span in geoname.spans:\n",
    "            for token_span in span_to_tokens[span]:\n",
    "                token = token_span.token\n",
    "                pos_tags += 1\n",
    "                if token.tag_.startswith(\"NN\") or token.tag_ == \"FW\":\n",
    "                    noun_pos_tags += 1\n",
    "                else:\n",
    "                    other_pos_tags += 1\n",
    "        d['combined_span_parents'] = len(geoname.parents)\n",
    "        d['noun_portion'] = float(noun_pos_tags) / pos_tags\n",
    "        d['other_pos_portion'] = float(other_pos_tags) / pos_tags\n",
    "        d['num_tokens'] = pos_tags\n",
    "        d['ambiguity'] = len(geoname.alternate_locations)\n",
    "        feature_code = geoname.feature_code\n",
    "        if feature_code.startswith('PPL'):\n",
    "            d['PPL_feature_code'] = 1\n",
    "        elif feature_code.startswith('ADM'):\n",
    "            d['ADM_feature_code'] = 1\n",
    "        elif feature_code.startswith('CONT'):\n",
    "            d['CONT_feature_code'] = 1\n",
    "        else:\n",
    "            d['other_feature_code'] = 1\n",
    "        self._values = [0] * len(self.feature_names)\n",
    "        self.set_values(d)\n",
    "\n",
    "    def set_value(self, feature_name, value):\n",
    "        self._values[self.feature_names.index(feature_name)] = value\n",
    "\n",
    "    def set_values(self, value_dict):\n",
    "        for idx, name in enumerate(self.feature_names):\n",
    "            if name in value_dict:\n",
    "                self._values[idx] = value_dict[name]\n",
    "\n",
    "    def set_contextual_features(self):\n",
    "        \"\"\"\n",
    "        GeonameFeatures are initialized with only values that can be extracted\n",
    "        from the geoname database and span. This extends the GeonameFeature\n",
    "        with values that require information from nearby_mentions.\n",
    "        \"\"\"\n",
    "        geoname = self.geoname\n",
    "        close_locations = 0\n",
    "        very_close_locations = 0\n",
    "        containing_locations = 0\n",
    "        max_containment_level = 0\n",
    "        for recently_mentioned_geoname in self.nearby_mentions:\n",
    "            if recently_mentioned_geoname == geoname:\n",
    "                continue\n",
    "            containment_level = max(\n",
    "                location_contains(geoname, recently_mentioned_geoname),\n",
    "                location_contains(recently_mentioned_geoname, geoname))\n",
    "            if containment_level > 0:\n",
    "                containing_locations += 1\n",
    "            if containment_level > max_containment_level:\n",
    "                max_containment_level = containment_level\n",
    "            distance = great_circle(\n",
    "                recently_mentioned_geoname.lat_long, geoname.lat_long\n",
    "            ).kilometers\n",
    "            if distance < 400:\n",
    "                close_locations += 1\n",
    "            if distance < 100:\n",
    "                very_close_locations += 1\n",
    "        self.set_values(dict(\n",
    "            close_locations=close_locations,\n",
    "            very_close_locations=very_close_locations,\n",
    "            containing_locations=containing_locations,\n",
    "            max_containment_level=max_containment_level))\n",
    "\n",
    "\n",
    "class GeonameAnnotator(Annotator):\n",
    "    def __init__(self, custom_classifier=None):\n",
    "        self.connection = get_database_connection()\n",
    "        self.connection.row_factory = sqlite3.Row\n",
    "        if custom_classifier:\n",
    "            self.geoname_classifier = custom_classifier\n",
    "        else:\n",
    "            self.geoname_classifier = geoname_classifier\n",
    "\n",
    "    def get_candidate_geonames(self, doc):\n",
    "        \"\"\"\n",
    "        Returns an array of geoname dicts correponding to locations that the\n",
    "        document may refer to.\n",
    "        The dicts are extended with lists of associated AnnoSpans.\n",
    "        \"\"\"\n",
    "        if 'ngrams' not in doc.tiers:\n",
    "            doc.add_tiers(NgramAnnotator())\n",
    "        logger.info('Ngrams annotated')\n",
    "        if 'nes' not in doc.tiers:\n",
    "            doc.add_tiers(NEAnnotator())\n",
    "        logger.info('Named entities annotated')\n",
    "\n",
    "        all_ngrams = list(set([span.text.lower()\n",
    "                               for span in doc.tiers['ngrams'].spans\n",
    "                               if is_possible_geoname(span.text)\n",
    "                               ]))\n",
    "        cursor = self.connection.cursor()\n",
    "        geoname_results = list(cursor.execute('''\n",
    "        SELECT\n",
    "            geonames.*,\n",
    "            count AS name_count,\n",
    "            group_concat(alternatename, \";\") AS names_used\n",
    "        FROM geonames\n",
    "        JOIN alternatename_counts USING ( geonameid )\n",
    "        JOIN alternatenames USING ( geonameid )\n",
    "        WHERE alternatename_lemmatized IN\n",
    "        (''' + ','.join('?' for x in all_ngrams) + ''')\n",
    "        GROUP BY geonameid''', all_ngrams))\n",
    "        logger.info('%s geonames fetched' % len(geoname_results))\n",
    "        geoname_results = [GeonameRow(g) for g in geoname_results]\n",
    "        # Associate spans with the geonames.\n",
    "        # This is done up front so span information can be used in the scoring\n",
    "        # function\n",
    "        span_text_to_spans = defaultdict(list)\n",
    "        for span in doc.tiers['ngrams'].spans:\n",
    "            if is_possible_geoname(span.text):\n",
    "                span_text_to_spans[span.text.lower()].append(span)\n",
    "        candidate_geonames = []\n",
    "        for geoname in geoname_results:\n",
    "            geoname.add_spans(span_text_to_spans)\n",
    "            # In rare cases geonames may have no matching spans because\n",
    "            # sqlite unicode equivalency rules match geonames that use different\n",
    "            # characters the document spans used to query them.\n",
    "            # These geonames are ignored.\n",
    "            if len(geoname.spans) > 0:\n",
    "                candidate_geonames.append(geoname)\n",
    "        # Add combined spans to locations that are adjacent to a span linked to\n",
    "        # an administrative division. e.g. Seattle, WA\n",
    "        span_to_geonames = defaultdict(list)\n",
    "        for geoname in candidate_geonames:\n",
    "            for span in geoname.spans:\n",
    "                span_to_geonames[span].append(geoname)\n",
    "        geoname_spans = span_to_geonames.keys()\n",
    "        combined_spans = AnnoTier(geoname_spans).chains(at_least=2, at_most=4, max_dist=4)\n",
    "        for combined_span in combined_spans:\n",
    "            leaf_spans = combined_span.iterate_leaf_base_spans()\n",
    "            first_spans = next(leaf_spans)\n",
    "            potential_geonames = {geoname: set()\n",
    "                                  for geoname in span_to_geonames[first_spans]}\n",
    "            for leaf_span in leaf_spans:\n",
    "                leaf_span_geonames = span_to_geonames[leaf_span]\n",
    "                next_potential_geonames = defaultdict(set)\n",
    "                for potential_geoname, prev_containing_geonames in potential_geonames.items():\n",
    "                    containing_geonames = [\n",
    "                        containing_geoname\n",
    "                        for containing_geoname in leaf_span_geonames\n",
    "                        if location_contains(containing_geoname, potential_geoname) > 0]\n",
    "                    if len(containing_geonames) > 0:\n",
    "                        next_potential_geonames[potential_geoname] |= prev_containing_geonames | set(containing_geonames)\n",
    "                potential_geonames = next_potential_geonames\n",
    "            for geoname, containing_geonames in potential_geonames.items():\n",
    "                geoname.spans.add(combined_span)\n",
    "                geoname.parents |= containing_geonames\n",
    "        # Replace individual spans with combined spans.\n",
    "        span_to_geonames = defaultdict(list)\n",
    "        for geoname in candidate_geonames:\n",
    "            geoname.spans = set(AnnoTier(geoname.spans).optimal_span_set().spans)\n",
    "            for span in geoname.spans:\n",
    "                span_to_geonames[span].append(geoname)\n",
    "        # Find locations with overlapping spans\n",
    "        # Note that is is possible for two valid locations to have\n",
    "        # overlapping names. For example, Harare Province has\n",
    "        # Harare as an alternate name, so the city Harare is very\n",
    "        # likely to be an alternate location that competes with it.\n",
    "        for span, geonames in span_to_geonames.items():\n",
    "            geoname_set = set(geonames)\n",
    "            for geoname in geonames:\n",
    "                geoname.alternate_locations |= geoname_set\n",
    "        for geoname in candidate_geonames:\n",
    "            geoname.alternate_locations -= set([geoname])\n",
    "        logger.info('%s alternative locations found' % sum([\n",
    "            len(geoname.alternate_locations)\n",
    "            for geoname in candidate_geonames]))\n",
    "        logger.info('%s candidate locations prepared' %\n",
    "                    len(candidate_geonames))\n",
    "        return candidate_geonames\n",
    "\n",
    "   \n",
    "\n",
    "    def add_contextual_features(self, features):\n",
    "        \"\"\"\n",
    "        Extend a list of features with values that are based on the geonames\n",
    "        mentioned nearby.\n",
    "        \"\"\"\n",
    "        logger.info('adding contextual features')\n",
    "        span_to_features = defaultdict(list)\n",
    "        for feature in features:\n",
    "            for span in feature.geoname.spans:\n",
    "                span_to_features[span].append(feature)\n",
    "        geoname_span_tier = AnnoTier(list(span_to_features.keys()))\n",
    "\n",
    "\n",
    "\n",
    "    def annotate(self, doc):\n",
    "        logger.info('geoannotator started')\n",
    "        candidate_geonames = self.get_candidate_geonames(doc)\n",
    "        features = self.extract_features(candidate_geonames, doc)\n",
    "        if len(features) == 0:\n",
    "            doc.tiers['geonames'] = AnnoTier([])\n",
    "            return doc\n",
    "\n",
    "        scores = self.geoname_classifier.predict_proba_base([\n",
    "            list(f.values()) for f in features])\n",
    "        for geoname, feature, score in zip(candidate_geonames, features, scores):\n",
    "            geoname.high_confidence = float(\n",
    "                score[1]) > self.geoname_classifier.HIGH_CONFIDENCE_THRESHOLD\n",
    "            feature.set_value('high_confidence', geoname.high_confidence)\n",
    "        has_high_confidence_features = any(\n",
    "            [geoname.high_confidence for geoname in candidate_geonames])\n",
    "        if has_high_confidence_features:\n",
    "            self.add_contextual_features(features)\n",
    "            scores = self.geoname_classifier.predict_proba_contextual([\n",
    "                list(f.values()) for f in features])\n",
    "        for geoname, score in zip(candidate_geonames, scores):\n",
    "            geoname.score = float(score[1])\n",
    "        culled_geonames = [geoname\n",
    "                           for geoname in candidate_geonames\n",
    "                           if geoname.score > self.geoname_classifier.GEONAME_SCORE_THRESHOLD]\n",
    "        cursor = self.connection.cursor()\n",
    "        for geoname in culled_geonames:\n",
    "            geoname_results = list(cursor.execute('''\n",
    "                SELECT\n",
    "                    cc.name,\n",
    "                    a1.name,\n",
    "                    a2.name,\n",
    "                    a3.name\n",
    "                FROM adminnames a3\n",
    "                JOIN adminnames a2 ON (\n",
    "                    a2.country_code = a3.country_code AND\n",
    "                    a2.admin1_code = a3.admin1_code AND\n",
    "                    a2.admin2_code = a3.admin2_code AND\n",
    "                    a2.admin3_code = \"\" )\n",
    "                JOIN adminnames a1 ON (\n",
    "                    a1.country_code = a3.country_code AND\n",
    "                    a1.admin1_code = a3.admin1_code AND\n",
    "                    a1.admin2_code = \"\" AND\n",
    "                    a1.admin3_code = \"\" )\n",
    "                JOIN adminnames cc ON (\n",
    "                    cc.country_code = a3.country_code AND\n",
    "                    cc.admin1_code = \"00\" AND\n",
    "                    cc.admin2_code = \"\" AND\n",
    "                    cc.admin3_code = \"\" )\n",
    "                WHERE (a3.country_code = ? AND a3.admin1_code = ? AND a3.admin2_code = ? AND a3.admin3_code = ?)\n",
    "                ''', (\n",
    "                geoname.country_code or \"\",\n",
    "                geoname.admin1_code or \"\",\n",
    "                geoname.admin2_code or \"\",\n",
    "                geoname.admin3_code or \"\",)))\n",
    "            for result in geoname_results:\n",
    "                prev_val = None\n",
    "                for idx, attr in enumerate(['country_name', 'admin1_name', 'admin2_name', 'admin3_name']):\n",
    "                    val = result[idx]\n",
    "                    if val == prev_val:\n",
    "                        # Names are repeated for admin levels beyond that of\n",
    "                        # the geoname.\n",
    "                        break\n",
    "                    setattr(geoname, attr, val)\n",
    "                    prev_val = val\n",
    "        logger.info('admin names added')\n",
    "        geo_spans = []\n",
    "        for geoname in culled_geonames:\n",
    "            for span in geoname.spans:\n",
    "                geo_span = GeoSpan(\n",
    "                    span.start, span.end, doc, geoname)\n",
    "                geo_spans.append(geo_span)\n",
    "        culled_geospans = AnnoTier(geo_spans).optimal_span_set(prefer=lambda x: (x.size(), x.geoname.score,))\n",
    "        logger.info('overlapping geospans removed')\n",
    "        return {'geonames': culled_geospans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==abE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
