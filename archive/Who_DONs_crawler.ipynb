{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sys import stdout\n",
    "from time import sleep\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHO SCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_by_year(list_of_years=None, proxies={'http': 'http://fw-bln.rki.local:8020'}):\n",
    "    '''Get (all) the anual links of the WHO DONs\n",
    "    \n",
    "    list_of_years -- a list of years (YYYY format) you want to parse (default None)\n",
    "    proxies -- the proxy to use while scraping (default {'http': 'http://fw-bln.rki.local:8020'})\n",
    "    '''\n",
    "\n",
    "    page = requests.get('http://www.who.int/csr/don/archive/year/en/',proxies=proxies)\n",
    "    soup = BeautifulSoup(page.content,'html.parser')\n",
    "    archiv_years = soup.find('ul',attrs={'class':'list'})\n",
    "    years_links_html = archiv_years.find_all('a')\n",
    "    if list_of_years:\n",
    "        return ['http://www.who.int' + link.get('href') for link in years_links_html if any(year in link for year in list_of_years)]\n",
    "    else:\n",
    "        return ['http://www.who.int' + link.get('href') for link in years_links_html]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all provided links per year\n",
    "\n",
    "def get_links_per_year(years_links, list_of_months=None, proxies={'http': 'http://fw-bln.rki.local:8020'}):\n",
    "    '''Take a list of links to the annual archive and return a list of DON links of these years\n",
    "    \n",
    "    years_links -- a list of links of the anual archive to parse \n",
    "    list_of_months -- a list of months (MMM* format) you want to parse (default None)\n",
    "    proxies -- the proxy to use while scraping (default {'http': 'http://fw-bln.rki.local:8020'})\n",
    "    '''\n",
    "    all_links = []\n",
    "    \n",
    "    for year_link in years_links:\n",
    "        page_year = requests.get(year_link,proxies=proxies)\n",
    "        soup_year = BeautifulSoup(page_year.content,'html.parser')\n",
    "        archive_year = soup_year.find('ul',attrs={'class':'auto_archive'})\n",
    "        daily_links = ['http://www.who.int' + link.get('href') for link in archive_year.find_all('a')]\n",
    "        all_links.extend(daily_links)\n",
    "    \n",
    "    if list_of_months:\n",
    "        all_links = [link for link in all_links if any(month in link for month in map(lambda s:s.lower(),list_of_months))]\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Auss Abbood, www.rki.de',\n",
    "    'From': 'abbooda@rki.de'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_from_links(all_links,headers,proxies={'http': 'http://fw-bln.rki.local:8020'},num_last_reports=None):\n",
    "    '''Take a list of links of WHO DONs and return a list of their content \n",
    "    \n",
    "    all_links -- a list of links of the WHO DONs to parse \n",
    "    headers -- a header dictonary to be indentifiable as a parser\n",
    "    proxies -- the proxy to use while scraping (default {'http': 'http://fw-bln.rki.local:8020'})\n",
    "    '''\n",
    "    all_text = []\n",
    "    if num_last_reports:\n",
    "        all_links = all_links[-num_last_reports:]\n",
    "    for step,single_report in enumerate(all_links):\n",
    "        page_single_report = requests.get(single_report,proxies=proxies,headers=headers)\n",
    "        soup_single_report = BeautifulSoup(page_single_report.content,'html.parser')\n",
    "        text_single_report = [parse.get_text() for parse in soup_single_report.find_all('span') if (len(parse.get_text()) > 65)]\n",
    "        all_text.extend(text_single_report)\n",
    "\n",
    "        stdout.write(\"\\r%d\" % step + '/'+ str(len(all_links)) + ' links processed | ' \n",
    "                     + str('%.0f' % (step/len(all_links) *100)) + '% complete' )\n",
    "        stdout.flush()\n",
    "        sleep(0.1)\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = date = str(datetime.datetime.now())[:-7].replace(' ','-')\n",
    "def scrape(years=None,months=None,num_last_reports=None,headers=None):\n",
    "    years = get_links_by_year(list_of_years=years)\n",
    "    all_links = get_links_per_year(years,list_of_months=months)\n",
    "    all_text = scrape_from_links(all_links,headers=headers)\n",
    "    with open(\"who_crawl_{}.p\".format(date), \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(all_text, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 22] invalid mode ('wb') or filename: 'who_crawl_2018-10-31-14:40:51.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-7e3890822858>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-86-ed0e90912dfb>\u001b[0m in \u001b[0;36mscrape\u001b[1;34m(years, months, num_last_reports, headers)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mall_links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_links_per_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myears\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_of_months\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmonths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mall_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_from_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_links\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"who_crawl_{}.p\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m#Pickling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 22] invalid mode ('wb') or filename: 'who_crawl_2018-10-31-14:40:51.p'"
     ]
    }
   ],
   "source": [
    "scrape([2018],headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"who_crawl.p\", \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpype\n",
    "import urllib2\n",
    "import socket\n",
    "import charade\n",
    "import threading\n",
    "\n",
    "socket.setdefaulttimeout(15)\n",
    "lock = threading.Lock()\n",
    "\n",
    "InputSource        = jpype.JClass('org.xml.sax.InputSource')\n",
    "StringReader       = jpype.JClass('java.io.StringReader')\n",
    "HTMLHighlighter    = jpype.JClass('de.l3s.boilerpipe.sax.HTMLHighlighter')\n",
    "BoilerpipeSAXInput = jpype.JClass('de.l3s.boilerpipe.sax.BoilerpipeSAXInput')\n",
    "\n",
    "class Extractor(object):\n",
    "    \"\"\"\n",
    "    Extract text. Constructor takes 'extractor' as a keyword argument,\n",
    "    being one of the boilerpipe extractors:\n",
    "    - DefaultExtractor\n",
    "    - ArticleExtractor\n",
    "    - ArticleSentencesExtractor\n",
    "    - KeepEverythingExtractor\n",
    "    - KeepEverythingWithMinKWordsExtractor\n",
    "    - LargestContentExtractor\n",
    "    - NumWordsRulesExtractor\n",
    "    - CanolaExtractor\n",
    "    \"\"\"\n",
    "    extractor = None\n",
    "    source    = None\n",
    "    data      = None\n",
    "    headers   = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    def __init__(self, extractor='DefaultExtractor', **kwargs):\n",
    "        if kwargs.get('url'):\n",
    "            proxy = urllib2.ProxyHandler({'http': 'http://fw-bln.rki.local:8020'})\n",
    "            opener = urllib2.build_opener(proxy)\n",
    "            urllib2.install_opener(opener)\n",
    "            urllib2.urlopen('http://www.google.com')\n",
    "            request     = urllib2.Request(kwargs['url'], headers=self.headers)\n",
    "            connection  = urllib2.urlopen(request)\n",
    "            self.data   = connection.read()\n",
    "            encoding    = connection.headers['content-type'].lower().split('charset=')[-1]\n",
    "            if encoding.lower() == 'text/html':\n",
    "                encoding = charade.detect(self.data)['encoding']\n",
    "            self.data = unicode(self.data, encoding)\n",
    "        elif kwargs.get('html'):\n",
    "            self.data = kwargs['html']\n",
    "            if not isinstance(self.data, unicode):\n",
    "                self.data = unicode(self.data, charade.detect(self.data)['encoding'])\n",
    "        else:\n",
    "            raise Exception('No text or url provided')\n",
    "\n",
    "        try:\n",
    "            # make it thread-safe\n",
    "            if threading.activeCount() > 1:\n",
    "                if jpype.isThreadAttachedToJVM() == False:\n",
    "                    jpype.attachThreadToJVM()\n",
    "            lock.acquire()\n",
    "            \n",
    "            self.extractor = jpype.JClass(\n",
    "                \"de.l3s.boilerpipe.extractors.\"+extractor).INSTANCE\n",
    "        finally:\n",
    "            lock.release()\n",
    "    \n",
    "        reader = StringReader(self.data)\n",
    "        self.source = BoilerpipeSAXInput(InputSource(reader)).getTextDocument()\n",
    "        self.extractor.process(self.source)\n",
    "    \n",
    "    def getText(self):\n",
    "        return self.source.getContent()\n",
    "    \n",
    "    def getHTML(self):\n",
    "        highlighter = HTMLHighlighter.newExtractingInstance()\n",
    "        return highlighter.process(self.source, self.data)\n",
    "    \n",
    "    def getImages(self):\n",
    "        extractor = jpype.JClass(\n",
    "            \"de.l3s.boilerpipe.sax.ImageExtractor\").INSTANCE\n",
    "        images = extractor.process(self.source, self.data)\n",
    "        jpype.java.util.Collections.sort(images)\n",
    "        images = [\n",
    "            {\n",
    "                'src'   : image.getSrc(),\n",
    "                'width' : image.getWidth(),\n",
    "                'height': image.getHeight(),\n",
    "                'alt'   : image.getAlt(),\n",
    "                'area'  : image.getArea()\n",
    "            } for image in images\n",
    "        ]\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_url = 'http://www.who.int/csr/don/15-october-2018-chikungunya-sudan/en/'\n",
    "extractor = Extractor(extractor='ArticleExtractor', url=your_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_text = extractor.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epitator.annotator import AnnoDoc\n",
    "from epitator.geoname_annotator import GeonameAnnotator\n",
    "from epitator.count_annotator import CountAnnotator\n",
    "from epitator.date_annotator import DateAnnotator\n",
    "from epitator.resolved_keyword_annotator import ResolvedKeywordAnnotator\n",
    "from epitator.structured_incident_annotator import StructuredIncidentAnnotator\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_doc = AnnoDoc(extracted_text,date=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_doc.add_tier(ResolvedKeywordAnnotator())\n",
    "logger.info('resolved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_doc.add_tier(DateAnnotator())\n",
    "logger.info('dates annotated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_doc.add_tier(CountAnnotator())\n",
    "logger.info('counts annotated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_doc.add_tier(GeonameAnnotator())\n",
    "logger.info('geonames annotated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_doc.add_tier(StructuredIncidentAnnotator())\n",
    "logger.info('structured incidents annotated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Tier does not exist: diseases\n",
      "Warning! Tier does not exist: hosts\n",
      "Warning! Tier does not exist: modes\n",
      "Warning! Tier does not exist: pathogens\n",
      "Warning! Tier does not exist: symptoms\n"
     ]
    }
   ],
   "source": [
    "anno_doc.filter_overlapping_spans(tier_names=['dates','geonames','diseases','hosts','modes','pathogens','symptoms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnoTier([AnnoSpan(129-149, four suspected cases), AnnoSpan(351-365, suspected case), AnnoSpan(450-454, male), AnnoSpan(501-534, cases have been reported in three), AnnoSpan(622-624, 24), AnnoSpan(650-652, 22), AnnoSpan(787-804, an additional 100), AnnoSpan(853-856, ten), AnnoSpan(1142-1163, total of 13 978 cases)])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_doc.tiers['counts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
