{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_md')\n",
    "from epitator.annotator import AnnoDoc\n",
    "from epitator.geoname_annotator import GeonameAnnotator\n",
    "from epitator.resolved_keyword_annotator import ResolvedKeywordAnnotator\n",
    "from epitator.count_annotator import CountAnnotator\n",
    "from epitator.date_annotator import DateAnnotator\n",
    "from boilerpipe.extract import Extractor\n",
    "from itertools import groupby\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import epitator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "type(pd.DataFrame({\"a\":[1],\"b\":[2]})) == pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Ereignisdatenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ereignisdatenbank = pd.read_csv(\"Ereignisse_utf8.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ereignisdatenbank.columns = list(map(lambda x:x.strip(\" \"),ereignisdatenbank.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_not_null = ereignisdatenbank[pd.notnull(ereignisdatenbank[\"Ausgangs- bzw. Ausbruchsland\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = countries_not_null[\"Ausgangs- bzw. Ausbruchsland\"].copy(deep=True)\n",
    "countries = list(map(lambda x:x.strip(\" \"),countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nigeria\n",
      "Benin\n",
      "Liberia\n",
      "Ghana\n",
      "Fiji\n",
      "Zentralafrikanische Republik\n",
      "Angola\n",
      "Äthiopien\n",
      "Zambia\n",
      "Südafrika\n",
      "Simbabwe\n",
      "Tschad\n",
      "Namibia\n",
      "Yemen\n",
      "Sudan\n",
      "Indien\n",
      "Burundi\n",
      "Demokratische Republik Kongo\n",
      "Kenia\n",
      "Malawi\n",
      "Mosambik\n",
      "Nigeria\n",
      "Somalia\n",
      "Tanzania\n",
      "Uganda\n",
      "Oman\n",
      "Saudi-Arabien\n",
      "Deutschland\n",
      "Syrien\n",
      "Deutschland\n",
      "Südsudan\n",
      "Südafrika\n",
      "Südafrika\n",
      "Kenia\n",
      "Namibia\n",
      "Bangladesch\n",
      "Bangladesch\n",
      "Bangladesch\n",
      "Brasilien\n",
      "Brasilien\n",
      "Brasilien\n",
      "Nigeria\n",
      "Nigeria\n",
      "Nigeria\n",
      "La Reunion\n",
      "La Reunion\n",
      "La Reunion\n",
      "Liberia\n",
      "Demokratische Republik Kongo\n",
      "Zentralafrikanische Republik\n",
      "Namibia\n",
      "Uganda\n",
      "Australien\n",
      "Brasilien\n",
      "Pakistan\n",
      "La Reunion\n",
      "Madagaskar\n",
      "Südafrika\n",
      "Liberia\n",
      "Bolivien\n",
      "Simbabwe\n",
      "Uganda\n",
      "Angola\n",
      "Angola\n",
      "Demokratische Republik Kongo\n",
      "Malawi\n",
      "Mosambik\n",
      "Nigeria\n",
      "Nigeria\n",
      "Nigeria\n",
      "Nigeria\n",
      "Kenia\n",
      "Südafrika\n",
      "Tanzania\n",
      "Zambia\n",
      "Somalia\n",
      "Yemen\n",
      "Kongo\n",
      "Vereinigtes Königreich\n",
      "La Reunion\n",
      "Schweiz\n",
      "Algerien\n",
      "Venezuela\n",
      "La Reunion\n",
      "Israel\n",
      "Australien\n",
      "La Reunion\n",
      "La Reunion\n",
      "La Reunion\n",
      "Uganda\n",
      "Kenia\n",
      "Trinidad & Tobago\n",
      "Brasilien\n",
      "USA\n",
      "Niger\n",
      "Brasilien\n",
      "Nigeria\n",
      "Demokratische Republik Kongo\n",
      "Frankreich\n",
      "Mauretanien\n",
      "Brasilien\n",
      "Tanzania\n",
      "Liberia\n",
      "Uganda\n",
      "Nigeria\n",
      "Nigeria\n",
      "Demokratische Republik Kongo\n",
      "Demokratische Republik Kongo\n",
      "Malawi\n",
      "Costa Rica\n",
      "Simbabwe\n",
      "La Reunion\n",
      "Demokratische Republik Kongo\n",
      "Demokratische Republik Kongo\n",
      "Nigeria\n",
      "Indien\n",
      "Kamerun\n",
      "Frankreich\n",
      "Peru\n",
      "nan\n",
      "Demokratische Republik Kongo\n",
      "Demokratische Republik Kongo\n",
      "Indien\n",
      "Indien\n",
      "Demokratische Republik Kongo\n",
      "Demokratische Republik Kongo\n",
      "Brasilien\n",
      "La Reunion\n",
      "Demokratische Republik Kongo\n",
      "Indien\n",
      "Demokratische Republik Kongo\n",
      "Demokratische Republik Kongo\n",
      "Demokratische Republik Kongo\n",
      "Demokratische Republik Kongo\n",
      "Indien\n",
      "Demokratische Republik Kongo\n",
      "Indien\n",
      "Iran\n",
      "Nigeria\n",
      "Saudi-Arabien\n",
      "Indien\n",
      "Saudi-Arabien\n",
      "Indien\n",
      "Demokratische Republik Kongo\n",
      "Indien\n",
      "Kamerun\n",
      "Taiwan\n",
      "Vereinigte Arabische Emirate\n",
      "Kenia\n",
      "Brasilien\n",
      "Uganda\n",
      "Nigeria\n",
      "Französisch-Polynesien\n",
      "USA\n",
      "Pakistan\n",
      "USA\n",
      "Sri Lanka\n",
      "Uganda\n",
      "Liberia\n",
      "El Salvador\n",
      "Venezuela\n",
      "Venezuela\n",
      "Kenia\n",
      "Kenia\n",
      "La Reunion\n",
      "UK\n",
      "nan\n",
      "Frankreich\n",
      "Papua-Neuguinea\n",
      "Westafrika\n",
      "Kenia\n",
      "Französich_Polynesien\n",
      "Kenia\n",
      "DRCongo\n",
      "Spanien\n",
      "Irak\n",
      "Kolumbien\n",
      "Namibia\n",
      "Demokratische Republik Kongo\n",
      "Indien\n",
      "DRCongo\n",
      "Philippinen\n",
      "Kenia\n",
      "Namibia\n",
      "Deutschland\n",
      "USA\n",
      "Algerien\n",
      "Liberia\n",
      "Indien\n",
      "La Reunion\n",
      "Pakistan\n",
      "Kenia\n",
      "Peru\n",
      "DRCongo\n",
      "nan\n",
      "Yemen\n",
      "Italien, Griechenland, Ungarn, Rumänien\n",
      "Uganda\n",
      "Uganda\n",
      "nan\n",
      "DRC\n",
      "Saudi-Arabien\n",
      "Papua-Neuguinea\n",
      "Süd Sudan\n",
      "Indien\n",
      "Nordeuropa\n",
      "Mali\n",
      "Uganda\n",
      "Nigeria\n",
      "Tschechien\n",
      "Mali\n",
      "Ukraine\n",
      "Spanien\n",
      "Angola\n",
      "Spanien\n",
      "China\n",
      "La Reunion\n",
      "Italien, Serbien, Griechenland, Rumänien, Ungarn, Frankreich, Kosovo, Albanien, Macedonien, Montenegro, Serbien, Türkei\n",
      "Polen\n",
      "DRC\n",
      "Brasilien\n",
      "Brasilien\n",
      "nan\n",
      "nan\n",
      "Papua-Neuguinea\n",
      "nan\n",
      "Papua-Neuguinea\n",
      "Papua-Neuguinea\n",
      "Papua-Neuguinea\n",
      "Papua-Neuguinea\n",
      "Typhus\n",
      "USA\n",
      "USA\n",
      "Schweiz\n",
      "Uganda\n",
      "Nigeria\n",
      "DRC\n",
      "Algerien\n",
      "Frankreich\n",
      "China\n",
      "Pakistan\n",
      "Vereinigtes Königreich\n",
      "Italien, Griechenland, Rumanien, Ungarn, Frankreich\n",
      "Afghanistan, DR Congo, Nigeria, Somalia\n",
      "Kanada\n",
      "USA, Delaware\n",
      "Vereinigtes Königreich\n",
      "Kongo\n",
      "Französiche Guyana\n",
      "Italien, Griechenland, Rumanien, Ungarn, Frankreich\n",
      "Italien, Griechenland, Rumanien, Ungarn, Frankreich\n",
      "Algerien\n",
      "nan\n",
      "China\n",
      "Algerien\n",
      "Saudi-Arabien\n",
      "Saudi-Arabien\n",
      "USA\n",
      "Vereinigtes Königreich\n",
      "Israel\n",
      "China\n",
      "Kongo\n",
      "Pakistan\n",
      "Papua-Neuguinea\n",
      "nan\n",
      "Serbien, Italien, Griechenland, Ungarn,  Rumänien\n",
      "Kroatien\n",
      "Afghanistan\n",
      "Südafrika\n",
      "Papua-Neuguinea\n",
      "Papua-Neuguinea\n",
      "nan\n",
      "VAE Dubai\n",
      "Frankreich\n",
      "Myanmar\n",
      "Myanmar\n",
      "Myanmar\n",
      "Haiti\n",
      "Nigeria\n",
      "Kuwait\n",
      "VAE\n",
      "Italien\n",
      "Nigeria\n",
      "Papua-Neuguinea\n",
      "Nigeria\n",
      "DRC\n",
      "Südafrika\n",
      "Papua-Neuguinea\n",
      "Simbabwe\n",
      "Simbabwe\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      None\n",
       "1      None\n",
       "2      None\n",
       "3      None\n",
       "4      None\n",
       "5      None\n",
       "6      None\n",
       "7      None\n",
       "8      None\n",
       "9      None\n",
       "10     None\n",
       "11     None\n",
       "12     None\n",
       "13     None\n",
       "14     None\n",
       "15     None\n",
       "16     None\n",
       "17     None\n",
       "18     None\n",
       "19     None\n",
       "20     None\n",
       "21     None\n",
       "22     None\n",
       "23     None\n",
       "24     None\n",
       "25     None\n",
       "26     None\n",
       "27     None\n",
       "28     None\n",
       "29     None\n",
       "       ... \n",
       "274    None\n",
       "275    None\n",
       "276    None\n",
       "277    None\n",
       "278    None\n",
       "279    None\n",
       "280    None\n",
       "281    None\n",
       "282    None\n",
       "283    None\n",
       "284    None\n",
       "285    None\n",
       "286    None\n",
       "287    None\n",
       "288    None\n",
       "289    None\n",
       "290    None\n",
       "291    None\n",
       "292    None\n",
       "293    None\n",
       "294    None\n",
       "295    None\n",
       "296    None\n",
       "297    None\n",
       "298    None\n",
       "299    None\n",
       "300    None\n",
       "301    None\n",
       "302    None\n",
       "303    None\n",
       "Name: Ausgangs- bzw. Ausbruchsland, Length: 304, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_country_names(country):\n",
    "    \"\"\"Takes a list of countries (from Ereginsdatenbank) and returns a set of cleaned country names\"\"\"\n",
    "\n",
    "    country = str(country)\n",
    "    card_dir = re.compile(r\"(Süd|Nord|West|Ost)\\s(\\S*)\")  # Matches cardinal directions and the string after it\n",
    "\n",
    "    # Because someone used new lines in entries instead of comma to list countries\n",
    "    countries_unique  = re.sub(r'\\n', ', ', country)\n",
    "\n",
    "    # Because the line above adds one comma to much\n",
    "    countries_unique = re.sub(r',,', ',',  countries_unique)\n",
    "    countries_unique = re.sub(r'\\(.*\\)', \"\", countries_unique).strip(\" \")\n",
    "    countries_unique.replace(\"&\", \"und\")\n",
    "    if \",\" in countries_unique:\n",
    "        countries_unique.split(\",\")\n",
    "    if type(countries_unique) != list:\n",
    "        countries_unique.replace(\"_\", \" \") \n",
    "\n",
    "    print(countries_unique)\n",
    "\n",
    "ereignisdatenbank[\"Ausgangs- bzw. Ausbruchsland\"].apply(clean_country_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18 01 2018',\n",
       " '08 01 2018',\n",
       " '01 01 2018',\n",
       " '23 02 2018',\n",
       " '20 03 2018',\n",
       " '21 03 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '04 10 2017',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '01 01 2018',\n",
       " 'nan',\n",
       " '20 11 2018',\n",
       " '12 08 2017',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '01 01 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '07 03 2018',\n",
       " '07 12 2017',\n",
       " '04 12 2017',\n",
       " '04 12 2017',\n",
       " '15 12 2017',\n",
       " '08 09 2017',\n",
       " '08 11 2017',\n",
       " '08 11 2017',\n",
       " '08 11 2017',\n",
       " '01 07 2017',\n",
       " '01 07 2017',\n",
       " '01 07 2017',\n",
       " 'nan',\n",
       " '01 01 2018',\n",
       " '01 01 2018',\n",
       " '16 03 2018',\n",
       " '16 03 2018',\n",
       " '16 03 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '29 03 2018',\n",
       " 'nan',\n",
       " '17 01 2018',\n",
       " '01 07 2017',\n",
       " '2016',\n",
       " '16 03 2018',\n",
       " 'nan',\n",
       " '04 12 2017',\n",
       " '01 01 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '12 02 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '01 01 2018',\n",
       " '20 11 2018',\n",
       " '12 08 2017',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '01 01 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '01 01 2018',\n",
       " '04 10 2017',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '18 03 2018',\n",
       " '23 03 2018',\n",
       " '01 01 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '01 07 2016',\n",
       " '16 03 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '16 03 2018',\n",
       " '16 03 2018',\n",
       " '16 03 2018',\n",
       " 'nan',\n",
       " '?',\n",
       " '08 03 2018',\n",
       " '01 07 2017',\n",
       " '01 08 2016',\n",
       " '03 05 2018',\n",
       " '01 07 2017',\n",
       " '01 09 2017',\n",
       " '04 04 2018',\n",
       " 'nan',\n",
       " '16 04 2018',\n",
       " '01 07 2017',\n",
       " '01 01 2018',\n",
       " 'nan',\n",
       " '12 02 2018',\n",
       " '18 01 2018',\n",
       " '01 01 2018',\n",
       " '01 01 2018',\n",
       " '04 04 2018',\n",
       " '43424',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '16 03 2018',\n",
       " '04 04 2018',\n",
       " '04 04 2018',\n",
       " '30 04 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '16 04 2018',\n",
       " 'nan',\n",
       " '04 04 2018',\n",
       " '04 04 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '04 04 2018',\n",
       " '04 04 2018',\n",
       " '01 07 2017',\n",
       " '16 03 2018',\n",
       " '04 04 2018',\n",
       " 'nan',\n",
       " '04 04 2018',\n",
       " '04 04 2018',\n",
       " '04 04 2018',\n",
       " '04 04 2018',\n",
       " 'nan',\n",
       " '04 04 2018',\n",
       " 'nan',\n",
       " '01 01 2018',\n",
       " '18 09 2017',\n",
       " '17 05 2018',\n",
       " 'nan',\n",
       " '17 05 2018',\n",
       " 'nan',\n",
       " '04 04 2018',\n",
       " 'nan',\n",
       " '30 04 1018',\n",
       " '01 01 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '11 06 2018',\n",
       " '18 09 2017',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '08 06 2018',\n",
       " '08 06 2018',\n",
       " '16 03 2018',\n",
       " 'nan',\n",
       " '18 06 2018',\n",
       " 'nan',\n",
       " '28 04 2018',\n",
       " '01 01 2018',\n",
       " '08 06 2018',\n",
       " '23  KW ',\n",
       " 'nan',\n",
       " '04 04 2018',\n",
       " 'Januar 2018',\n",
       " '30 06 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '04 04 2018',\n",
       " 'Seit 2018',\n",
       " 'nan',\n",
       " '08 09 2017',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Juni 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '16 03 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '2010',\n",
       " 'nan',\n",
       " 'Mai 2018',\n",
       " 'ongoing',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'ongoing',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '23 6 2018 1  Todesfall',\n",
       " 'Hämorrhagisches Fieber',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Mai 2018?',\n",
       " '08 08 2018',\n",
       " '28 06 2018\\nSechs Kühe auf einem Bauernhof gestorben ',\n",
       " '10 08 2018',\n",
       " '18 08 2018',\n",
       " '16 08 2018',\n",
       " 'Trend',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '13 08 2018',\n",
       " 'nan',\n",
       " '14 08 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '07 08 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '07 08 2018',\n",
       " '30 08 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '13 08 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '01 01 2018',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '04 12 2017',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '?',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " '30 04 2018',\n",
       " 'Mai 2018?',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "strptime = datetime.strptime\n",
    "strftime = datetime.strftime\n",
    "\n",
    "dates = ereignisdatenbank[\"Frühestbekannter Ereignisbeginn\"].tolist()\n",
    "dates = [str(date).replace('.', ' ') for date in dates]\n",
    "list(map(lambda x: strptime(x, '%d %m %Y').strftime(\"%Y-%m-%d\") \n",
    "         if re.match(r\"\\d\\d\\.\\d\\d\\.\\d\\d\\d\\d\", x) else x, dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the testing begin\n",
    "## Parser for Wikipedia table of countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://de.wikipedia.org/wiki/Liste_der_Staaten_der_Erde\")\n",
    "soup = BeautifulSoup(req.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_soup = soup.find(\"table\",class_=\"wikitable sortable zebra\").find(\"tbody\") # Find table with of all countries\n",
    "parsed_soup = parsed_soup.find_all(\"tr\") # Get entries of countries form table\n",
    "amount_countries = len(parsed_soup)\n",
    "parsed_soup = [parsed_soup[i].find_all('td') \\\n",
    "               for i in range(amount_countries)] # Extract table entries from country entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dict = {\"state_name_de\":[],\n",
    "             \"full_state_name_de\":[],\n",
    "             \"capital_de\":[],\"translation_state_name\":[],\n",
    "             \"wiki_abbreviations\":[]}\n",
    "dash = u\"\\u2014\" # Used dash for missing entry in Wikipedia table\n",
    "regex = re.compile(r\"\\[\\d*\\]\") # To remove footnotes in the names\n",
    "for i in range(amount_countries):\n",
    "    try:\n",
    "        state_name_de = regex.sub(\"\",(parsed_soup[i][0].text).replace(\"\\n\",\"\")\\\n",
    "                                  .replace(\"\\xad\",\"\")) # Remove soft hyphen used in Zentralafr. Rep.\n",
    "        \n",
    "        # Remove additional information that are note the state name\n",
    "        state_name_de = re.sub(r\"((mit)|(ohne)).*\",\"\",state_name_de) \n",
    "        wiki_dict[\"state_name_de\"].append(state_name_de) \n",
    "        \n",
    "        # Removes new lines\n",
    "        wiki_dict[\"full_state_name_de\"].append(regex.sub(\"\",parsed_soup[i][1].text).replace(\"\\n\",\"\")) \n",
    "        wiki_dict[\"capital_de\"].append(regex.sub(\"\",parsed_soup[i][2].text).replace(\"\\n\",\"\")) \n",
    "        wiki_dict[\"translation_state_name\"].append(regex.sub(\"\",parsed_soup[i][10].text).replace(\"\\n\",\"\"))\n",
    "        \n",
    "        # Also removes new lines. Column 7 and 8 are long and short official abbreviations for the countries\n",
    "        list_abbreviation = [parsed_soup[i][7].text.replace(\"\\n\",\"\"),parsed_soup[i][8].text.replace(\"\\n\",\"\")] \n",
    "        \n",
    "        # Remove empty abbrev. entries. E.g. Abchasien | [\"ABC\", \"-\"]| Abkhazia --> Abchasien |\"ABC\" | Abhkazia\n",
    "        list_abbreviation = list(filter(lambda x: x not in [\"\",dash],list_abbreviation)) \n",
    "        if len(list_abbreviation) > 1:\n",
    "            wiki_dict[\"wiki_abbreviations\"].append(list_abbreviation)\n",
    "        else:\n",
    "            # When after removal of empty entries no abbrev. remains, enter a single dash\n",
    "            wiki_dict[\"wiki_abbreviations\"].append(dash)\n",
    "    except IndexError as e: # Because header and footer are part of the table, soup opperations don't work\n",
    "        if i not in [0, 213]:\n",
    "            print(\"Entry {} failed unexpected because of {}\".format(i,e)) # Except that the first and last entry fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name_de</th>\n",
       "      <th>full_state_name_de</th>\n",
       "      <th>capital_de</th>\n",
       "      <th>translation_state_name</th>\n",
       "      <th>wiki_abbreviations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erde</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>Earth</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Europäische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Brüssel</td>\n",
       "      <td>European Union</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Union Südamerikanischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Union of South American Nations</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afrikanische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Addis Abeba</td>\n",
       "      <td>African Union</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Verband Südostasiatischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Association of Southeast Asian Nations</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        state_name_de full_state_name_de   capital_de  \\\n",
       "0                                Erde                  —            —   \n",
       "1                   Europäische Union                  —      Brüssel   \n",
       "2    Union Südamerikanischer Nationen                  —        Quito   \n",
       "3                  Afrikanische Union                  —  Addis Abeba   \n",
       "4  Verband Südostasiatischer Nationen                  —      Jakarta   \n",
       "\n",
       "                   translation_state_name wiki_abbreviations  \n",
       "0                                   Earth                  —  \n",
       "1                          European Union                  —  \n",
       "2         Union of South American Nations                  —  \n",
       "3                           African Union                  —  \n",
       "4  Association of Southeast Asian Nations                  —  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_country_list = pd.DataFrame.from_dict(wiki_dict)\n",
    "wikipedia_country_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate(country_name):\n",
    "    \"\"\"Abbreviates entries of list of country names\n",
    "    \n",
    "    Example: United Kingdom --> UK\n",
    "    \"\"\"\n",
    "    country_name = re.sub(r\"\\(.*\\)\",\"\", country_name) # Delete content in paranthesis since not relevant for abbrev.\n",
    "    if \",\" in country_name:\n",
    "        # If there is a comma, switch order to yield a more common abbreviation: Korea, Nord --> Nord Korea\n",
    "        matched = re.match(r\"([A-Za-z]*), (.*)\",country_name) # Extract capital letters\n",
    "        country_name = matched[2] + \" \" + matched[1] # Patch capital letters together\n",
    "    abbreviation = None\n",
    "    if len(re.findall(r\"([A-Z|Ä|Ö|Ü])\",country_name)) > 1:\n",
    "        abbreviation = \"\".join(re.findall(r\"([A-Z|Ä|Ö|Ü])\",country_name))\n",
    "    return abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Search for names that might have abbreviations. If they consist of two or more words that start with a capital\n",
    "# letter, it makes an abbreviation out of it\n",
    "abb_state_de = list(map(abbreviate,wikipedia_country_list[\"state_name_de\"].tolist()))\n",
    "abb_full_state_de = list(map(abbreviate,wikipedia_country_list[\"full_state_name_de\"].tolist()))\n",
    "abb_state_trans = list(map(abbreviate,wikipedia_country_list[\"translation_state_name\"].tolist()))\n",
    "                   \n",
    "abbreviations = [list(a) for a in zip(abb_state_de,abb_full_state_de,abb_state_trans)]\n",
    "abbreviations = [list(filter(None,abb)) for abb in abbreviations if str(abb) != 'None'] # Removes Nones\n",
    "abbreviations = list(map(lambda x: list(set(x)) if len(x)>0 else \"-\", abbreviations)) # Removes redundance\n",
    "#abbreviations = list(map(\", \".join,cleaned_abbreviations)) # Unpack list of abbreviations to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_name_de</th>\n",
       "      <th>full_state_name_de</th>\n",
       "      <th>capital_de</th>\n",
       "      <th>translation_state_name</th>\n",
       "      <th>wiki_abbreviations</th>\n",
       "      <th>inoff_abbreviations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erde</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>Earth</td>\n",
       "      <td>—</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Europäische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Brüssel</td>\n",
       "      <td>European Union</td>\n",
       "      <td>—</td>\n",
       "      <td>[EU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Union Südamerikanischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Union of South American Nations</td>\n",
       "      <td>—</td>\n",
       "      <td>[USN, USAN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afrikanische Union</td>\n",
       "      <td>—</td>\n",
       "      <td>Addis Abeba</td>\n",
       "      <td>African Union</td>\n",
       "      <td>—</td>\n",
       "      <td>[AU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Verband Südostasiatischer Nationen</td>\n",
       "      <td>—</td>\n",
       "      <td>Jakarta</td>\n",
       "      <td>Association of Southeast Asian Nations</td>\n",
       "      <td>—</td>\n",
       "      <td>[VSN, ASAN]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        state_name_de full_state_name_de   capital_de  \\\n",
       "0                                Erde                  —            —   \n",
       "1                   Europäische Union                  —      Brüssel   \n",
       "2    Union Südamerikanischer Nationen                  —        Quito   \n",
       "3                  Afrikanische Union                  —  Addis Abeba   \n",
       "4  Verband Südostasiatischer Nationen                  —      Jakarta   \n",
       "\n",
       "                   translation_state_name wiki_abbreviations  \\\n",
       "0                                   Earth                  —   \n",
       "1                          European Union                  —   \n",
       "2         Union of South American Nations                  —   \n",
       "3                           African Union                  —   \n",
       "4  Association of Southeast Asian Nations                  —   \n",
       "\n",
       "  inoff_abbreviations  \n",
       "0                   -  \n",
       "1                [EU]  \n",
       "2         [USN, USAN]  \n",
       "3                [AU]  \n",
       "4         [VSN, ASAN]  "
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_country_list[\"inoff_abbreviations\"] =  abbreviations\n",
    "wikipedia_country_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ontology/Comparison (Transformed to .py until here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'Munich',\n",
       " 'textOffsets': [[10, 15]],\n",
       " 'geoname': {'geonameid': '2867714',\n",
       "  'name': 'Munich',\n",
       "  'feature_code': 'PPLA',\n",
       "  'country_code': 'DE',\n",
       "  'admin1_code': '02',\n",
       "  'admin2_code': '091',\n",
       "  'admin3_code': '09162',\n",
       "  'admin4_code': '09162000',\n",
       "  'longitude': 11.57549,\n",
       "  'latitude': 48.13743,\n",
       "  'population': 1260391,\n",
       "  'asciiname': 'Munich',\n",
       "  'names_used': 'Munic',\n",
       "  'name_count': 88,\n",
       "  'country_name': 'Federal Republic of Germany',\n",
       "  'admin1_name': 'Bavaria',\n",
       "  'admin2_name': 'Upper Bavaria',\n",
       "  'admin3_name': 'Kreisfreie Stadt München',\n",
       "  'parents': [],\n",
       "  'score': 0.21453999698331166}}"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from epitator.annotator import AnnoDoc\n",
    "from epitator.geoname_annotator import location_contains\n",
    "doc = AnnoDoc(\"I live in Munic!\")\n",
    "doc.add_tiers(GeonameAnnotator())\n",
    "annotations = doc.tiers[\"geonames\"]\n",
    "geoname = annotations[0]\n",
    "geoname.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To .py and renamed to clean_country_names\n",
    "def clean_entries(countries):\n",
    "    \"\"\"Takes a list of countries (from Ereginsdatenbank) and returns a set of cleaned country names\"\"\"\n",
    "    card_dir = re.compile(r\"(Süd|Nord|West|Ost)\\s(\\S*)\") # Matches cardinal directions and the string after it\n",
    "    countries_unique = list(set(countries)) # Optional. Used for better overview and faster calculation\n",
    "    \n",
    "    # Because someone used new lines in entries instead of comma to list countries\n",
    "    countries_unique = list(map(lambda x: re.sub(r'\\n',', ',x), countries_unique))\n",
    "    \n",
    "    # Because the line above adds one comma to much\n",
    "    countries_unique = list(map(lambda x: re.sub(r',,',',',x), countries_unique)) \n",
    "    countries_unique = list(map(lambda x: re.sub(r'\\(.*\\)',\"\",x).strip(\" \"), countries_unique))\n",
    "    countries_unique = list(map(lambda x: x.replace(\"&\", \"und\"), countries_unique))\n",
    "    countries_unique = list(map(lambda x: x.split(\",\") if \",\" in x else x, countries_unique)) # For entries with more than one country\n",
    "    countries_unique = list(map(lambda x: x.replace(\"_\",\" \") if type(x) != list else x,countries_unique))\n",
    "    \n",
    "    # To transform Süd Sudan to Südsudan\n",
    "    try:\n",
    "        countries_unique = list(map(lambda x: card_dir.match(x)[1] + card_dir.match(x)[2].lower()\\\n",
    "                                    if type(x) != list and card_dir.match(x) else x, countries_unique ))\n",
    "    except IndexError:\n",
    "        print(card_dir.match, \" has a cardinal direction but is not of the form 'Süd Sudan'\")\n",
    "    \n",
    "    #\"Recursively\" clean lists\n",
    "    countries_unique = list(map(lambda x: clean_entries(x) if type(x) == list else x,countries_unique))\n",
    "    return countries_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test succesful\n"
     ]
    }
   ],
   "source": [
    "# Test for clean_entries()\n",
    "from deep_eq import deep_eq\n",
    "clean_entries(countries)\n",
    "example_countries_to_clean = [\" Australien\",\n",
    "                              \"Kongo \\nUSA\",\n",
    "                              \"Italien, Deutschland, Belgien \",\n",
    "                              \"Franz._Polynesien\", \n",
    "                              \"Trinidad & Tobago\"]\n",
    "expected_countries_to_clean = [\"Trinidad und Tobago\",\"Australien\"\n",
    "                               ,['Belgien', 'Deutschland', 'Italien']\n",
    "                               ,[\"USA\", \"Kongo\"], \"Franz. Polynesien\"]\n",
    "if deep_eq(clean_entries(example_countries_to_clean),expected_countries_to_clean):\n",
    "    print(\"Test succesful\")\n",
    "else:\n",
    "    print(\"Test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING. RETURNS TUPLE WITH ABBREVIATION AND TRANSLATION\n",
    "# # Takes a list of not matched/translated entries and tries to match them to the wikipedia table and find the full name\n",
    "# countries_not_translated = [entry for entry in countries_unique \\\n",
    "#                             if entry not in wikipedia_country_list[\"state_name_de\"].tolist()]\n",
    "# def translate_abbreviation(to_translate):\n",
    "#     abb_to_country = []\n",
    "#     if type(to_translate) == str:\n",
    "#         to_translate = [to_translate]\n",
    "#     for column in [\"wiki_abbreviations\",\"inoff_abbreviations\"]:\n",
    "#         for potential_abbreviation in to_translate:\n",
    "#             if type(potential_abbreviation) == str:\n",
    "#                 for i, abbreviation in enumerate(wikipedia_country_list[column]):\n",
    "#                     if potential_abbreviation in abbreviation:\n",
    "#                         abb_to_country.append((potential_abbreviation,\\\n",
    "#                                                wikipedia_country_list[\"translation_state_name\"].tolist()[i]))\n",
    "#                         to_translate.remove(potential_abbreviation)\n",
    "#             elif type(potential_abbreviation) == list:\n",
    "#                 abb_to_country.append(translate_abbreviation(potential_abbreviation))\n",
    "#     return(abb_to_country,to_translate)\n",
    "\n",
    "# abbreviation_tuple, countries_not_translated = translate_abbreviation(countries_not_translated)\n",
    "\n",
    "# #abbreviation_tuple\n",
    "# #print(\"****************************************************************************************************************\")\n",
    "# #print(countries_not_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO if abbreviation found, don't continue searching\n",
    "def translate_abbreviation(to_translate):\n",
    "    \"\"\"Takes a list of countries and/or abbreviations and translates the abbreviations to the full state name\"\"\"\n",
    "    to_return = []\n",
    "    if type(to_translate) == str:\n",
    "        to_translate = [to_translate]\n",
    "    for potential_abbreviation in to_translate:\n",
    "        if type(potential_abbreviation) == str and not re.findall(r\"([^A-Z]+)\",potential_abbreviation):\n",
    "            \n",
    "            # First check the official abrev. than the self created ones e.g. VAE for the Emirates\n",
    "            for column in [\"wiki_abbreviations\",\"inoff_abbreviations\"]:\n",
    "                for i, abbreviation in enumerate(wikipedia_country_list[column]):\n",
    "                    if potential_abbreviation in abbreviation:\n",
    "                        to_return.append(wikipedia_country_list[\"state_name_de\"].tolist()[i])\n",
    "        elif type(potential_abbreviation) == list:\n",
    "            list_entry = [translate_abbreviation(nested_entry) for nested_entry in potential_abbreviation]\n",
    "            flattened = [entry for sublist in list_entry for entry in sublist]\n",
    "            to_return.append(flattened)\n",
    "        else:\n",
    "            to_return.append(potential_abbreviation)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test succesful\n"
     ]
    }
   ],
   "source": [
    "# Test for translate_abbreviation()\n",
    "example_to_abbreviate = [\"USA\",\"VAE\",'Italien', \"DR Cong\",[\"Deutschland\", \"EU\"],[\"Belgien\",\"DRC\"]]\n",
    "desired_output = ['Vereinigte Staaten','Vereinigte Arabische Emirate','Italien','DR Cong',\n",
    "                  ['Deutschland', 'Europäische Union'],\n",
    "                  ['Belgien', 'Kongo, Demokratische Republik']]\n",
    "if deep_eq(translate_abbreviation(example_to_abbreviate),desired_output):\n",
    "    print(\"Test succesful\")\n",
    "else:\n",
    "    Print(\"Test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SIMPLE TRANSLATION. FAST BUT DOES NOT TRANSLATE LISTS OF LISTS\n",
    "# # Translate German entries of Ereignisdatenbank to English. Might be inefficient since I go through the wiki list\n",
    "# # entirely which is longer then the list of countries to translate\n",
    "# translated_ereignisdatenbank_countries = [(entry,wikipedia_country_list[\"translation_state_name\"].tolist()[indx])\\\n",
    "#                                           for indx,entry \\\n",
    "#                                           in enumerate(wikipedia_country_list[\"state_name_de\"].tolist())\\\n",
    "#                                           if entry in countries_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "from didyoumean import didyoumean\n",
    "\n",
    "def translate(countries_unique):\n",
    "    \"\"\"Translate German entries of Ereignisdatenbank to English an returns tuple \n",
    "    of German word and English translation(s if ambigious)\"\"\" \n",
    "\n",
    "    continents = [\"europa\",\"africa\",\"america\",\"australien\",\"asia\"]\n",
    "    translated_ereignisdatenbank_countries = []\n",
    "    state_name_de = wikipedia_country_list[\"state_name_de\"].tolist()\n",
    "    full_state_name_de = wikipedia_country_list[\"full_state_name_de\"].tolist()\n",
    "    translation = wikipedia_country_list[\"translation_state_name\"].tolist()\n",
    "    \n",
    "    if type(countries_unique) == str:\n",
    "        countries_unique = [countries_unique]\n",
    "    \n",
    "    for entry in countries_unique:\n",
    "        if type(entry) == str:\n",
    "            sucessfull_search = list(filter(lambda x: re.findall(entry,x),state_name_de))\n",
    "            if sucessfull_search:\n",
    "                found = [translation[state_name_de.index(entry)] for entry in sucessfull_search]\n",
    "                if len(found) == 1:\n",
    "                    translated_ereignisdatenbank_countries.append((entry,found[0]))\n",
    "                else:\n",
    "                    \n",
    "                    # Check for idendity in not ambigious case otherwise e.g Niger --> (Niger, Nigeria)\n",
    "                    identical = [found_ent for found_ent in found if entry == found_ent]\n",
    "                    if identical:\n",
    "                        translated_ereignisdatenbank_countries.append((entry,identical[0]))\n",
    "                    else:\n",
    "                        translated_ereignisdatenbank_countries.append((entry,found))\n",
    "            else:\n",
    "                \n",
    "                # If entry not in state_name_de, search in full_state_name_de\n",
    "                sucessfull_search = list(filter(lambda x: re.findall(entry,x),full_state_name_de))\n",
    "                if sucessfull_search:\n",
    "                    found = [translation[full_state_name_de.index(entry)] for entry in sucessfull_search]\n",
    "                    if len(found) == 1:\n",
    "                        translated_ereignisdatenbank_countries.append((entry,found[0]))\n",
    "                    else:\n",
    "                        translated_ereignisdatenbank_countries.append((entry,found))\n",
    "                else:\n",
    "                    sucessfull_search_en = list(filter(lambda x: re.findall(entry,x),translation))\n",
    "                    if sucessfull_search_en:\n",
    "                        found = [state_name_de[translation.index(entry)] for entry in sucessfull_search_en]\n",
    "                        if len(found) == 1:\n",
    "                            translated_ereignisdatenbank_countries.append((found[0],entry))\n",
    "                        else:\n",
    "                            translated_ereignisdatenbank_countries.append((found,entry))\n",
    "                    else:\n",
    "                        \n",
    "                        # If there was not match at all, check for spelling mistakes\n",
    "                        did_u_mean = didyoumean.didYouMean(entry,state_name_de)\n",
    "                        \n",
    "                        \"\"\"Exlude words with continent names since there are countries with a continent name\n",
    "                        but there are also entries in the Ereignisdatenbank that mean the whole country. They\n",
    "                        are not matched (e.g. Nordafrika) and there is must not be a match, otherwise \n",
    "                        didYouMean would falsly return Südafrika.\n",
    "                        \"\"\"\n",
    "                        if did_u_mean and (did_u_mean not in continents):\n",
    "                            translated_ereignisdatenbank_countries.append(translate(did_u_mean))\n",
    "                        else:\n",
    "                            translated_ereignisdatenbank_countries.append(entry)\n",
    "                                                            \n",
    "        elif type(entry) == list:\n",
    "            translated_ereignisdatenbank_countries.append(translate(entry))\n",
    "        else:\n",
    "            translated_ereignisdatenbank_countries.append(entry)\n",
    "    return translated_ereignisdatenbank_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test succesfull\n"
     ]
    }
   ],
   "source": [
    "example_to_translate = [\"Deutschland\",\"Delaware\",[\"Kongo\",\"China\"],\"Niger\"]\n",
    "expected_result_translate = [('Deutschland', 'Germany'),\n",
    "                             'Delaware',\n",
    "                             [('Kongo',\n",
    "                               ['Congo, Democratic Republic of the (Kinshasa)','Congo, Republic of (Brazzaville)']),\n",
    "                              ('China','China')],\n",
    "                            ('Niger', 'Niger')]\n",
    "if deep_eq(translate(example_to_translate),expected_result_translate):\n",
    "    print(\"Test succesfull\")\n",
    "else:\n",
    "    print(\"Test failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Algerien', 'Algeria'),\n",
       " ('Peru', 'Peru'),\n",
       " ('Äthiopien', 'Ethiopia'),\n",
       " [('Ungarn', 'Hungary'),\n",
       "  ('Serbien', 'Serbia'),\n",
       "  ('Rumänien', 'Romania'),\n",
       "  ('Italien', 'Italy'),\n",
       "  ('Griechenland', 'Greece')],\n",
       " ('Sri Lanka', 'Sri Lanka'),\n",
       " ('El Salvador', 'El Salvador'),\n",
       " ('Jemen', 'Yemen'),\n",
       " ('Namibia', 'Namibia'),\n",
       " ('Simbabwe', 'Zimbabwe'),\n",
       " ('Kongo, Demokratische Republik',\n",
       "  'Congo, Democratic Republic of the (Kinshasa)'),\n",
       " ('Angola', 'Angola'),\n",
       " ('Burundi', 'Burundi'),\n",
       " ('Vereinigte Staaten', 'United States'),\n",
       " ('Kolumbien', 'Colombia'),\n",
       " ('Bolivien', 'Bolivia'),\n",
       " ('Schweiz', 'Switzerland'),\n",
       " ('Südsudan', 'South Sudan'),\n",
       " ('Haiti', 'Haiti'),\n",
       " ('Venezuela', 'Venezuela'),\n",
       " ('Niger', 'Niger'),\n",
       " ('Ghana', 'Ghana'),\n",
       " ('Costa Rica', 'Costa Rica'),\n",
       " ('China', 'China'),\n",
       " ('Bangladesch', 'Bangladesh'),\n",
       " ('Somalia', 'Somalia'),\n",
       " ('Syrien', 'Syria'),\n",
       " 'Französich Polynesien',\n",
       " ('Kanada', 'Canada'),\n",
       " ('Tansania', 'Tanzania'),\n",
       " ('Nigeria', 'Nigeria'),\n",
       " ('Taiwan', 'Taiwan oder Republic of China'),\n",
       " ('Saudi-Arabien', 'Saudi Arabia'),\n",
       " ('Südsudan', 'South Sudan'),\n",
       " ('Mosambik', 'Mozambique'),\n",
       " 'Nordeuropa',\n",
       " ('Kongo',\n",
       "  ['Congo, Democratic Republic of the (Kinshasa)',\n",
       "   'Congo, Republic of (Brazzaville)']),\n",
       " 'Typhus',\n",
       " 'DRCongo',\n",
       " ('Benin', 'Benin'),\n",
       " ('Liberia', 'Liberia'),\n",
       " ('Israel', 'Israel'),\n",
       " ('Trinidad und Tobago', 'Trinidad and Tobago'),\n",
       " ('Sambia', 'Zambia'),\n",
       " [('Ungarn', 'Hungary'),\n",
       "  ('Italien', 'Italy'),\n",
       "  [('Rumänien', 'Romania')],\n",
       "  ('Frankreich', 'France'),\n",
       "  ('Griechenland', 'Greece')],\n",
       " [('Afghanistan', 'Afghanistan'),\n",
       "  ('Nigeria', 'Nigeria'),\n",
       "  'DR Congo',\n",
       "  ('Somalia', 'Somalia')],\n",
       " ('Fidschi', 'Fiji'),\n",
       " ('Kamerun', 'Cameroon'),\n",
       " ('Irak', 'Iraq'),\n",
       " ('Vereinigte Arabische Emirate', 'United Arab Emirates'),\n",
       " ('Komoren', 'Comoros'),\n",
       " ('Vereinigtes Königreich', 'United Kingdom'),\n",
       " ('Deutschland', 'Germany'),\n",
       " ('Australien', 'Australia'),\n",
       " 'Französiche Guyana',\n",
       " ('Brasilien', 'Brazil'),\n",
       " ('Zentralafrikanische Republik', 'Central African Republic'),\n",
       " ('Spanien', 'Spain'),\n",
       " ('Ukraine', 'Ukraine'),\n",
       " ('Kroatien', 'Croatia'),\n",
       " ('Tschad', 'Chad'),\n",
       " ('Oman', 'Oman'),\n",
       " 'La Reunion',\n",
       " ('Kenia', 'Kenya'),\n",
       " ('Sudan', 'Sudan'),\n",
       " ('Polen', 'Poland'),\n",
       " ['Delaware', ('Vereinigte Staaten', 'United States')],\n",
       " 'VAE Dubai',\n",
       " ('Kuwait', 'Kuwait'),\n",
       " 'Französisch-Polynesien',\n",
       " ('Vereinigte Arabische Emirate', 'United Arab Emirates'),\n",
       " ('Frankreich', 'France'),\n",
       " ('Demokratische Republik Kongo',\n",
       "  'Congo, Democratic Republic of the (Kinshasa)'),\n",
       " ('Südafrika', 'South Africa'),\n",
       " ('Philippinen', 'Philippines'),\n",
       " ('Mali', 'Mali'),\n",
       " ('Pakistan', 'Pakistan'),\n",
       " ('Vereinigtes Königreich', 'United Kingdom'),\n",
       " ('Afghanistan', 'Afghanistan'),\n",
       " ('Tschechien', 'Czech Republic'),\n",
       " ('Uganda', 'Uganda'),\n",
       " ('Myanmar', 'Myanmar oder Burma'),\n",
       " ('Indien', 'India'),\n",
       " ('Iran', 'Iran'),\n",
       " [('Südafrika', 'South Africa')],\n",
       " ('Papua-Neuguinea', 'Papua New Guinea'),\n",
       " ('Italien', 'Italy'),\n",
       " [('Rumänien', 'Romania'),\n",
       "  ('Griechenland', 'Greece'),\n",
       "  ('Ungarn', 'Hungary'),\n",
       "  ('Italien', 'Italy')],\n",
       " ('Malawi', 'Malawi'),\n",
       " [('Rumänien', 'Romania'),\n",
       "  ('Serbien', 'Serbia'),\n",
       "  ('Ungarn', 'Hungary'),\n",
       "  ('Montenegro', 'Montenegro'),\n",
       "  ('Italien', 'Italy'),\n",
       "  ('Albanien', 'Albania'),\n",
       "  ('Frankreich', 'France'),\n",
       "  ('Türkei', 'Turkey'),\n",
       "  ('Griechenland', 'Greece'),\n",
       "  ('Kosovo', 'Kosovo'),\n",
       "  [('Mazedonien', 'Macedonia')]],\n",
       " ('Madagaskar', 'Madagascar'),\n",
       " ('Mauretanien', 'Mauritania')]"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated = translate(translate_abbreviation(clean_entries(countries)))\n",
    "translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DR Congo',\n",
       " 'DRCongo',\n",
       " 'Delaware',\n",
       " 'Französich Polynesien',\n",
       " 'Französiche Guyana',\n",
       " 'Französisch-Polynesien',\n",
       " 'La Reunion',\n",
       " 'Nordeuropa',\n",
       " 'Typhus',\n",
       " 'VAE Dubai'}"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened = [entry if type(sublist) == list else sublist for sublist in translated for entry in sublist ]\n",
    "countries_not_translated = set([entry for entry in flattened if type(entry) == str])\n",
    "countries_not_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unterstanding geoname annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Geoname Annotator\"\"\"\n",
    "\n",
    "from geopy.distance import great_circle\n",
    "from .maximum_weight_interval_set import Interval, find_maximum_weight_interval_set\n",
    "\n",
    "# Containment levels indicate which properties must match when determing\n",
    "# whether a geoname of a given containment level contains another geoname.\n",
    "# The admin codes generally correspond to states, provinces and cities.\n",
    "CONTAINMENT_LEVELS = [\n",
    "    'country_code',\n",
    "    'admin1_code',\n",
    "    'admin2_code',\n",
    "    'admin3_code',\n",
    "    'admin4_code'\n",
    "]\n",
    "\n",
    "GEONAME_ATTRS = [\n",
    "    'geonameid',\n",
    "    'name',\n",
    "    'feature_code',\n",
    "    'country_code',\n",
    "    'admin1_code',\n",
    "    'admin2_code',\n",
    "    'admin3_code',\n",
    "    'admin4_code',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'population',\n",
    "    'asciiname',\n",
    "    'names_used',\n",
    "    'name_count']\n",
    "def location_contains(loc_outer, loc_inner):\n",
    "    \"\"\"\n",
    "    Do a comparison to see if the first geoname contains the second.\n",
    "    It returns an integer to indicate the level of containment.\n",
    "    0 indicates no containment. Siblings locations and identical locations\n",
    "    have 0 containment. The level of containment is determined by the specificty\n",
    "    of the outer location. e.g. USA would be a smaller number than Texas.\n",
    "    In order for containment to be detected the outer location must have a\n",
    "    ADM* or PCL* feature code, which is most countries, states, and districts.\n",
    "    \"\"\"\n",
    "    # Test the country code in advance for efficiency. The country code must match for\n",
    "    # any level of containment.\n",
    "    if loc_outer.country_code != loc_inner.country_code or loc_outer.country_code == '':\n",
    "        return 0\n",
    "    feature_code = loc_outer.feature_code\n",
    "    if feature_code == 'ADM1':\n",
    "        outer_feature_level = 2\n",
    "    elif feature_code == 'ADM2':\n",
    "        outer_feature_level = 3\n",
    "    elif feature_code == 'ADM3':\n",
    "        outer_feature_level = 4\n",
    "    elif feature_code == 'ADM4':\n",
    "        outer_feature_level = 5\n",
    "    elif re.match(\"^PCL.\", feature_code):\n",
    "        outer_feature_level = 1\n",
    "    else:\n",
    "        return 0\n",
    "    for prop in CONTAINMENT_LEVELS[1:outer_feature_level]:\n",
    "        if loc_outer[prop] == '':\n",
    "            return 0\n",
    "        if loc_outer[prop] != loc_inner[prop]:\n",
    "            return 0\n",
    "    if loc_outer.geonameid == loc_inner.geonameid:\n",
    "        return 0\n",
    "    return outer_feature_level\n",
    "\n",
    "\n",
    "\n",
    "GEONAME_ATTRS = [\n",
    "    'geonameid',\n",
    "    'name',\n",
    "    'feature_code',\n",
    "    'country_code',\n",
    "    'admin1_code',\n",
    "    'admin2_code',\n",
    "    'admin3_code',\n",
    "    'admin4_code',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'population',\n",
    "    'asciiname',\n",
    "    'names_used',\n",
    "    'name_count']\n",
    "\n",
    "\n",
    "ADMINNAME_ATTRS = [\n",
    "    'country_name',\n",
    "    'admin1_name',\n",
    "    'admin2_name',\n",
    "    'admin3_name']\n",
    "\n",
    "\n",
    "class GeonameRow(object):\n",
    "    __slots__ = GEONAME_ATTRS + ADMINNAME_ATTRS + [\n",
    "        'alternate_locations',\n",
    "        'spans',\n",
    "        'parents',\n",
    "        'score',\n",
    "        'lat_long',\n",
    "        'high_confidence']\n",
    "\n",
    "    def __init__(self, sqlite3_row):\n",
    "        for key in sqlite3_row.keys():\n",
    "            if key in GEONAME_ATTRS:\n",
    "                setattr(self, key, sqlite3_row[key])\n",
    "        self.lat_long = (self.latitude, self.longitude,)\n",
    "        self.alternate_locations = set()\n",
    "        self.spans = set()\n",
    "        self.parents = set()\n",
    "        self.score = None\n",
    "\n",
    "\n",
    "\n",
    "    def to_dict(self):\n",
    "        result = {}\n",
    "        for key in GEONAME_ATTRS:\n",
    "            result[key] = self[key]\n",
    "        for key in ADMINNAME_ATTRS:\n",
    "            if hasattr(self, key):\n",
    "                result[key] = self[key]\n",
    "        result['parents'] = [p.to_dict() for p in self.parents]\n",
    "        result['score'] = self.score\n",
    "        return result\n",
    "\n",
    "\n",
    "class GeonameFeatures(object):\n",
    "    \"\"\"\n",
    "    This represents the aspects of a condidate geoname that are used to\n",
    "    determine whether it is being referenced.\n",
    "    \"\"\"\n",
    "    # The feature name array is used to maintain the order of the\n",
    "    # values in the feature vector.\n",
    "    feature_names = [\n",
    "        'log_population',\n",
    "        'name_count',\n",
    "        'num_spans',\n",
    "        'max_span_length',\n",
    "        'cannonical_name_used',\n",
    "        'loc_NE_portion',\n",
    "        'other_NE_portion',\n",
    "        'noun_portion',\n",
    "        'other_pos_portion',\n",
    "        'num_tokens',\n",
    "        'ambiguity',\n",
    "        'PPL_feature_code',\n",
    "        'ADM_feature_code',\n",
    "        'CONT_feature_code',\n",
    "        'other_feature_code',\n",
    "        'combined_span_parents',\n",
    "        'close_locations',\n",
    "        'very_close_locations',\n",
    "        'containing_locations',\n",
    "        'max_containment_level',\n",
    "        # high_confidence indicates the base feature set received a high score.\n",
    "        # It is an useful feature for preventing high confidence geonames\n",
    "        # from receiving low final scores when they lack contextual cues -\n",
    "        # for example, when they are the only location mentioned.\n",
    "        'high_confidence',\n",
    "    ]\n",
    "\n",
    "    def __init__(self, geoname, spans_to_nes, span_to_tokens):\n",
    "        self.geoname = geoname\n",
    "        # The set of geonames that are mentioned in proximity to the spans\n",
    "        # corresponding to this feature.\n",
    "        # This will be populated by the add_contextual_features function.\n",
    "        self.nearby_mentions = set()\n",
    "        d = {}\n",
    "        d['log_population'] = math.log(geoname.population + 1)\n",
    "        # Geonames with lots of alternate names\n",
    "        # tend to be the ones most commonly referred to.\n",
    "        d['name_count'] = geoname.name_count\n",
    "        d['num_spans'] = len(geoname.spans)\n",
    "        d['max_span_length'] = max([\n",
    "            len(span.text) for span in geoname.spans])\n",
    "\n",
    "        def cannonical_name_match(span, geoname):\n",
    "            first_leaf = next(span.iterate_leaf_base_spans(), None)\n",
    "            if first_leaf:\n",
    "                span_text = first_leaf.text\n",
    "            else:\n",
    "                span_text = span.text\n",
    "            span_in_name = span_text in geoname.name or span_text in geoname.asciiname\n",
    "            return (float(len(span_text)) if span_in_name else 0) / len(geoname.name)\n",
    "        d['cannonical_name_used'] = max([\n",
    "            cannonical_name_match(span, geoname)\n",
    "            for span in geoname.spans\n",
    "        ])\n",
    "        loc_NEs_overlap = 0\n",
    "        other_NEs_overlap = 0\n",
    "        total_spans = len(geoname.spans)\n",
    "        for span in geoname.spans:\n",
    "            for ne_span in spans_to_nes[span]:\n",
    "                if ne_span.label == 'GPE' or ne_span.label == 'LOC':\n",
    "                    loc_NEs_overlap += 1\n",
    "                else:\n",
    "                    other_NEs_overlap += 1\n",
    "        d['loc_NE_portion'] = float(loc_NEs_overlap) / total_spans\n",
    "        d['other_NE_portion'] = float(other_NEs_overlap) / total_spans\n",
    "        noun_pos_tags = 0\n",
    "        other_pos_tags = 0\n",
    "        pos_tags = 0\n",
    "        for span in geoname.spans:\n",
    "            for token_span in span_to_tokens[span]:\n",
    "                token = token_span.token\n",
    "                pos_tags += 1\n",
    "                if token.tag_.startswith(\"NN\") or token.tag_ == \"FW\":\n",
    "                    noun_pos_tags += 1\n",
    "                else:\n",
    "                    other_pos_tags += 1\n",
    "        d['combined_span_parents'] = len(geoname.parents)\n",
    "        d['noun_portion'] = float(noun_pos_tags) / pos_tags\n",
    "        d['other_pos_portion'] = float(other_pos_tags) / pos_tags\n",
    "        d['num_tokens'] = pos_tags\n",
    "        d['ambiguity'] = len(geoname.alternate_locations)\n",
    "        feature_code = geoname.feature_code\n",
    "        if feature_code.startswith('PPL'):\n",
    "            d['PPL_feature_code'] = 1\n",
    "        elif feature_code.startswith('ADM'):\n",
    "            d['ADM_feature_code'] = 1\n",
    "        elif feature_code.startswith('CONT'):\n",
    "            d['CONT_feature_code'] = 1\n",
    "        else:\n",
    "            d['other_feature_code'] = 1\n",
    "        self._values = [0] * len(self.feature_names)\n",
    "        self.set_values(d)\n",
    "\n",
    "    def set_value(self, feature_name, value):\n",
    "        self._values[self.feature_names.index(feature_name)] = value\n",
    "\n",
    "    def set_values(self, value_dict):\n",
    "        for idx, name in enumerate(self.feature_names):\n",
    "            if name in value_dict:\n",
    "                self._values[idx] = value_dict[name]\n",
    "\n",
    "    def set_contextual_features(self):\n",
    "        \"\"\"\n",
    "        GeonameFeatures are initialized with only values that can be extracted\n",
    "        from the geoname database and span. This extends the GeonameFeature\n",
    "        with values that require information from nearby_mentions.\n",
    "        \"\"\"\n",
    "        geoname = self.geoname\n",
    "        close_locations = 0\n",
    "        very_close_locations = 0\n",
    "        containing_locations = 0\n",
    "        max_containment_level = 0\n",
    "        for recently_mentioned_geoname in self.nearby_mentions:\n",
    "            if recently_mentioned_geoname == geoname:\n",
    "                continue\n",
    "            containment_level = max(\n",
    "                location_contains(geoname, recently_mentioned_geoname),\n",
    "                location_contains(recently_mentioned_geoname, geoname))\n",
    "            if containment_level > 0:\n",
    "                containing_locations += 1\n",
    "            if containment_level > max_containment_level:\n",
    "                max_containment_level = containment_level\n",
    "            distance = great_circle(\n",
    "                recently_mentioned_geoname.lat_long, geoname.lat_long\n",
    "            ).kilometers\n",
    "            if distance < 400:\n",
    "                close_locations += 1\n",
    "            if distance < 100:\n",
    "                very_close_locations += 1\n",
    "        self.set_values(dict(\n",
    "            close_locations=close_locations,\n",
    "            very_close_locations=very_close_locations,\n",
    "            containing_locations=containing_locations,\n",
    "            max_containment_level=max_containment_level))\n",
    "\n",
    "\n",
    "class GeonameAnnotator(Annotator):\n",
    "    def __init__(self, custom_classifier=None):\n",
    "        self.connection = get_database_connection()\n",
    "        self.connection.row_factory = sqlite3.Row\n",
    "        if custom_classifier:\n",
    "            self.geoname_classifier = custom_classifier\n",
    "        else:\n",
    "            self.geoname_classifier = geoname_classifier\n",
    "\n",
    "    def get_candidate_geonames(self, doc):\n",
    "        \"\"\"\n",
    "        Returns an array of geoname dicts correponding to locations that the\n",
    "        document may refer to.\n",
    "        The dicts are extended with lists of associated AnnoSpans.\n",
    "        \"\"\"\n",
    "        if 'ngrams' not in doc.tiers:\n",
    "            doc.add_tiers(NgramAnnotator())\n",
    "        logger.info('Ngrams annotated')\n",
    "        if 'nes' not in doc.tiers:\n",
    "            doc.add_tiers(NEAnnotator())\n",
    "        logger.info('Named entities annotated')\n",
    "\n",
    "        all_ngrams = list(set([span.text.lower()\n",
    "                               for span in doc.tiers['ngrams'].spans\n",
    "                               if is_possible_geoname(span.text)\n",
    "                               ]))\n",
    "        cursor = self.connection.cursor()\n",
    "        geoname_results = list(cursor.execute('''\n",
    "        SELECT\n",
    "            geonames.*,\n",
    "            count AS name_count,\n",
    "            group_concat(alternatename, \";\") AS names_used\n",
    "        FROM geonames\n",
    "        JOIN alternatename_counts USING ( geonameid )\n",
    "        JOIN alternatenames USING ( geonameid )\n",
    "        WHERE alternatename_lemmatized IN\n",
    "        (''' + ','.join('?' for x in all_ngrams) + ''')\n",
    "        GROUP BY geonameid''', all_ngrams))\n",
    "        logger.info('%s geonames fetched' % len(geoname_results))\n",
    "        geoname_results = [GeonameRow(g) for g in geoname_results]\n",
    "        # Associate spans with the geonames.\n",
    "        # This is done up front so span information can be used in the scoring\n",
    "        # function\n",
    "        span_text_to_spans = defaultdict(list)\n",
    "        for span in doc.tiers['ngrams'].spans:\n",
    "            if is_possible_geoname(span.text):\n",
    "                span_text_to_spans[span.text.lower()].append(span)\n",
    "        candidate_geonames = []\n",
    "        for geoname in geoname_results:\n",
    "            geoname.add_spans(span_text_to_spans)\n",
    "            # In rare cases geonames may have no matching spans because\n",
    "            # sqlite unicode equivalency rules match geonames that use different\n",
    "            # characters the document spans used to query them.\n",
    "            # These geonames are ignored.\n",
    "            if len(geoname.spans) > 0:\n",
    "                candidate_geonames.append(geoname)\n",
    "        # Add combined spans to locations that are adjacent to a span linked to\n",
    "        # an administrative division. e.g. Seattle, WA\n",
    "        span_to_geonames = defaultdict(list)\n",
    "        for geoname in candidate_geonames:\n",
    "            for span in geoname.spans:\n",
    "                span_to_geonames[span].append(geoname)\n",
    "        geoname_spans = span_to_geonames.keys()\n",
    "        combined_spans = AnnoTier(geoname_spans).chains(at_least=2, at_most=4, max_dist=4)\n",
    "        for combined_span in combined_spans:\n",
    "            leaf_spans = combined_span.iterate_leaf_base_spans()\n",
    "            first_spans = next(leaf_spans)\n",
    "            potential_geonames = {geoname: set()\n",
    "                                  for geoname in span_to_geonames[first_spans]}\n",
    "            for leaf_span in leaf_spans:\n",
    "                leaf_span_geonames = span_to_geonames[leaf_span]\n",
    "                next_potential_geonames = defaultdict(set)\n",
    "                for potential_geoname, prev_containing_geonames in potential_geonames.items():\n",
    "                    containing_geonames = [\n",
    "                        containing_geoname\n",
    "                        for containing_geoname in leaf_span_geonames\n",
    "                        if location_contains(containing_geoname, potential_geoname) > 0]\n",
    "                    if len(containing_geonames) > 0:\n",
    "                        next_potential_geonames[potential_geoname] |= prev_containing_geonames | set(containing_geonames)\n",
    "                potential_geonames = next_potential_geonames\n",
    "            for geoname, containing_geonames in potential_geonames.items():\n",
    "                geoname.spans.add(combined_span)\n",
    "                geoname.parents |= containing_geonames\n",
    "        # Replace individual spans with combined spans.\n",
    "        span_to_geonames = defaultdict(list)\n",
    "        for geoname in candidate_geonames:\n",
    "            geoname.spans = set(AnnoTier(geoname.spans).optimal_span_set().spans)\n",
    "            for span in geoname.spans:\n",
    "                span_to_geonames[span].append(geoname)\n",
    "        # Find locations with overlapping spans\n",
    "        # Note that is is possible for two valid locations to have\n",
    "        # overlapping names. For example, Harare Province has\n",
    "        # Harare as an alternate name, so the city Harare is very\n",
    "        # likely to be an alternate location that competes with it.\n",
    "        for span, geonames in span_to_geonames.items():\n",
    "            geoname_set = set(geonames)\n",
    "            for geoname in geonames:\n",
    "                geoname.alternate_locations |= geoname_set\n",
    "        for geoname in candidate_geonames:\n",
    "            geoname.alternate_locations -= set([geoname])\n",
    "        logger.info('%s alternative locations found' % sum([\n",
    "            len(geoname.alternate_locations)\n",
    "            for geoname in candidate_geonames]))\n",
    "        logger.info('%s candidate locations prepared' %\n",
    "                    len(candidate_geonames))\n",
    "        return candidate_geonames\n",
    "\n",
    "   \n",
    "\n",
    "    def add_contextual_features(self, features):\n",
    "        \"\"\"\n",
    "        Extend a list of features with values that are based on the geonames\n",
    "        mentioned nearby.\n",
    "        \"\"\"\n",
    "        logger.info('adding contextual features')\n",
    "        span_to_features = defaultdict(list)\n",
    "        for feature in features:\n",
    "            for span in feature.geoname.spans:\n",
    "                span_to_features[span].append(feature)\n",
    "        geoname_span_tier = AnnoTier(list(span_to_features.keys()))\n",
    "\n",
    "\n",
    "\n",
    "    def annotate(self, doc):\n",
    "        logger.info('geoannotator started')\n",
    "        candidate_geonames = self.get_candidate_geonames(doc)\n",
    "        features = self.extract_features(candidate_geonames, doc)\n",
    "        if len(features) == 0:\n",
    "            doc.tiers['geonames'] = AnnoTier([])\n",
    "            return doc\n",
    "\n",
    "        scores = self.geoname_classifier.predict_proba_base([\n",
    "            list(f.values()) for f in features])\n",
    "        for geoname, feature, score in zip(candidate_geonames, features, scores):\n",
    "            geoname.high_confidence = float(\n",
    "                score[1]) > self.geoname_classifier.HIGH_CONFIDENCE_THRESHOLD\n",
    "            feature.set_value('high_confidence', geoname.high_confidence)\n",
    "        has_high_confidence_features = any(\n",
    "            [geoname.high_confidence for geoname in candidate_geonames])\n",
    "        if has_high_confidence_features:\n",
    "            self.add_contextual_features(features)\n",
    "            scores = self.geoname_classifier.predict_proba_contextual([\n",
    "                list(f.values()) for f in features])\n",
    "        for geoname, score in zip(candidate_geonames, scores):\n",
    "            geoname.score = float(score[1])\n",
    "        culled_geonames = [geoname\n",
    "                           for geoname in candidate_geonames\n",
    "                           if geoname.score > self.geoname_classifier.GEONAME_SCORE_THRESHOLD]\n",
    "        cursor = self.connection.cursor()\n",
    "        for geoname in culled_geonames:\n",
    "            geoname_results = list(cursor.execute('''\n",
    "                SELECT\n",
    "                    cc.name,\n",
    "                    a1.name,\n",
    "                    a2.name,\n",
    "                    a3.name\n",
    "                FROM adminnames a3\n",
    "                JOIN adminnames a2 ON (\n",
    "                    a2.country_code = a3.country_code AND\n",
    "                    a2.admin1_code = a3.admin1_code AND\n",
    "                    a2.admin2_code = a3.admin2_code AND\n",
    "                    a2.admin3_code = \"\" )\n",
    "                JOIN adminnames a1 ON (\n",
    "                    a1.country_code = a3.country_code AND\n",
    "                    a1.admin1_code = a3.admin1_code AND\n",
    "                    a1.admin2_code = \"\" AND\n",
    "                    a1.admin3_code = \"\" )\n",
    "                JOIN adminnames cc ON (\n",
    "                    cc.country_code = a3.country_code AND\n",
    "                    cc.admin1_code = \"00\" AND\n",
    "                    cc.admin2_code = \"\" AND\n",
    "                    cc.admin3_code = \"\" )\n",
    "                WHERE (a3.country_code = ? AND a3.admin1_code = ? AND a3.admin2_code = ? AND a3.admin3_code = ?)\n",
    "                ''', (\n",
    "                geoname.country_code or \"\",\n",
    "                geoname.admin1_code or \"\",\n",
    "                geoname.admin2_code or \"\",\n",
    "                geoname.admin3_code or \"\",)))\n",
    "            for result in geoname_results:\n",
    "                prev_val = None\n",
    "                for idx, attr in enumerate(['country_name', 'admin1_name', 'admin2_name', 'admin3_name']):\n",
    "                    val = result[idx]\n",
    "                    if val == prev_val:\n",
    "                        # Names are repeated for admin levels beyond that of\n",
    "                        # the geoname.\n",
    "                        break\n",
    "                    setattr(geoname, attr, val)\n",
    "                    prev_val = val\n",
    "        logger.info('admin names added')\n",
    "        geo_spans = []\n",
    "        for geoname in culled_geonames:\n",
    "            for span in geoname.spans:\n",
    "                geo_span = GeoSpan(\n",
    "                    span.start, span.end, doc, geoname)\n",
    "                geo_spans.append(geo_span)\n",
    "        culled_geospans = AnnoTier(geo_spans).optimal_span_set(prefer=lambda x: (x.size(), x.geoname.score,))\n",
    "        logger.info('overlapping geospans removed')\n",
    "        return {'geonames': culled_geospans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==abE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
